Les <a href="">This is some text with a term that has a definition: <span class="has-definition" data-definition="This is the definition of the term.">GML</span></a> ne sont pas qu‚Äôun effet de mode
Les profondeurs cach√©es des GML : Plus que de simples syst√®mes d'auto-compl√©tion

Lorsque nous r√©fl√©chissons au fonctionnement du langage, il est √©vident que la signification d'un mot n'existe pas de mani√®re isol√©e. Des mots comme "chaise" ou "chien" ne portent pas de sens en eux-m√™mes - ce ne sont que des lettres que nous avons regroup√©es pour repr√©senter des concepts que nous reconnaissons.

La signification des mots provient de deux facteurs cl√©s : les autres mots de la phrase et les autres mots de toutes les phrases pr√©c√©dentes que vous avez d√©j√† rencontr√©es. Cette double source de sens explique pourquoi les grands mod√®les de langues (GML), aliment√©s par (1) des m√©canismes d'attention et (2) un entra√Ænement intensif sur des millions (voire des milliards) de phrases, sont si performants.

N'est-il pas ironique que le sens d'un mot vienne de tout sauf de lui-m√™me ?

Ce double processus refl√®te la fa√ßon dont les humains comprennent le langage. Nous tirons le sens (1) du contexte fourni et (2) de nos exp√©riences pass√©es.

Ainsi, c'est une id√©e fausse courante que les GML autor√©gressifs ne sont que des syst√®mes d'auto-compl√©tion avanc√©s. En effet, ces GML font bien plus que pr√©dire le mot suivant en se basant sur une simple probabilit√© statistique, comme le font les syst√®mes d'auto-compl√©tion de base. Les GML ne se contentent pas d'imiter la compr√©hension, ils en atteignent une certaine forme.

Les mod√®les autor√©gressifs g√©n√®rent du texte un mot √† la fois, en pr√©disant chaque mot en fonction du contexte pr√©c√©dent.

Contrairement aux GML, nous pensons et planifions avant de parler - nous ne sommes pas autor√©gressifs. Nous n'attendons pas de dire un mot pour commencer √† penser au suivant. Nous construisons des id√©es et des concepts, puis les mots et les phrases suivent. Cela conduit √† l'une des capacit√©s les plus fascinantes des GML, qui est observ√©e gr√¢ce √† l'utilisation de l'invite en cha√Æne de pens√©e (CoT).

CoT permet √† ces mod√®les d'imiter ce comportement de planification. Ils peuvent planifier leurs r√©ponses, en se d√©tachant des pr√©dictions simplistes mot par mot typiques des mod√®les autor√©gressifs. Cette capacit√© √©loigne encore plus les GML du simple statut d'auto-compl√©tion et les rapproche d'une forme d'intelligence - selon la fa√ßon dont vous la d√©finissez.

Au lieu de g√©n√©rer des mots directement, un par un, ils √©laborent un plan et l'int√®grent dans leur contexte, un peu comme nous r√©fl√©chissons avant de parler. C'est pourquoi l'invite CoT, surtout lorsqu'elle est combin√©e avec des exemples (invite en contexte) (comme nous l'avons vu dans une it√©ration r√©cente sur les approches essentielles d'invite), rend les mod√®les si puissants. Elle refl√®te notre processus de pens√©e tout en imitant notre capacit√© √† nous adapter √† diff√©rentes situations.

<aside>
üôè Nous aimerions remercier chaleureusement Geoffrey Hinton pour sa formidable conf√©rence d'ouverture √† la conf√©rence Ai4 √† laquelle nous avons assist√©, qui m'a inspir√© pour √©crire cet article.

</aside>

Vocabulaire
Voici la liste des mots fran√ßais utilis√©s dans la traduction et leurs √©quivalents anglais :

GML : LLMs
facteur : factors
mod√®le : models
attention : attention
processus : process
vraisemblance : likelihood
id√©es : ideas
Planification : planning
cassage : breaking
pr√©diction : predictions
exemples : examples
it√©ration : iteration
