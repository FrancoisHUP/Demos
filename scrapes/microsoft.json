[
{"url": "https://microsoft.github.io/graphrag/", "text": "Welcome to GraphRAG GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Welcome to GraphRAG üëâ Microsoft Research Blog Post üëâ GraphRAG Accelerator üëâ GitHub Repository üëâ GraphRAG Arxiv Figure 1: An LLM-generated knowledge graph built using GPT-4 Turbo. GraphRAG is a structured, hierarchical approach to Retrieval Augmented Generation (RAG), as opposed to naive semantic-search\napproaches using plain text snippets. The GraphRAG process involves extracting a knowledge graph out of raw text, building a community hierarchy, generating summaries for these communities, and then leveraging these structures when perform RAG-based tasks. To learn more about GraphRAG and how it can be used to enhance your LLMs ability to reason about your private data, please visit the Microsoft Research Blog Post . Solution Accelerator üöÄ To quickstart the GraphRAG system we recommend trying the Solution Accelerator package. This provides a user-friendly end-to-end experience with Azure resources. Get Started with GraphRAG üöÄ To start using GraphRAG, check out the Get Started guide.\nFor a deeper dive into the main sub-systems, please visit the docpages for the Indexer and Query packages. GraphRAG vs Baseline RAG üîç Retrieval-Augmented Generation (RAG) is a technique to improve LLM outputs using real-world information. This technique is an important part of most LLM-based tools and the majority of RAG approaches use vector similarity as the search technique, which we call Baseline RAG . GraphRAG uses knowledge graphs to provide substantial improvements in question-and-answer performance when reasoning about complex information. RAG techniques have shown promise in helping LLMs to reason about private datasets - data that the LLM is not trained on and has never seen before, such as an enterprise‚Äôs proprietary research, business documents, or communications. Baseline RAG was created to help solve this problem, but we observe situations where baseline RAG performs very poorly. For example: Baseline RAG struggles to connect the dots. This happens when answering a question requires traversing disparate pieces of information through their shared attributes in order to provide new synthesized insights. Baseline RAG performs poorly when being asked to holistically understand summarized semantic concepts over large data collections or even singular large documents. To address this, the tech community is working to develop methods that extend and enhance RAG. Microsoft Research‚Äôs new approach, GraphRAG, uses LLMs to create a knowledge graph based on an input corpus. This graph, along with community summaries and graph machine learning outputs, are used to augment prompts at query time. GraphRAG shows substantial improvement in answering the two classes of questions described above, demonstrating intelligence or mastery that outperforms other approaches previously applied to private datasets. The GraphRAG Process ü§ñ GraphRAG builds upon our prior research and tooling using graph machine learning. The basic steps of the GraphRAG process are as follows: Index Slice up an input corpus into a series of TextUnits, which act as analyzable units for the rest of the process, and provide fine-grained references in our outputs. Extract all entities, relationships, and key claims from the TextUnits using an LLM. Perform a hierarchical clustering of the graph using the Leiden technique . To see this visually, check out Figure 1 above. Each circle is an entity (e.g., a person, place, or organization), with the size representing the degree of the entity, and the color representing its community. Generate summaries of each community and its constituents from the bottom-up. This aids in holistic understanding of the dataset. Query At query time, these structures are used to provide materials for the LLM context window when answering a question. The primary query modes are: Global Search for reasoning about holistic questions about the corpus by leveraging the community summaries. Local Search for reasoning about specific entities by fanning-out to their neighbors and associated concepts. Prompt Tuning Using GraphRAG with your data out of the box may not yield the best possible results.\nWe strongly recommend to fine-tune your prompts following the Prompt Tuning Guide in our documentation. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/", "text": "Welcome to GraphRAG GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Welcome to GraphRAG üëâ Microsoft Research Blog Post üëâ GraphRAG Accelerator üëâ GitHub Repository üëâ GraphRAG Arxiv Figure 1: An LLM-generated knowledge graph built using GPT-4 Turbo. GraphRAG is a structured, hierarchical approach to Retrieval Augmented Generation (RAG), as opposed to naive semantic-search\napproaches using plain text snippets. The GraphRAG process involves extracting a knowledge graph out of raw text, building a community hierarchy, generating summaries for these communities, and then leveraging these structures when perform RAG-based tasks. To learn more about GraphRAG and how it can be used to enhance your LLMs ability to reason about your private data, please visit the Microsoft Research Blog Post . Solution Accelerator üöÄ To quickstart the GraphRAG system we recommend trying the Solution Accelerator package. This provides a user-friendly end-to-end experience with Azure resources. Get Started with GraphRAG üöÄ To start using GraphRAG, check out the Get Started guide.\nFor a deeper dive into the main sub-systems, please visit the docpages for the Indexer and Query packages. GraphRAG vs Baseline RAG üîç Retrieval-Augmented Generation (RAG) is a technique to improve LLM outputs using real-world information. This technique is an important part of most LLM-based tools and the majority of RAG approaches use vector similarity as the search technique, which we call Baseline RAG . GraphRAG uses knowledge graphs to provide substantial improvements in question-and-answer performance when reasoning about complex information. RAG techniques have shown promise in helping LLMs to reason about private datasets - data that the LLM is not trained on and has never seen before, such as an enterprise‚Äôs proprietary research, business documents, or communications. Baseline RAG was created to help solve this problem, but we observe situations where baseline RAG performs very poorly. For example: Baseline RAG struggles to connect the dots. This happens when answering a question requires traversing disparate pieces of information through their shared attributes in order to provide new synthesized insights. Baseline RAG performs poorly when being asked to holistically understand summarized semantic concepts over large data collections or even singular large documents. To address this, the tech community is working to develop methods that extend and enhance RAG. Microsoft Research‚Äôs new approach, GraphRAG, uses LLMs to create a knowledge graph based on an input corpus. This graph, along with community summaries and graph machine learning outputs, are used to augment prompts at query time. GraphRAG shows substantial improvement in answering the two classes of questions described above, demonstrating intelligence or mastery that outperforms other approaches previously applied to private datasets. The GraphRAG Process ü§ñ GraphRAG builds upon our prior research and tooling using graph machine learning. The basic steps of the GraphRAG process are as follows: Index Slice up an input corpus into a series of TextUnits, which act as analyzable units for the rest of the process, and provide fine-grained references in our outputs. Extract all entities, relationships, and key claims from the TextUnits using an LLM. Perform a hierarchical clustering of the graph using the Leiden technique . To see this visually, check out Figure 1 above. Each circle is an entity (e.g., a person, place, or organization), with the size representing the degree of the entity, and the color representing its community. Generate summaries of each community and its constituents from the bottom-up. This aids in holistic understanding of the dataset. Query At query time, these structures are used to provide materials for the LLM context window when answering a question. The primary query modes are: Global Search for reasoning about holistic questions about the corpus by leveraging the community summaries. Local Search for reasoning about specific entities by fanning-out to their neighbors and associated concepts. Prompt Tuning Using GraphRAG with your data out of the box may not yield the best possible results.\nWe strongly recommend to fine-tune your prompts following the Prompt Tuning Guide in our documentation. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/query/overview/", "text": "Query Engine  üîé GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Query Engine  üîé The Query Engine is the retrieval module of the Graph RAG Library. It is one of the two main components of the Graph RAG library, the other being the Indexing Pipeline (see Indexing Pipeline ).\nIt is responsible for the following tasks: Local Search Global Search Question Generation Local Search Local search method generates answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents (e.g. What are the healing properties of chamomile?). For more details about how Local Search works please refer to the Local Search documentation. Global Search Global search method generates answers by searching over all AI-generated community reports in a map-reduce fashion. This is a resource-intensive method, but often gives good responses for questions that require an understanding of the dataset as a whole (e.g. What are the most significant values of the herbs mentioned in this notebook?). More about this can be checked at the Global Search documentation. Question Generation This functionality takes a list of user queries and generates the next candidate questions. This is useful for generating follow-up questions in a conversation or for generating a list of questions for the investigator to dive deeper into the dataset. Information about how question generation works can be found at the Question Generation documentation page. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/query/0-global_search/", "text": "Global Search üîé GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Global Search üîé Whole Dataset Reasoning Baseline RAG struggles with queries that require aggregation of information across the dataset to compose an answer. Queries such as ‚ÄúWhat are the top 5 themes in the data?‚Äù perform terribly because baseline RAG relies on a vector search of semantically similar text content within the dataset. There is nothing in the query to direct it to the correct information. However, with GraphRAG we can answer such questions, because the structure of the LLM-generated knowledge graph tells us about the structure (and thus themes) of the dataset as a whole. This allows the private dataset to be organized into meaningful semantic clusters that are pre-summarized. Using our global search method, the LLM uses these clusters to summarize these themes when responding to a user query. Methodology ---\ntitle: Global Search Dataflow\n---\n%%{ init: { 'flowchart': { 'curve': 'step' } } }%%\nflowchart LR\n\n    uq[User Query] --- .1\n    ch1[Conversation History] --- .1\n\n    subgraph RIR\n        direction TB\n        ri1[Rated Intermediate<br/>Response 1]~~~ri2[Rated Intermediate<br/>Response 2] -.\"{1..N}\".-rin[Rated Intermediate<br/>Response N]\n    end\n\n    .1--Shuffled Community<br/>Report Batch 1-->RIR\n    .1--Shuffled Community<br/>Report Batch 2-->RIR---.2\n    .1--Shuffled Community<br/>Report Batch N-->RIR\n\n    .2--Ranking +<br/>Filtering-->agr[Aggregated Intermediate<br/>Responses]-->res[Response]\n\n\n\n     classDef green fill:#26B653,stroke:#333,stroke-width:2px,color:#fff;\n     classDef turquoise fill:#19CCD3,stroke:#333,stroke-width:2px,color:#fff;\n     classDef rose fill:#DD8694,stroke:#333,stroke-width:2px,color:#fff;\n     classDef orange fill:#F19914,stroke:#333,stroke-width:2px,color:#fff;\n     classDef purple fill:#B356CD,stroke:#333,stroke-width:2px,color:#fff;\n     classDef invisible fill:#fff,stroke:#fff,stroke-width:0px,color:#fff, width:0px;\n     class uq,ch1 turquoise;\n     class ri1,ri2,rin rose;\n     class agr orange;\n     class res purple;\n     class .1,.2 invisible; Given a user query and, optionally, the conversation history, the global search method uses a collection of LLM-generated community reports from a specified level of the graph's community hierarchy as context data to generate response in a map-reduce manner. At the map step, community reports are segmented into text chunks of pre-defined size. Each text chunk is then used to produce an intermediate response containing a list of point, each of which is accompanied by a numerical rating indicating the importance of the point. At the reduce step, a filtered set of the most important points from the intermediate responses are aggregated and used as the context to generate the final response. The quality of the global search‚Äôs response can be heavily influenced by the level of the community hierarchy chosen for sourcing community reports. Lower hierarchy levels, with their detailed reports, tend to yield more thorough responses, but may also increase the time and LLM resources needed to generate the final response due to the volume of reports. Configuration Below are the key parameters of the GlobalSearch class : llm : OpenAI model object to be used for response generation context_builder : context builder object to be used for preparing context data from community reports map_system_prompt : prompt template used in the map stage. Default template can be found at map_system_prompt reduce_system_prompt : prompt template used in the reduce stage, default template can be found at reduce_system_prompt response_type : free-form text describing the desired response type and format (e.g., Multiple Paragraphs , Multi-Page Report ) allow_general_knowledge : setting this to True will include additional instructions to the reduce_system_prompt to prompt the LLM to incorporate relevant real-world knowledge outside of the dataset. Note that this may increase hallucinations, but can be useful for certain scenarios. Default is False\n* general_knowledge_inclusion_prompt : instruction to add to the reduce_system_prompt if allow_general_knowledge is enabled. Default instruction can be found at general_knowledge_instruction max_data_tokens : token budget for the context data map_llm_params : a dictionary of additional parameters (e.g., temperature, max_tokens) to be passed to the LLM call at the map stage reduce_llm_params : a dictionary of additional parameters (e.g., temperature, max_tokens) to passed to the LLM call at the reduce stage context_builder_params : a dictionary of additional parameters to be passed to the context_builder object when building context window for the map stage. concurrent_coroutines : controls the degree of parallelism in the map stage. callbacks : optional callback functions, can be used to provide custom event handlers for LLM's completion streaming events How to Use An example of a global search scenario can be found in the following notebook . Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/query/3-cli/", "text": "Query CLI GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Query CLI The GraphRAG query CLI allows for no-code usage of the GraphRAG Query engine. python -m graphrag.query --config < config_file.yml > --data < path-to-data > --community_level < comunit-level > --response_type < response-type > --method < \"local\" | \"global\" > < query > CLI Arguments --config <config_file.yml> - The configuration yaml file to use when running the query. If this is used, then none of the environment-variables below will apply. --data <path-to-data> - Folder containing the .parquet output files from running the Indexer. --community_level <community-level> - Community level in the Leiden community hierarchy from which we will load the community reports higher value means we use reports on smaller communities. Default: 2 --response_type <response-type> - Free form text describing the response type and format, can be anything, e.g. Multiple Paragraphs , Single Paragraph , Single Sentence , List of 3-7 Points , Single Page , Multi-Page Report . Default: Multiple Paragraphs . --method <\"local\"|\"global\"> - Method to use to answer the query, one of local or global. For more information check Overview Env Variables Required environment variables to execute: GRAPHRAG_API_KEY - API Key for executing the model, will fallback to OPENAI_API_KEY if one is not provided. GRAPHRAG_LLM_MODEL - Model to use for Chat Completions. GRAPHRAG_EMBEDDING_MODEL - Model to use for Embeddings. You can further customize the execution by providing these environment variables: GRAPHRAG_LLM_API_BASE - The API Base URL. Default: None GRAPHRAG_LLM_TYPE - The LLM operation type. Either openai_chat or azure_openai_chat . Default: openai_chat GRAPHRAG_LLM_MAX_RETRIES - The maximum number of retries to attempt when a request fails. Default: 20 GRAPHRAG_EMBEDDING_API_BASE - The API Base URL. Default: None GRAPHRAG_EMBEDDING_TYPE - The embedding client to use. Either openai_embedding or azure_openai_embedding . Default: openai_embedding GRAPHRAG_EMBEDDING_MAX_RETRIES - The maximum number of retries to attempt when a request fails. Default: 20 GRAPHRAG_LOCAL_SEARCH_TEXT_UNIT_PROP - Proportion of context window dedicated to related text units. Default: 0.5 GRAPHRAG_LOCAL_SEARCH_COMMUNITY_PROP - Proportion of context window dedicated to community reports. Default: 0.1 GRAPHRAG_LOCAL_SEARCH_CONVERSATION_HISTORY_MAX_TURNS - Maximum number of turns to include in the conversation history. Default: 5 GRAPHRAG_LOCAL_SEARCH_TOP_K_ENTITIES - Number of related entities to retrieve from the entity description embedding store. Default: 10 GRAPHRAG_LOCAL_SEARCH_TOP_K_RELATIONSHIPS - Control the number of out-of-network relationships to pull into the context window. Default: 10 GRAPHRAG_LOCAL_SEARCH_MAX_TOKENS - Change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000). Default: 12000 GRAPHRAG_LOCAL_SEARCH_LLM_MAX_TOKENS - Change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000=1500). Default: 2000 GRAPHRAG_GLOBAL_SEARCH_MAX_TOKENS - Change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000). Default: 12000 GRAPHRAG_GLOBAL_SEARCH_DATA_MAX_TOKENS - Change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000). Default: 12000 GRAPHRAG_GLOBAL_SEARCH_MAP_MAX_TOKENS - Default: 500 GRAPHRAG_GLOBAL_SEARCH_REDUCE_MAX_TOKENS - Change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000-1500). Default: 2000 GRAPHRAG_GLOBAL_SEARCH_CONCURRENCY - Default: 32 Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/prompt_tuning/manual_prompt_tuning/", "text": "Prompt Tuning‚öôÔ∏è GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Prompt Tuning‚öôÔ∏è The GraphRAG indexer, by default, will run with a handful of prompts that are designed to work well in the broad context of knowledge discovery.\nHowever, it is quite common to want to tune the prompts to better suit your specific use case.\nWe provide a means for you to do this by allowing you to specify a custom prompt file, which will each use a series of token-replacements internally. Each of these prompts may be overridden by writing a custom prompt file in plaintext. We use token-replacements in the form of {token_name} , and the descriptions for the available tokens can be found below. Entity/Relationship Extraction Prompt Source Tokens (values provided by extractor) {input_text} - The input text to be processed. {entity_types} - A list of entity types {tuple_delimiter} - A delimiter for separating values within a tuple. A single tuple is used to represent an individual entity or relationship. {record_delimiter} - A delimiter for separating tuple instances. {completion_delimiter} - An indicator for when generation is complete. Summarize Entity/Relationship Descriptions Prompt Source Tokens (values provided by extractor) {entity_name} - The name of the entity or the source/target pair of the relationship. {description_list} - A list of descriptions for the entity or relationship. Claim Extraction Prompt Source Tokens (values provided by extractor) {input_text} - The input text to be processed. {tuple_delimiter} - A delimiter for separating values within a tuple. A single tuple is used to represent an individual entity or relationship. {record_delimiter} - A delimiter for separating tuple instances. {completion_delimiter} - An indicator for when generation is complete. Note: there is additional paramater for the Claim Description that is used in claim extraction.\nThe default value is \"Any claims or facts that could be relevant to information discovery.\" See the configuration documentation for details on how to change this. Generate Community Reports Prompt Source Tokens (values provided by extractor) {input_text} - The input text to generate the report with. This will contain tables of entities and relationships. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/query/1-local_search/", "text": "Local Search üîé GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Local Search üîé Entity-based Reasoning The local search method combines structured data from the knowledge graph with unstructured data from the input documents to augment the LLM context with relevant entity information at query time. It is well-suited for answering questions that require an understanding of specific entities mentioned in the input documents (e.g., ‚ÄúWhat are the healing properties of chamomile?‚Äù). Methodology ---\ntitle: Local Search Dataflow\n---\n%%{ init: { 'flowchart': { 'curve': 'step' } } }%%\nflowchart LR\n\n    uq[User Query] ---.1\n    ch1[Conversation<br/>History]---.1\n\n    .1--Entity<br/>Description<br/>Embedding--> ee[Extracted Entities]\n\n    ee[Extracted Entities] ---.2--Entity-Text<br/>Unit Mapping--> ctu[Candidate<br/>Text Units]--Ranking + <br/>Filtering -->ptu[Prioritized<br/>Text Units]---.3\n    .2--Entity-Report<br/>Mapping--> ccr[Candidate<br/>Community Reports]--Ranking + <br/>Filtering -->pcr[Prioritized<br/>Community Reports]---.3\n    .2--Entity-Entity<br/>Relationships--> ce[Candidate<br/>Entities]--Ranking + <br/>Filtering -->pe[Prioritized<br/>Entities]---.3\n    .2--Entity-Entity<br/>Relationships--> cr[Candidate<br/>Relationships]--Ranking + <br/>Filtering -->pr[Prioritized<br/>Relationships]---.3\n    .2--Entity-Covariate<br/>Mappings--> cc[Candidate<br/>Covariates]--Ranking + <br/>Filtering -->pc[Prioritized<br/>Covariates]---.3\n    ch1 -->ch2[Conversation History]---.3\n    .3-->res[Response]\n\n     classDef green fill:#26B653,stroke:#333,stroke-width:2px,color:#fff;\n     classDef turquoise fill:#19CCD3,stroke:#333,stroke-width:2px,color:#fff;\n     classDef rose fill:#DD8694,stroke:#333,stroke-width:2px,color:#fff;\n     classDef orange fill:#F19914,stroke:#333,stroke-width:2px,color:#fff;\n     classDef purple fill:#B356CD,stroke:#333,stroke-width:2px,color:#fff;\n     classDef invisible fill:#fff,stroke:#fff,stroke-width:0px,color:#fff, width:0px;\n     class uq,ch1 turquoise\n     class ee green\n     class ctu,ccr,ce,cr,cc rose\n     class ptu,pcr,pe,pr,pc,ch2 orange\n     class res purple\n     class .1,.2,.3 invisible Given a user query and, optionally, the conversation history, the local search method identifies a set of entities from the knowledge graph that are semantically-related to the user input. These entities serve as access points into the knowledge graph, enabling the extraction of further relevant details such as connected entities, relationships, entity covariates, and community reports. Additionally, it also extracts relevant text chunks from the raw input documents that are associated with the identified entities. These candidate data sources are then prioritized and filtered to fit within a single context window of pre-defined size, which is used to generate a response to the user query. Configuration Below are the key parameters of the LocalSearch class : llm : OpenAI model object to be used for response generation context_builder : context builder object to be used for preparing context data from collections of knowledge model objects system_prompt : prompt template used to generate the search response. Default template can be found at system_prompt response_type : free-form text describing the desired response type and format (e.g., Multiple Paragraphs , Multi-Page Report ) llm_params : a dictionary of additional parameters (e.g., temperature, max_tokens) to be passed to the LLM call context_builder_params : a dictionary of additional parameters to be passed to the context_builder object when building context for the search prompt callbacks : optional callback functions, can be used to provide custom event handlers for LLM's completion streaming events How to Use An example of a local search scenario can be found in the following notebook . Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/query/2-question_generation/", "text": "Question Generation ‚ùî GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Question Generation ‚ùî Entity-based Question Generation The question generation method combines structured data from the knowledge graph with unstructured data from the input documents to generate candidate questions related to specific entities. Methodology Given a list of prior user questions, the question generation method uses the same context-building approach employed in local search to extract and prioritize relevant structured and unstructured data, including entities, relationships, covariates, community reports and raw text chunks. These data records are then fitted into a single LLM prompt to generate candidate follow-up questions that represent the most important or urgent information content or themes in the data. Configuration Below are the key parameters of the Question Generation class : llm : OpenAI model object to be used for response generation context_builder : context builder object to be used for preparing context data from collections of knowledge model objects, using the same context builder class as in local search system_prompt : prompt template used to generate candidate questions. Default template can be found at system_prompt llm_params : a dictionary of additional parameters (e.g., temperature, max_tokens) to be passed to the LLM call context_builder_params : a dictionary of additional parameters to be passed to the context_builder object when building context for the question generation prompt callbacks : optional callback functions, can be used to provide custom event handlers for LLM's completion streaming events How to Use An example of the question generation function can be found in the following notebook . Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/query/notebooks/overview/", "text": "Query Engine Notebooks GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Query Engine Notebooks For examples about running Query please refer to the following notebooks: Global Search Notebook Local Search Notebook The test dataset for these notebooks can be found here . Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/prompt_tuning/overview/", "text": "Prompt Tuning ‚öôÔ∏è GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Prompt Tuning ‚öôÔ∏è This page provides an overview of the prompt tuning options available for the GraphRAG indexing engine. Default Prompts The default prompts are the simplest way to get started with the GraphRAG system. It is designed to work out-of-the-box with minimal configuration. You can find more detail about these prompts in the following links: Entity/Relationship Extraction Entity/Relationship Description Summarization Claim Extraction Community Reports Auto Templating Auto Templating leverages your input data and LLM interactions to create domain adaptive templates for the generation of the knowledge graph. It is highly encouraged to run it as it will yield better results when executing an Index Run. For more details about how to use it, please refer to the Auto Templating documentation. Manual Configuration Manual configuration is an advanced use-case. Most users will want to use the Auto Templating feature instead. Details about how to use manual configuration are available in the Manual Prompt Configuration documentation. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/prompt_tuning/auto_prompt_tuning/", "text": "Prompt Tuning ‚öôÔ∏è GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Prompt Tuning ‚öôÔ∏è GraphRAG provides the ability to create domain adaptive templates for the generation of the knowledge graph. This step is optional, though it is highly encouraged to run it as it will yield better results when executing an Index Run. The templates are generated by loading the inputs, splitting them into chunks (text units) and then running a series of LLM invocations and template substitutions to generate the final prompts. We suggest using the default values provided by the script, but in this page you'll find the detail of each in case you want to further explore and tweak the template generation algorithm. Prerequisites Before running the automatic template generation make sure you have already initialized your workspace with the graphrag.index --init command. This will create the necessary configuration files and the default prompts. Refer to the Init Documentation for more information about the initialization process. Usage You can run the main script from the command line with various options: python -m graphrag.prompt_tune [ --root ROOT ] [ --domain DOMAIN ] [ --method METHOD ] [ --limit LIMIT ] [ --language LANGUAGE ] [ --max-tokens MAX_TOKENS ] [ --chunk-size CHUNK_SIZE ] [ --no-entity-types ] [ --output OUTPUT ] Command-Line Options --root (optional): The data project root directory, including the config files (YML, JSON, or .env). Defaults to the current directory. --domain (optional): The domain related to your input data, such as 'space science', 'microbiology', or 'environmental news'. If left empty, the domain will be inferred from the input data. --method (optional): The method to select documents. Options are all, random, or top. Default is random. --limit (optional): The limit of text units to load when using random or top selection. Default is 15. --language (optional): The language to use for input processing. If it is different from the inputs' language, the LLM will translate. Default is \"\" meaning it will be automatically detected from the inputs. --max-tokens (optional): Maximum token count for prompt generation. Default is 2000. --chunk-size (optional): The size in tokens to use for generating text units from input documents. Default is 200. --no-entity-types (optional): Use untyped entity extraction generation. We recommend using this when your data covers a lot of topics or it is highly randomized. --output (optional): The folder to save the generated prompts. Default is \"prompts\". Example Usage python -m graphrag.prompt_tune --root /path/to/project --domain \"environmental news\" --method random --limit 10 --language English --max-tokens 2048 --chunk-size 256 --no-entity-types --output /path/to/output or, with minimal configuration (suggested): python -m graphrag.prompt_tune --root /path/to/project --no-entity-types Document Selection Methods The auto template feature ingests the input data and then divides it into text units the size of the chunk size parameter.\nAfter that, it uses one of the following selection methods to pick a sample to work with for template generation: random : Select text units randomly. This is the default and recommended option. top : Select the head n text units. all : Use all text units for the generation. Use only with small datasets; this option is not usually recommended. Modify Env Vars After running auto-templating, you should modify the following environment variables (or config variables) to pick up the new prompts on your index run. Note: Please make sure to update the correct path to the generated prompts, in this example we are using the default \"prompts\" path. GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE = \"prompts/entity_extraction.txt\" GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE = \"prompts/community_report.txt\" GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE = \"prompts/summarize_descriptions.txt\" Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/query/notebooks/local_search_nb/", "text": "GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License. import os import pandas as pd import tiktoken from graphrag . query . context_builder . entity_extraction import EntityVectorStoreKey from graphrag . query . indexer_adapters import ( read_indexer_covariates , read_indexer_entities , read_indexer_relationships , read_indexer_reports , read_indexer_text_units , ) from graphrag . query . input . loaders . dfs import ( store_entity_semantic_embeddings , ) from graphrag . query . llm . oai . chat_openai import ChatOpenAI from graphrag . query . llm . oai . embedding import OpenAIEmbedding from graphrag . query . llm . oai . typing import OpenaiApiType from graphrag . query . question_gen . local_gen import LocalQuestionGen from graphrag . query . structured_search . local_search . mixed_context import ( LocalSearchMixedContext , ) from graphrag . query . structured_search . local_search . search import LocalSearch from graphrag . vector_stores . lancedb import LanceDBVectorStore Local Search Example Local search method generates answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents (e.g. What are the healing properties of chamomile?). Load text units and graph data tables as context for local search In this test we first load indexing outputs from parquet files to dataframes, then convert these dataframes into collections of data objects aligning with the knowledge model. Load tables to dataframes INPUT_DIR = \"./inputs/operation dulce\" LANCEDB_URI = f\" { INPUT_DIR } /lancedb\" COMMUNITY_REPORT_TABLE = \"create_final_community_reports\" ENTITY_TABLE = \"create_final_nodes\" ENTITY_EMBEDDING_TABLE = \"create_final_entities\" RELATIONSHIP_TABLE = \"create_final_relationships\" COVARIATE_TABLE = \"create_final_covariates\" TEXT_UNIT_TABLE = \"create_final_text_units\" COMMUNITY_LEVEL = 2 Read entities # read nodes table to get community and degree data entity_df = pd . read_parquet ( f\" { INPUT_DIR } / { ENTITY_TABLE } .parquet\" ) entity_embedding_df = pd . read_parquet ( f\" { INPUT_DIR } / { ENTITY_EMBEDDING_TABLE } .parquet\" ) entities = read_indexer_entities ( entity_df , entity_embedding_df , COMMUNITY_LEVEL ) # load description embeddings to an in-memory lancedb vectorstore # to connect to a remote db, specify url and port values. description_embedding_store = LanceDBVectorStore ( collection_name = \"entity_description_embeddings\" , ) description_embedding_store . connect ( db_uri = LANCEDB_URI ) entity_description_embeddings = store_entity_semantic_embeddings ( entities = entities , vectorstore = description_embedding_store ) print ( f\"Entity count: { len ( entity_df ) } \" ) entity_df . head ( ) Read relationships relationship_df = pd . read_parquet ( f\" { INPUT_DIR } / { RELATIONSHIP_TABLE } .parquet\" ) relationships = read_indexer_relationships ( relationship_df ) print ( f\"Relationship count: { len ( relationship_df ) } \" ) relationship_df . head ( ) covariate_df = pd . read_parquet ( f\" { INPUT_DIR } / { COVARIATE_TABLE } .parquet\" ) claims = read_indexer_covariates ( covariate_df ) print ( f\"Claim records: { len ( claims ) } \" ) covariates = { \"claims\" : claims } Read community reports report_df = pd . read_parquet ( f\" { INPUT_DIR } / { COMMUNITY_REPORT_TABLE } .parquet\" ) reports = read_indexer_reports ( report_df , entity_df , COMMUNITY_LEVEL ) print ( f\"Report records: { len ( report_df ) } \" ) report_df . head ( ) Read text units text_unit_df = pd . read_parquet ( f\" { INPUT_DIR } / { TEXT_UNIT_TABLE } .parquet\" ) text_units = read_indexer_text_units ( text_unit_df ) print ( f\"Text unit records: { len ( text_unit_df ) } \" ) text_unit_df . head ( ) api_key = os . environ [ \"GRAPHRAG_API_KEY\" ] llm_model = os . environ [ \"GRAPHRAG_LLM_MODEL\" ] embedding_model = os . environ [ \"GRAPHRAG_EMBEDDING_MODEL\" ] llm = ChatOpenAI ( api_key = api_key , model = llm_model , api_type = OpenaiApiType . OpenAI , # OpenaiApiType.OpenAI or OpenaiApiType.AzureOpenAI max_retries = 20 , ) token_encoder = tiktoken . get_encoding ( \"cl100k_base\" ) text_embedder = OpenAIEmbedding ( api_key = api_key , api_base = None , api_type = OpenaiApiType . OpenAI , model = embedding_model , deployment_name = embedding_model , max_retries = 20 , ) Create local search context builder context_builder = LocalSearchMixedContext ( community_reports = reports , text_units = text_units , entities = entities , relationships = relationships , covariates = covariates , entity_text_embeddings = description_embedding_store , embedding_vectorstore_key = EntityVectorStoreKey . ID , # if the vectorstore uses entity title as ids, set this to EntityVectorStoreKey.TITLE text_embedder = text_embedder , token_encoder = token_encoder , ) Create local search engine # text_unit_prop: proportion of context window dedicated to related text units # community_prop: proportion of context window dedicated to community reports. # The remaining proportion is dedicated to entities and relationships. Sum of text_unit_prop and community_prop should be <= 1 # conversation_history_max_turns: maximum number of turns to include in the conversation history. # conversation_history_user_turns_only: if True, only include user queries in the conversation history. # top_k_mapped_entities: number of related entities to retrieve from the entity description embedding store. # top_k_relationships: control the number of out-of-network relationships to pull into the context window. # include_entity_rank: if True, include the entity rank in the entity table in the context window. Default entity rank = node degree. # include_relationship_weight: if True, include the relationship weight in the context window. # include_community_rank: if True, include the community rank in the context window. # return_candidate_context: if True, return a set of dataframes containing all candidate entity/relationship/covariate records that # could be relevant. Note that not all of these records will be included in the context window. The \"in_context\" column in these # dataframes indicates whether the record is included in the context window. # max_tokens: maximum number of tokens to use for the context window. local_context_params = { \"text_unit_prop\" : 0.5 , \"community_prop\" : 0.1 , \"conversation_history_max_turns\" : 5 , \"conversation_history_user_turns_only\" : True , \"top_k_mapped_entities\" : 10 , \"top_k_relationships\" : 10 , \"include_entity_rank\" : True , \"include_relationship_weight\" : True , \"include_community_rank\" : False , \"return_candidate_context\" : False , \"embedding_vectorstore_key\" : EntityVectorStoreKey . ID , # set this to EntityVectorStoreKey.TITLE if the vectorstore uses entity title as ids \"max_tokens\" : 12_000 , # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000) } llm_params = { \"max_tokens\" : 2_000 , # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000=1500) \"temperature\" : 0.0 , } search_engine = LocalSearch ( llm = llm , context_builder = context_builder , token_encoder = token_encoder , llm_params = llm_params , context_builder_params = local_context_params , response_type = \"multiple paragraphs\" , # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report ) Run local search on sample queries result = await search_engine . asearch ( \"Tell me about Agent Mercer\" ) print ( result . response ) question = \"Tell me about Dr. Jordan Hayes\" result = await search_engine . asearch ( question ) print ( result . response ) Inspecting the context data used to generate the response result . context_data [ \"entities\" ] . head ( ) result . context_data [ \"relationships\" ] . head ( ) result . context_data [ \"reports\" ] . head ( ) result . context_data [ \"sources\" ] . head ( ) if \"claims\" in result . context_data : print ( result . context_data [ \"claims\" ] . head ( ) ) Question Generation This function takes a list of user queries and generates the next candidate questions. question_generator = LocalQuestionGen ( llm = llm , context_builder = context_builder , token_encoder = token_encoder , llm_params = llm_params , context_builder_params = local_context_params , ) question_history = [ \"Tell me about Agent Mercer\" , \"What happens in Dulce military base?\" , ] candidate_questions = await question_generator . agenerate ( question_history = question_history , context_data = None , question_count = 5 ) print ( candidate_questions . response ) Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/config/template/", "text": "Configuration Template GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Configuration Template The following template can be used and stored as a .env in the the directory where you're are pointing\nthe --root parameter on your Indexing Pipeline execution. For details about how to run the Indexing Pipeline, refer to the Index CLI documentation. .env File Template Required variables are uncommented. All the optional configuration can be turned on or off as needed. Minimal Configuration # Base LLM Settings GRAPHRAG_API_KEY = \"your_api_key\" GRAPHRAG_API_BASE = \"http://<domain>.openai.azure.com\" # For Azure OpenAI Users GRAPHRAG_API_VERSION = \"api_version\" # For Azure OpenAI Users # Text Generation Settings GRAPHRAG_LLM_TYPE = \"azure_openai_chat\" # or openai_chat GRAPHRAG_LLM_DEPLOYMENT_NAME = \"gpt-4-turbo-preview\" GRAPHRAG_LLM_MODEL_SUPPORTS_JSON = True # Text Embedding Settings GRAPHRAG_EMBEDDING_TYPE = \"azure_openai_embedding\" # or openai_embedding GRAPHRAG_LLM_DEPLOYMENT_NAME = \"text-embedding-3-small\" # Data Mapping Settings GRAPHRAG_INPUT_TYPE = \"text\" Full Configuration # Required LLM Config # Input Data Configuration GRAPHRAG_INPUT_TYPE = \"file\" # Plaintext Input Data Configuration # GRAPHRAG_INPUT_FILE_PATTERN=.*\\.txt # Text Input Data Configuration GRAPHRAG_INPUT_FILE_TYPE = \"text\" GRAPHRAG_INPUT_FILE_PATTERN = \".*\\.txt$\" GRAPHRAG_INPUT_SOURCE_COLUMN = source # GRAPHRAG_INPUT_TIMESTAMP_COLUMN=None # GRAPHRAG_INPUT_TIMESTAMP_FORMAT=None # GRAPHRAG_INPUT_TEXT_COLUMN=\"text\" # GRAPHRAG_INPUT_ATTRIBUTE_COLUMNS=id # GRAPHRAG_INPUT_TITLE_COLUMN=\"title\" # GRAPHRAG_INPUT_TYPE=\"file\" # GRAPHRAG_INPUT_CONNECTION_STRING=None # GRAPHRAG_INPUT_CONTAINER_NAME=None # GRAPHRAG_INPUT_BASE_DIR=None # Base LLM Settings GRAPHRAG_API_KEY = \"your_api_key\" GRAPHRAG_API_BASE = \"http://<domain>.openai.azure.com\" # For Azure OpenAI Users GRAPHRAG_API_VERSION = \"api_version\" # For Azure OpenAI Users # GRAPHRAG_API_ORGANIZATION=None # GRAPHRAG_API_PROXY=None # Text Generation Settings # GRAPHRAG_LLM_TYPE=openai_chat GRAPHRAG_LLM_API_KEY = \"your_api_key\" # If GRAPHRAG_API_KEY is not set GRAPHRAG_LLM_API_BASE = \"http://<domain>.openai.azure.com\" # For Azure OpenAI Users and if GRAPHRAG_API_BASE is not set GRAPHRAG_LLM_API_VERSION = \"api_version\" # For Azure OpenAI Users and if GRAPHRAG_API_VERSION is not set GRAPHRAG_LLM_MODEL_SUPPORTS_JSON = True # Suggested by default # GRAPHRAG_LLM_API_ORGANIZATION=None # GRAPHRAG_LLM_API_PROXY=None # GRAPHRAG_LLM_DEPLOYMENT_NAME=None # GRAPHRAG_LLM_MODEL=gpt-4-turbo-preview # GRAPHRAG_LLM_MAX_TOKENS=4000 # GRAPHRAG_LLM_REQUEST_TIMEOUT=180 # GRAPHRAG_LLM_THREAD_COUNT=50 # GRAPHRAG_LLM_THREAD_STAGGER=0.3 # GRAPHRAG_LLM_CONCURRENT_REQUESTS=25 # GRAPHRAG_LLM_TPM=0 # GRAPHRAG_LLM_RPM=0 # GRAPHRAG_LLM_MAX_RETRIES=10 # GRAPHRAG_LLM_MAX_RETRY_WAIT=10 # GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION=True # Text Embedding Settings # GRAPHRAG_EMBEDDING_TYPE=openai_embedding GRAPHRAG_EMBEDDING_API_KEY = \"your_api_key\" # If GRAPHRAG_API_KEY is not set GRAPHRAG_EMBEDDING_API_BASE = \"http://<domain>.openai.azure.com\" # For Azure OpenAI Users and if GRAPHRAG_API_BASE is not set GRAPHRAG_EMBEDDING_API_VERSION = \"api_version\" # For Azure OpenAI Users and if GRAPHRAG_API_VERSION is not set # GRAPHRAG_EMBEDDING_API_ORGANIZATION=None # GRAPHRAG_EMBEDDING_API_PROXY=None # GRAPHRAG_EMBEDDING_DEPLOYMENT_NAME=None # GRAPHRAG_EMBEDDING_MODEL=text-embedding-3-small # GRAPHRAG_EMBEDDING_BATCH_SIZE=16 # GRAPHRAG_EMBEDDING_BATCH_MAX_TOKENS=8191 # GRAPHRAG_EMBEDDING_TARGET=required # GRAPHRAG_EMBEDDING_SKIP=None # GRAPHRAG_EMBEDDING_THREAD_COUNT=None # GRAPHRAG_EMBEDDING_THREAD_STAGGER=50 # GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS=25 # GRAPHRAG_EMBEDDING_TPM=0 # GRAPHRAG_EMBEDDING_RPM=0 # GRAPHRAG_EMBEDDING_MAX_RETRIES=10 # GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT=10 # GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION=True # Data Mapping Settings # GRAPHRAG_INPUT_ENCODING=utf-8 # Data Chunking # GRAPHRAG_CHUNK_SIZE=1200 # GRAPHRAG_CHUNK_OVERLAP=100 # GRAPHRAG_CHUNK_BY_COLUMNS=id # Prompting Overrides # GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE=None # GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS=1 # GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES=organization,person,event,geo # GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE=None # GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH=500 # GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION=\"Any claims or facts that could be relevant to threat analysis.\" # GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE=None # GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS=1 # GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE=None # GRAPHRAG_COMMUNITY_REPORT_MAX_LENGTH=1500 # Storage # GRAPHRAG_STORAGE_TYPE=file # GRAPHRAG_STORAGE_CONNECTION_STRING=None # GRAPHRAG_STORAGE_CONTAINER_NAME=None # GRAPHRAG_STORAGE_BASE_DIR=None # Cache # GRAPHRAG_CACHE_TYPE=file # GRAPHRAG_CACHE_CONNECTION_STRING=None # GRAPHRAG_CACHE_CONTAINER_NAME=None # GRAPHRAG_CACHE_BASE_DIR=None # Reporting # GRAPHRAG_REPORTING_TYPE=file # GRAPHRAG_REPORTING_CONNECTION_STRING=None # GRAPHRAG_REPORTING_CONTAINER_NAME=None # GRAPHRAG_REPORTING_BASE_DIR=None # Node2Vec Parameters # GRAPHRAG_NODE2VEC_ENABLED=False # GRAPHRAG_NODE2VEC_NUM_WALKS=10 # GRAPHRAG_NODE2VEC_WALK_LENGTH=40 # GRAPHRAG_NODE2VEC_WINDOW_SIZE=2 # GRAPHRAG_NODE2VEC_ITERATIONS=3 # GRAPHRAG_NODE2VEC_RANDOM_SEED=597832 # Data Snapshotting # GRAPHRAG_SNAPSHOT_GRAPHML=False # GRAPHRAG_SNAPSHOT_RAW_ENTITIES=False # GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES=False # Miscellaneous Settings # GRAPHRAG_ASYNC_MODE=asyncio # GRAPHRAG_ENCODING_MODEL=cl100k_base # GRAPHRAG_MAX_CLUSTER_SIZE=10 # GRAPHRAG_ENTITY_RESOLUTION_ENABLED=False # GRAPHRAG_SKIP_WORKFLOWS=None # GRAPHRAG_UMAP_ENABLED=False Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/config/overview/", "text": "Configuring GraphRAG Indexing GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Configuring GraphRAG Indexing The GraphRAG system is highly configurable. This page provides an overview of the configuration options available for the GraphRAG indexing engine. Default Configuration Mode The default configuration mode is the simplest way to get started with the GraphRAG system. It is designed to work out-of-the-box with minimal configuration. The primary configuration sections for the Indexing Engine pipelines are described below. The main ways to set up GraphRAG in Default Configuration mode are via: Init command (recommended) Purely using environment variables Using JSON or YAML for deeper control Custom Configuration Mode Custom configuration mode is an advanced use-case. Most users will want to use the Default Configuration instead. The primary configuration sections for Indexing Engine pipelines are described below. Details about how to use custom configuration are available in the Custom Configuration Mode documentation. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/query/notebooks/global_search_nb/", "text": "GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License. '\\nCopyright (c) Microsoft Corporation.\\n' import os import pandas as pd import tiktoken from graphrag . query . indexer_adapters import read_indexer_entities , read_indexer_reports from graphrag . query . llm . oai . chat_openai import ChatOpenAI from graphrag . query . llm . oai . typing import OpenaiApiType from graphrag . query . structured_search . global_search . community_context import ( GlobalCommunityContext , ) from graphrag . query . structured_search . global_search . search import GlobalSearch Global Search example Global search method generates answers by searching over all AI-generated community reports in a map-reduce fashion. This is a resource-intensive method, but often gives good responses for questions that require an understanding of the dataset as a whole (e.g. What are the most significant values of the herbs mentioned in this notebook?). LLM setup api_key = os . environ [ \"GRAPHRAG_API_KEY\" ] llm_model = os . environ [ \"GRAPHRAG_LLM_MODEL\" ] llm = ChatOpenAI ( api_key = api_key , model = llm_model , api_type = OpenaiApiType . OpenAI , # OpenaiApiType.OpenAI or OpenaiApiType.AzureOpenAI max_retries = 20 , ) token_encoder = tiktoken . get_encoding ( \"cl100k_base\" ) Load community reports as context for global search Load all community reports in the create_final_community_reports table from the ire-indexing engine, to be used as context data for global search. Load entities from the create_final_nodes and create_final_entities tables from the ire-indexing engine, to be used for calculating community weights for context ranking. Note that this is optional (if no entities are provided, we will not calculate community weights and only use the rank attribute in the community reports table for context ranking) # parquet files generated from indexing pipeline INPUT_DIR = \"./inputs/operation dulce\" COMMUNITY_REPORT_TABLE = \"create_final_community_reports\" ENTITY_TABLE = \"create_final_nodes\" ENTITY_EMBEDDING_TABLE = \"create_final_entities\" # community level in the Leiden community hierarchy from which we will load the community reports # higher value means we use reports from more fine-grained communities (at the cost of higher computation cost) COMMUNITY_LEVEL = 2 entity_df = pd . read_parquet ( f\" { INPUT_DIR } / { ENTITY_TABLE } .parquet\" ) report_df = pd . read_parquet ( f\" { INPUT_DIR } / { COMMUNITY_REPORT_TABLE } .parquet\" ) entity_embedding_df = pd . read_parquet ( f\" { INPUT_DIR } / { ENTITY_EMBEDDING_TABLE } .parquet\" ) reports = read_indexer_reports ( report_df , entity_df , COMMUNITY_LEVEL ) entities = read_indexer_entities ( entity_df , entity_embedding_df , COMMUNITY_LEVEL ) print ( f\"Total report count: { len ( report_df ) } \" ) print ( f\"Report count after filtering by community level { COMMUNITY_LEVEL } : { len ( reports ) } \" ) report_df . head ( ) Build global context based on community reports context_builder = GlobalCommunityContext ( community_reports = reports , entities = entities , # default to None if you don't want to use community weights for ranking token_encoder = token_encoder , ) Perform global search context_builder_params = { \"use_community_summary\" : False , # False means using full community reports. True means using community short summaries. \"shuffle_data\" : True , \"include_community_rank\" : True , \"min_community_rank\" : 0 , \"community_rank_name\" : \"rank\" , \"include_community_weight\" : True , \"community_weight_name\" : \"occurrence weight\" , \"normalize_community_weight\" : True , \"max_tokens\" : 12_000 , # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000) \"context_name\" : \"Reports\" , } map_llm_params = { \"max_tokens\" : 1000 , \"temperature\" : 0.0 , \"response_format\" : { \"type\" : \"json_object\" } , } reduce_llm_params = { \"max_tokens\" : 2000 , # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000-1500) \"temperature\" : 0.0 , } search_engine = GlobalSearch ( llm = llm , context_builder = context_builder , token_encoder = token_encoder , max_data_tokens = 12_000 , # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000) map_llm_params = map_llm_params , reduce_llm_params = reduce_llm_params , allow_general_knowledge = False , # set this to True will add instruction to encourage the LLM to incorporate general knowledge in the response, which may increase hallucinations, but could be useful in some use cases. json_mode = True , # set this to False if your LLM model does not support JSON mode. context_builder_params = context_builder_params , concurrent_coroutines = 32 , response_type = \"multiple paragraphs\" , # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report ) result = await search_engine . asearch ( \"What is the major conflict in this story and who are the protagonist and antagonist?\" ) print ( result . response ) # inspect the data used to build the context for the LLM responses result . context_data [ \"reports\" ] # inspect number of LLM calls and tokens print ( f\"LLM calls: { result . llm_calls } . LLM tokens: { result . prompt_tokens } \" ) Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/index/2-cli/", "text": "Indexer CLI GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Indexer CLI The GraphRAG indexer CLI allows for no-code usage of the GraphRAG Indexer. python -m graphrag.index --verbose --root < /workspace/project/root > --config < custom_config.yml > --resume < timestamp > --reporter < rich | print | none > --emit json,csv,parquet --nocache CLI Arguments --verbose - Adds extra logging information during the run. --root <data-project-dir> - the data root directory. This should contain an input directory with the input data, and an .env file with environment variables. These are described below. --init - This will initialize the data project directory at the specified root with bootstrap configuration and prompt-overrides. --resume <output-timestamp> - if specified, the pipeline will attempt to resume a prior run. The parquet files from the prior run will be loaded into the system as inputs, and the workflows that generated those files will be skipped. The input value should be the timestamped output folder, e.g. \"20240105-143721\". --config <config_file.yml> - This will opt-out of the Default Configuration mode and execute a custom configuration. If this is used, then none of the environment-variables below will apply. --reporter <reporter> - This will specify the progress reporter to use. The default is rich . Valid values are rich , print , and none . --emit <types> - This specifies the table output formats the pipeline should emit. The default is parquet . Valid values are parquet , csv , and json , comma-separated. --nocache - This will disable the caching mechanism. This is useful for debugging and development, but should not be used in production. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/", "text": "Indexing Dataflow GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Indexing Dataflow The GraphRAG Knowledge Model The knowledge model is a specification for data outputs that conform to our data-model definition. You can find these definitions in the python/graphrag/graphrag/model folder within the GraphRAG repository. The following entity types are provided. The fields here represent the fields that are text-embedded by default. Document - An input document into the system. These either represent individual rows in a CSV or individual .txt file. TextUnit - A chunk of text to analyze. The size of these chunks, their overlap, and whether they adhere to any data boundaries may be configured below. A common use case is to set CHUNK_BY_COLUMNS to id so that there is a 1-to-many relationship between documents and TextUnits instead of a many-to-many. Entity - An entity extracted from a TextUnit. These represent people, places, events, or some other entity-model that you provide. Relationship - A relationship between two entities. These are generated from the covariates. Covariate - Extracted claim information, which contains statements about entities which may be time-bound. Community Report - Once entities are generated, we perform hierarchical community detection on them and generate reports for each community in this hierarchy. Node - This table contains layout information for rendered graph-views of the Entities and Documents which have been embedded and clustered. The Default Configuration Workflow Let's take a look at how the default-configuration workflow transforms text documents into the GraphRAG Knowledge Model . This page gives a general overview of the major steps in this process. To fully configure this workflow, check out the configuration documentation. ---\ntitle: Dataflow Overview\n---\nflowchart TB\n    subgraph phase1[Phase 1: Compose TextUnits]\n    documents[Documents] --> chunk[Chunk]\n    chunk --> embed[Embed] --> textUnits[Text Units]\n    end\n    subgraph phase2[Phase 2: Graph Extraction]\n    textUnits --> graph_extract[Entity & Relationship Extraction]\n    graph_extract --> graph_summarize[Entity & Relationship Summarization]\n    graph_summarize --> entity_resolve[Entity Resolution]\n    entity_resolve --> claim_extraction[Claim Extraction]\n    claim_extraction --> graph_outputs[Graph Tables]\n    end\n    subgraph phase3[Phase 3: Graph Augmentation]\n    graph_outputs --> community_detect[Community Detection]\n    community_detect --> graph_embed[Graph Embedding]\n    graph_embed --> augmented_graph[Augmented Graph Tables]\n    end\n    subgraph phase4[Phase 4: Community Summarization]\n    augmented_graph --> summarized_communities[Community Summarization]\n    summarized_communities --> embed_communities[Community Embedding]\n    embed_communities --> community_outputs[Community Tables]\n    end\n    subgraph phase5[Phase 5: Document Processing]\n    documents --> link_to_text_units[Link to TextUnits]\n    textUnits --> link_to_text_units\n    link_to_text_units --> embed_documents[Document Embedding]\n    embed_documents --> document_graph[Document Graph Creation]\n    document_graph --> document_outputs[Document Tables]\n    end\n    subgraph phase6[Phase 6: Network Visualization]\n    document_outputs --> umap_docs[Umap Documents]\n    augmented_graph --> umap_entities[Umap Entities]\n    umap_docs --> combine_nodes[Nodes Table]\n    umap_entities --> combine_nodes\n    end Phase 1: Compose TextUnits The first phase of the default-configuration workflow is to transform input documents into TextUnits . A TextUnit is a chunk of text that is used for our graph extraction techniques. They are also used as source-references by extracted knowledge items in order to empower breadcrumbs and provenance by concepts back to their original source tex. The chunk size (counted in tokens), is user-configurable. By default this is set to 300 tokens, although we've had positive experience with 1200-token chunks using a single \"glean\" step. (A \"glean\" step is a follow-on extraction). Larger chunks result in lower-fidelity output and less meaningful reference texts; however, using larger chunks can result in much faster processing time. The group-by configuration is also user-configurable. By default, we align our chunks to document boundaries, meaning that there is a strict 1-to-many relationship between Documents and TextUnits. In rare cases, this can be turned into a many-to-many relationship. This is useful when the documents are very short and we need several of them to compose a meaningful analysis unit (e.g. Tweets or a chat log) Each of these text-units are text-embedded and passed into the next phase of the pipeline. ---\ntitle: Documents into Text Chunks\n---\nflowchart LR\n    doc1[Document 1] --> tu1[TextUnit 1]\n    doc1 --> tu2[TextUnit 2]\n    doc2[Document 2] --> tu3[TextUnit 3]\n    doc2 --> tu4[TextUnit 4] Phase 2: Graph Extraction In this phase, we analyze each text unit and extract our graph primitives: Entities , Relationships , and Claims .\nEntities and Relationships are extracted at once in our entity_extract verb, and claims are extracted in our claim_extract verb. Results are then combined and passed into following phases of the pipeline. ---\ntitle: Graph Extraction\n---\nflowchart LR\n    tu[TextUnit] --> ge[Graph Extraction] --> gs[Graph Summarization] --> er[Entity Resolution]\n    tu --> ce[Claim Extraction] Entity & Relationship Extraction In this first step of graph extraction, we process each text-unit in order to extract entities and relationships out of the raw text using the LLM. The output of this step is a subgraph-per-TextUnit containing a list of entities with a name , type , and description , and a list of relationships with a source , target , and description . These subgraphs are merged together - any entities with the same name and type are merged by creating an array of their descriptions. Similarly, any relationships with the same source and target are merged by creating an array of their descriptions. Entity & Relationship Summarization Now that we have a graph of entities and relationships, each with a list of descriptions, we can summarize these lists into a single description per entity and relationship. This is done by asking the LLM for a short summary that captures all of the distinct information from each description. This allows all of our entities and relationships to have a single concise description. Entity Resolution (Not Enabled by Default) The final step of graph extraction is to resolve any entities that represent the same real-world entity but but have different names. Since this is done via LLM, and we don't want to lose information, we want to take a conservative, non-destructive approach to this. Our current implementation of Entity Resolution, however, is destructive. It will provide the LLM with a series of entities and ask it to determine which ones should be merged. Those entities are then merged together into a single entity and their relationships are updated. We are currently exploring other entity resolution techniques. In the near future, entity resolution will be executed by creating an edge between entity variants indicating that the entities have been resolved by the indexing engine. This will allow for end-users to undo indexing-side resolutions, and add their own non-destructive resolutions using a similar process. Claim Extraction & Emission Finally, as an independent workflow, we extract claims from the source TextUnits. These claims represent positive factual statements with an evaluated status and time-bounds. These are emitted as a primary artifact called Covariates . Phase 3: Graph Augmentation Now that we have a usable graph of entities and relationships, we want to understand their community structure and augment the graph with additional information. This is done in two steps: Community Detection and Graph Embedding . These give us explicit (communities) and implicit (embeddings) ways of understanding the topological structure of our graph. ---\ntitle: Graph Augmentation\n---\nflowchart LR\n    cd[Leiden Hierarchical Community Detection] --> ge[Node2Vec Graph Embedding] --> ag[Graph Table Emission] Community Detection In this step, we generate a hierarchy of entity communities using the Hierarchical Leiden Algorithm. This method will apply a recursive community-clustering to our graph until we reach a community-size threshold. This will allow us to understand the community structure of our graph and provide a way to navigate and summarize the graph at different levels of granularity. Graph Embedding In this step, we generate a vector representation of our graph using the Node2Vec algorithm. This will allow us to understand the implicit structure of our graph and provide an additional vector-space in which to search for related concepts during our query phase. Graph Tables Emission Once our graph augmentation steps are complete, the final Entities and Relationships tables are emitted after their text fields are text-embedded. Phase 4: Community Summarization ---\ntitle: Community Summarization\n---\nflowchart LR\n    sc[Generate Community Reports] --> ss[Summarize Community Reports] --> ce[Community Embedding] --> co[Community Tables Emission] At this point, we have a functional graph of entities and relationships, a hierarchy of communities for the entities, as well as node2vec embeddings. Now we want to build on the communities data and generate reports for each community. This gives us a high-level understanding of the graph at several points of graph granularity. For example, if community A is the top-level community, we'll get a report about the entire graph. If the community is lower-level, we'll get a report about a local cluster. Generate Community Reports In this step, we generate a summary of each community using the LLM. This will allow us to understand the distinct information contained within each community and provide a scoped understanding of the graph, from either a high-level or a low-level perspective. These reports contain an executive overview and reference the key entities, relationships, and claims within the community sub-structure. Summarize Community Reports In this step, each community report is then summarized via the LLM for shorthand use. Community Embedding In this step, we generate a vector representation of our communities by generating text embeddings of the community report, the community report summary, and the title of the community report. Community Tables Emission At this point, some bookkeeping work is performed and we emit the Communities and CommunityReports tables. Phase 5: Document Processing In this phase of the workflow, we create the Documents table for the knowledge model. ---\ntitle: Document Processing\n---\nflowchart LR\n    aug[Augment] --> dp[Link to TextUnits] --> de[Avg. Embedding] --> dg[Document Table Emission] Augment with Columns (CSV Only) If the workflow is operating on CSV data, you may configure your workflow to add additional fields to Documents output. These fields should exist on the incoming CSV tables. Details about configuring this can be found in the configuration documentation . Link to TextUnits In this step, we link each document to the text-units that were created in the first phase. This allows us to understand which documents are related to which text-units and vice-versa. Document Embedding In this step, we generate a vector representation of our documents using an average embedding of document slices. We re-chunk documents without overlapping chunks, and then generate an embedding for each chunk. We create an average of these chunks weighted by token-count and use this as the document embedding. This will allow us to understand the implicit relationship between documents, and will help us generate a network representation of our documents. Documents Table Emission At this point, we can emit the Documents table into the knowledge Model. Phase 6: Network Visualization In this phase of the workflow, we perform some steps to support network visualization of our high-dimensional vector spaces within our existing graphs. At this point there are two logical graphs at play: the Entity-Relationship graph and the Document graph. ---\ntitle: Network Visualization Workflows\n---\nflowchart LR\n    nv[Umap Documents] --> ne[Umap Entities] --> ng[Nodes Table Emission] For each of the logical graphs, we perform a UMAP dimensionality reduction to generate a 2D representation of the graph. This will allow us to visualize the graph in a 2D space and understand the relationships between the nodes in the graph. The UMAP embeddings are then emitted as a table of Nodes . The rows of this table include a discriminator indicating whether the node is a document or an entity, and the UMAP coordinates. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/config/custom/", "text": "Custom Configuration Mode GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Custom Configuration Mode The primary configuration sections for Indexing Engine pipelines are described below. Each configuration section can be expressed in Python (for use in Python API mode) as well as YAML, but YAML is show here for brevity. Using custom configuration is an advanced use-case. Most users will want to use the Default Configuration instead. Indexing Engine Examples The examples directory contains several examples of how to use the indexing engine with custom configuration . Most examples include two different forms of running the pipeline, both are contained in the examples run.py Using mostly the Python API Using mostly the a pipeline configuration file To run an example: Run poetry shell to activate a virtual environment with the required dependencies. Run PYTHONPATH=\"$(pwd)\" python examples/path_to_example/run.py from the root directory. For example to run the single_verb example, you would run the following commands: poetry shell PYTHONPATH = \" $( pwd ) \" python examples/single_verb/run.py Configuration Sections > extends This configuration allows you to extend a base configuration file or files. # single base extends : ../base_config.yml # multiple bases extends : - ../base_config.yml - ../base_config2.yml > root_dir This configuration allows you to set the root directory for the pipeline. All data inputs and outputs are assumed to be relative to this path. root_dir : /workspace/data_project > storage This configuration allows you define the output strategy for the pipeline. type : The type of storage to use. Options are file , memory , and blob base_dir ( type: file only): The base directory to store the data in. This is relative to the config root. connection_string ( type: blob only): The connection string to use for blob storage. container_name ( type: blob only): The container to use for blob storage. > cache This configuration allows you define the cache strategy for the pipeline. type : The type of cache to use. Options are file and memory , and blob . base_dir ( type: file only): The base directory to store the cache in. This is relative to the config root. connection_string ( type: blob only): The connection string to use for blob storage. container_name ( type: blob only): The container to use for blob storage. > reporting This configuration allows you define the reporting strategy for the pipeline. Report files are generated artifacts that summarize the performance metrics of the pipeline and emit any error messages. type : The type of reporting to use. Options are file , memory , and blob base_dir ( type: file only): The base directory to store the reports in. This is relative to the config root. connection_string ( type: blob only): The connection string to use for blob storage. container_name ( type: blob only): The container to use for blob storage. > workflows This configuration section defines the workflow DAG for the pipeline. Here we define an array of workflows and express their inter-dependencies in steps: name : The name of the workflow. This is used to reference the workflow in other parts of the config. steps : The DataShaper steps that this workflow comprises. If a step defines an input in the form of workflow:<workflow_name> , then it is assumed to have a dependency on the output of that workflow. workflows : - name : workflow1 steps : - verb : derive args : column1 : \"col1\" column2 : \"col2\" - name : workflow2 steps : - verb : derive args : column1 : \"col1\" column2 : \"col2\" input : # dependency established here source : workflow : workflow1 > input type : The type of input to use. Options are file or blob . file_type : The file type field discriminates between the different input types. Options are csv and text . base_dir : The base directory to read the input files from. This is relative to the config file. file_pattern : A regex to match the input files. The regex must have named groups for each of the fields in the file_filter. post_process : A DataShaper workflow definition to apply to the input before executing the primary workflow. source_column ( type: csv only): The column containing the source/author of the data text_column ( type: csv only): The column containing the text of the data timestamp_column ( type: csv only): The column containing the timestamp of the data timestamp_format ( type: csv only): The format of the timestamp input : type : file file_type : csv base_dir : ../data/csv # the directory containing the CSV files, this is relative to the config file file_pattern : '.*[\\/](?P<source>[^\\/]+)[\\/](?P<year>\\d{4})-(?P<month>\\d{2})-(?P<day>\\d{2})_(?P<author>[^_]+)_\\d+\\.csv$' # a regex to match the CSV files # An additional file filter which uses the named groups from the file_pattern to further filter the files # file_filter: #   # source: (source_filter) #   year: (2023) #   month: (06) #   # day: (22) source_column : \"author\" # the column containing the source/author of the data text_column : \"message\" # the column containing the text of the data timestamp_column : \"date(yyyyMMddHHmmss)\" # optional, the column containing the timestamp of the data timestamp_format : \"%Y%m%d%H%M%S\" # optional,  the format of the timestamp post_process : # Optional, set of steps to process the data before going into the workflow - verb : filter args : column : \"title\" , value : \"My document\" input : type : file file_type : csv base_dir : ../data/csv # the directory containing the CSV files, this is relative to the config file file_pattern : '.*[\\/](?P<source>[^\\/]+)[\\/](?P<year>\\d{4})-(?P<month>\\d{2})-(?P<day>\\d{2})_(?P<author>[^_]+)_\\d+\\.csv$' # a regex to match the CSV files # An additional file filter which uses the named groups from the file_pattern to further filter the files # file_filter: #   # source: (source_filter) #   year: (2023) #   month: (06) #   # day: (22) post_process : # Optional, set of steps to process the data before going into the workflow - verb : filter args : column : \"title\" , value : \"My document\" Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/config/env_vars/", "text": "Default Configuration Mode (using Env Vars) GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Default Configuration Mode (using Env Vars) Text-Embeddings Customization By default, the GraphRAG indexer will only emit embeddings required for our query methods. However, the model has embeddings defined for all plaintext fields, and these can be generated by setting the GRAPHRAG_EMBEDDING_TARGET environment variable to all . If the embedding target is all , and you want to only embed a subset of these fields, you may specify which embeddings to skip using the GRAPHRAG_EMBEDDING_SKIP argument described below. Embedded Fields text_unit.text document.raw_content entity.name entity.description relationship.description community.title community.summary community.full_content Input Data Our pipeline can ingest .csv or .txt data from an input folder. These files can be nested within subfolders. To configure how input data is handled, what fields are mapped over, and how timestamps are parsed, look for configuration values starting with GRAPHRAG_INPUT_ below. In general, CSV-based data provides the most customizeability. Each CSV should at least contain a text field (which can be mapped with environment variables), but it's helpful if they also have title , timestamp , and source fields. Additional fields can be included as well, which will land as extra fields on the Document table. Base LLM Settings These are the primary settings for configuring LLM connectivity. Parameter Required? Description Type Default Value GRAPHRAG_API_KEY Yes for OpenAI. Optional for AOAI The API key. (Note: `OPENAI_API_KEY is also used as a fallback). If not defined when using AOAI, managed identity will be used. str None GRAPHRAG_API_BASE For AOAI The API Base URL str None GRAPHRAG_API_VERSION For AOAI The AOAI API version. str None GRAPHRAG_API_ORGANIZATION The AOAI organization. str None GRAPHRAG_API_PROXY The AOAI proxy. str None Text Generation Settings These settings control the text generation model used by the pipeline. Any settings with a fallback will use the base LLM settings, if available. Parameter Required? Description Type Default Value GRAPHRAG_LLM_TYPE For AOAI The LLM operation type. Either openai_chat or azure_openai_chat str openai_chat GRAPHRAG_LLM_DEPLOYMENT_NAME For AOAI The AOAI model deployment name. str None GRAPHRAG_LLM_API_KEY Yes (uses fallback) The API key. If not defined when using AOAI, managed identity will be used. str None GRAPHRAG_LLM_API_BASE For AOAI (uses fallback) The API Base URL str None GRAPHRAG_LLM_API_VERSION For AOAI (uses fallback) The AOAI API version. str None GRAPHRAG_LLM_API_ORGANIZATION For AOAI (uses fallback) The AOAI organization. str None GRAPHRAG_LLM_API_PROXY The AOAI proxy. str None GRAPHRAG_LLM_MODEL The LLM model. str gpt-4-turbo-preview GRAPHRAG_LLM_MAX_TOKENS The maximum number of tokens. int 4000 GRAPHRAG_LLM_REQUEST_TIMEOUT The maximum number of seconds to wait for a response from the chat client. int 180 GRAPHRAG_LLM_MODEL_SUPPORTS_JSON Indicates whether the given model supports JSON output mode. True to enable. str None GRAPHRAG_LLM_THREAD_COUNT The number of threads to use for LLM parallelization. int 50 GRAPHRAG_LLM_THREAD_STAGGER The time to wait (in seconds) between starting each thread. float 0.3 GRAPHRAG_LLM_CONCURRENT_REQUESTS The number of concurrent requests to allow for the embedding client. int 25 GRAPHRAG_LLM_TOKENS_PER_MINUTE The number of tokens per minute to allow for the LLM client. 0 = Bypass int 0 GRAPHRAG_LLM_REQUESTS_PER_MINUTE The number of requests per minute to allow for the LLM client. 0 = Bypass int 0 GRAPHRAG_LLM_MAX_RETRIES The maximum number of retries to attempt when a request fails. int 10 GRAPHRAG_LLM_MAX_RETRY_WAIT The maximum number of seconds to wait between retries. int 10 GRAPHRAG_LLM_SLEEP_ON_RATE_LIMIT_RECOMMENDATION Whether to sleep on rate limit recommendation. (Azure Only) bool True GRAPHRAG_LLM_TEMPERATURE The temperature to use generation. float 0 GRAPHRAG_LLM_TOP_P The top_p to use for sampling. float 1 GRAPHRAG_LLM_N The number of responses to generate. int 1 Text Embedding Settings These settings control the text embedding model used by the pipeline. Any settings with a fallback will use the base LLM settings, if available. Parameter Required ? Description Type Default GRAPHRAG_EMBEDDING_TYPE For AOAI The embedding client to use. Either openai_embedding or azure_openai_embedding str openai_embedding GRAPHRAG_EMBEDDING_DEPLOYMENT_NAME For AOAI The AOAI deployment name. str None GRAPHRAG_EMBEDDING_API_KEY Yes (uses fallback) The API key to use for the embedding client. If not defined when using AOAI, managed identity will be used. str None GRAPHRAG_EMBEDDING_API_BASE For AOAI (uses fallback) The API base URL. str None GRAPHRAG_EMBEDDING_API_VERSION For AOAI (uses fallback) The AOAI API version to use for the embedding client. str None GRAPHRAG_EMBEDDING_API_ORGANIZATION For AOAI (uses fallback) The AOAI organization to use for the embedding client. str None GRAPHRAG_EMBEDDING_API_PROXY The AOAI proxy to use for the embedding client. str None GRAPHRAG_EMBEDDING_MODEL The model to use for the embedding client. str text-embedding-3-small GRAPHRAG_EMBEDDING_BATCH_SIZE The number of texts to embed at once. (Azure limit is 16) int 16 GRAPHRAG_EMBEDDING_BATCH_MAX_TOKENS The maximum tokens per batch (Azure limit is 8191) int 8191 GRAPHRAG_EMBEDDING_TARGET The target fields to embed. Either required or all . str required GRAPHRAG_EMBEDDING_SKIP A comma-separated list of fields to skip embeddings for . (e.g. 'relationship.description') str None GRAPHRAG_EMBEDDING_THREAD_COUNT The number of threads to use for parallelization for embeddings. int GRAPHRAG_EMBEDDING_THREAD_STAGGER The time to wait (in seconds) between starting each thread for embeddings. float 50 GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS The number of concurrent requests to allow for the embedding client. int 25 GRAPHRAG_EMBEDDING_TOKENS_PER_MINUTE The number of tokens per minute to allow for the embedding client. 0 = Bypass int 0 GRAPHRAG_EMBEDDING_REQUESTS_PER_MINUTE The number of requests per minute to allow for the embedding client. 0 = Bypass int 0 GRAPHRAG_EMBEDDING_MAX_RETRIES The maximum number of retries to attempt when a request fails. int 10 GRAPHRAG_EMBEDDING_MAX_RETRY_WAIT The maximum number of seconds to wait between retries. int 10 GRAPHRAG_EMBEDDING_TARGET The target fields to embed. Either required or all . str required GRAPHRAG_EMBEDDING_SLEEP_ON_RATE_LIMIT_RECOMMENDATION Whether to sleep on rate limit recommendation. (Azure Only) bool True Input Settings These settings control the data input used by the pipeline. Any settings with a fallback will use the base LLM settings, if available. Plaintext Input Data ( GRAPHRAG_INPUT_FILE_TYPE =text) Parameter Description Type Required or Optional Default GRAPHRAG_INPUT_FILE_PATTERN The file pattern regexp to use when reading input files from the input directory. str optional .*\\.txt$ CSV Input Data ( GRAPHRAG_INPUT_FILE_TYPE =csv) Parameter Description Type Required or Optional Default GRAPHRAG_INPUT_TYPE The input storage type to use when reading files. ( file or blob ) str optional file GRAPHRAG_INPUT_FILE_PATTERN The file pattern regexp to use when reading input files from the input directory. str optional .*\\.txt$ GRAPHRAG_INPUT_SOURCE_COLUMN The 'source' column to use when reading CSV input files. str optional source GRAPHRAG_INPUT_TIMESTAMP_COLUMN The 'timestamp' column to use when reading CSV input files. str optional None GRAPHRAG_INPUT_TIMESTAMP_FORMAT The timestamp format to use when parsing timestamps in the timestamp column. str optional None GRAPHRAG_INPUT_TEXT_COLUMN The 'text' column to use when reading CSV input files. str optional text GRAPHRAG_INPUT_DOCUMENT_ATTRIBUTE_COLUMNS A list of CSV columns, comma-separated, to incorporate as document fields. str optional id GRAPHRAG_INPUT_TITLE_COLUMN The 'title' column to use when reading CSV input files. str optional title GRAPHRAG_INPUT_STORAGE_ACCOUNT_BLOB_URL The Azure Storage blob endpoint to use when in blob mode and using managed identity. Will have the format https://<storage_account_name>.blob.core.windows.net str optional None GRAPHRAG_INPUT_CONNECTION_STRING The connection string to use when reading CSV input files from Azure Blob Storage. str optional None GRAPHRAG_INPUT_CONTAINER_NAME The container name to use when reading CSV input files from Azure Blob Storage. str optional None GRAPHRAG_INPUT_BASE_DIR The base directory to read input files from. str optional None Data Mapping Settings Parameter Description Type Required or Optional Default GRAPHRAG_INPUT_FILE_TYPE The type of input data, csv or text str optional text GRAPHRAG_INPUT_ENCODING The encoding to apply when reading CSV/text input files. str optional utf-8 Data Chunking Parameter Description Type Required or Optional Default GRAPHRAG_CHUNK_SIZE The chunk size in tokens for text-chunk analysis windows. str optional 1200 GRAPHRAG_CHUNK_OVERLAP The chunk overlap in tokens for text-chunk analysis windows. str optional 100 GRAPHRAG_CHUNK_BY_COLUMNS A comma-separated list of document attributes to groupby when performing TextUnit chunking. str optional id GRAPHRAG_CHUNK_ENCODING_MODEL The encoding model to use for chunking. str optional The top-level encoding model. Prompting Overrides Parameter Description Type Required or Optional Default GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE The path (relative to the root) of an entity extraction prompt template text file. str optional None GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS The maximum number of redrives (gleanings) to invoke when extracting entities in a loop. int optional 1 GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES A comma-separated list of entity types to extract. str optional organization,person,event,geo GRAPHRAG_ENTITY_EXTRACTION_ENCODING_MODEL The encoding model to use for entity extraction. str optional The top-level encoding model. GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE The path (relative to the root) of an description summarization prompt template text file. str optional None GRAPHRAG_SUMMARIZE_DESCRIPTIONS_MAX_LENGTH The maximum number of tokens to generate per description summarization. int optional 500 GRAPHRAG_CLAIM_EXTRACTION_ENABLED Whether claim extraction is enabled for this pipeline. bool optional False GRAPHRAG_CLAIM_EXTRACTION_DESCRIPTION The claim_description prompting argument to utilize. string optional \"Any claims or facts that could be relevant to threat analysis.\" GRAPHRAG_CLAIM_EXTRACTION_PROMPT_FILE The claim extraction prompt to utilize. string optional None GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS The maximum number of redrives (gleanings) to invoke when extracting claims in a loop. int optional 1 GRAPHRAG_CLAIM_EXTRACTION_ENCODING_MODEL The encoding model to use for claim extraction. str optional The top-level encoding model GRAPHRAG_COMMUNITY_REPORTS_PROMPT_FILE The community reports extraction prompt to utilize. string optional None GRAPHRAG_COMMUNITY_REPORTS_MAX_LENGTH The maximum number of tokens to generate per community reports. int optional 1500 Storage This section controls the storage mechanism used by the pipeline used for emitting output tables. Parameter Description Type Required or Optional Default GRAPHRAG_STORAGE_TYPE The type of reporter to use. Options are file , memory , or blob str optional file GRAPHRAG_STORAGE_STORAGE_ACCOUNT_BLOB_URL The Azure Storage blob endpoint to use when in blob mode and using managed identity. Will have the format https://<storage_account_name>.blob.core.windows.net str optional None GRAPHRAG_STORAGE_CONNECTION_STRING The Azure Storage connection string to use when in blob mode. str optional None GRAPHRAG_STORAGE_CONTAINER_NAME The Azure Storage container name to use when in blob mode. str optional None GRAPHRAG_STORAGE_BASE_DIR The base path to data outputs outputs. str optional None Cache This section controls the cache mechanism used by the pipeline. This is used to cache LLM invocation results. Parameter Description Type Required or Optional Default GRAPHRAG_CACHE_TYPE The type of cache to use. Options are file , memory , none or blob str optional file GRAPHRAG_CACHE_STORAGE_ACCOUNT_BLOB_URL The Azure Storage blob endpoint to use when in blob mode and using managed identity. Will have the format https://<storage_account_name>.blob.core.windows.net str optional None GRAPHRAG_CACHE_CONNECTION_STRING The Azure Storage connection string to use when in blob mode. str optional None GRAPHRAG_CACHE_CONTAINER_NAME The Azure Storage container name to use when in blob mode. str optional None GRAPHRAG_CACHE_BASE_DIR The base path to the reporting outputs. str optional None Reporting This section controls the reporting mechanism used by the pipeline, for common events and error messages. The default is to write reports to a file in the output directory. However, you can also choose to write reports to the console or to an Azure Blob Storage container. Parameter Description Type Required or Optional Default GRAPHRAG_REPORTING_TYPE The type of reporter to use. Options are file , console , or blob str optional file GRAPHRAG_REPORTING_STORAGE_ACCOUNT_BLOB_URL The Azure Storage blob endpoint to use when in blob mode and using managed identity. Will have the format https://<storage_account_name>.blob.core.windows.net str optional None GRAPHRAG_REPORTING_CONNECTION_STRING The Azure Storage connection string to use when in blob mode. str optional None GRAPHRAG_REPORTING_CONTAINER_NAME The Azure Storage container name to use when in blob mode. str optional None GRAPHRAG_REPORTING_BASE_DIR The base path to the reporting outputs. str optional None Node2Vec Parameters Parameter Description Type Required or Optional Default GRAPHRAG_NODE2VEC_ENABLED Whether to enable Node2Vec bool optional False GRAPHRAG_NODE2VEC_NUM_WALKS The Node2Vec number of walks to perform int optional 10 GRAPHRAG_NODE2VEC_WALK_LENGTH The Node2Vec walk length int optional 40 GRAPHRAG_NODE2VEC_WINDOW_SIZE The Node2Vec window size int optional 2 GRAPHRAG_NODE2VEC_ITERATIONS The number of iterations to run node2vec int optional 3 GRAPHRAG_NODE2VEC_RANDOM_SEED The random seed to use for node2vec int optional 597832 Data Snapshotting Parameter Description Type Required or Optional Default GRAPHRAG_SNAPSHOT_GRAPHML Whether to enable GraphML snapshots. bool optional False GRAPHRAG_SNAPSHOT_RAW_ENTITIES Whether to enable raw entity snapshots. bool optional False GRAPHRAG_SNAPSHOT_TOP_LEVEL_NODES Whether to enable top-level node snapshots. bool optional False Miscellaneous Settings Parameter Description Type Required or Optional Default GRAPHRAG_ASYNC_MODE Which async mode to use. Either asyncio or threaded . str optional asyncio GRAPHRAG_ENCODING_MODEL The text encoding model, used in tiktoken, to encode text. str optional cl100k_base GRAPHRAG_MAX_CLUSTER_SIZE The maximum number of entities to include in a single Leiden cluster. int optional 10 GRAPHRAG_SKIP_WORKFLOWS A comma-separated list of workflow names to skip. str optional None GRAPHRAG_UMAP_ENABLED Whether to enable UMAP layouts bool optional False Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/config/json_yaml/", "text": "Default Configuration Mode (using JSON/YAML) GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Default Configuration Mode (using JSON/YAML) The default configuration mode may be configured by using a config.json or config.yml file in the data project root. If a .env file is present along with this config file, then it will be loaded, and the environment variables defined therein will be available for token replacements in your configuration document using ${ENV_VAR} syntax. For example: # .env\nAPI_KEY=some_api_key\n\n# config.json\n{\n    \"llm\": {\n        \"api_key\": \"${API_KEY}\"\n    }\n} Config Sections input Fields type file|blob - The input type to use. Default= file file_type text|csv - The type of input data to load. Either text or csv . Default is text file_encoding str - The encoding of the input file. Default is utf-8 file_pattern str - A regex to match input files. Default is .*\\.csv$ if in csv mode and .*\\.txt$ if in text mode. source_column str - (CSV Mode Only) The source column name. timestamp_column str - (CSV Mode Only) The timestamp column name. timestamp_format str - (CSV Mode Only) The source format. text_column str - (CSV Mode Only) The text column name. title_column str - (CSV Mode Only) The title column name. document_attribute_columns list[str] - (CSV Mode Only) The additional document attributes to include. connection_string str - (blob only) The Azure Storage connection string. container_name str - (blob only) The Azure Storage container name. base_dir str - The base directory to read input from, relative to the root. storage_account_blob_url str - The storage account blob URL to use. llm This is the base LLM configuration section. Other steps may override this configuration with their own LLM configuration. Fields api_key str - The OpenAI API key to use. type openai_chat|azure_openai_chat|openai_embedding|azure_openai_embedding - The type of LLM to use. model str - The model name. max_tokens int - The maximum number of output tokens. request_timeout float - The per-request timeout. api_base str - The API base url to use. api_version str - The API version organization str - The client organization. proxy str - The proxy URL to use. cognitive_services_endpoint str - The url endpoint for cognitive services. deployment_name str - The deployment name to use (Azure). model_supports_json bool - Whether the model supports JSON-mode output. tokens_per_minute int - Set a leaky-bucket throttle on tokens-per-minute. requests_per_minute int - Set a leaky-bucket throttle on requests-per-minute. max_retries int - The maximum number of retries to use. max_retry_wait float - The maximum backoff time. sleep_on_rate_limit_recommendation bool - Whether to adhere to sleep recommendations (Azure). concurrent_requests int The number of open requests to allow at once. temperature float - The temperature to use. top_p float - The top-p value to use. n int - The number of completions to generate. parallelization Fields stagger float - The threading stagger value. num_threads int - The maximum number of work threads. async_mode asyncio|threaded The async mode to use. Either asyncio or `threaded. embeddings Fields llm (see LLM top-level config) parallelization (see Parallelization top-level config) async_mode (see Async Mode top-level config) batch_size int - The maximum batch size to use. batch_max_tokens int - The maximum batch #-tokens. target required|all - Determines which set of embeddings to emit. skip list[str] - Which embeddings to skip. strategy dict - Fully override the text-embedding strategy. chunks Fields size int - The max chunk size in tokens. overlap int - The chunk overlap in tokens. group_by_columns list[str] - group documents by fields before chunking. encoding_model str - The text encoding model to use. Default is to use the top-level encoding model. strategy dict - Fully override the chunking strategy. cache Fields type file|memory|none|blob - The cache type to use. Default= file connection_string str - (blob only) The Azure Storage connection string. container_name str - (blob only) The Azure Storage container name. base_dir str - The base directory to write cache to, relative to the root. storage_account_blob_url str - The storage account blob URL to use. storage Fields type file|memory|blob - The storage type to use. Default= file connection_string str - (blob only) The Azure Storage connection string. container_name str - (blob only) The Azure Storage container name. base_dir str - The base directory to write reports to, relative to the root. storage_account_blob_url str - The storage account blob URL to use. reporting Fields type file|console|blob - The reporting type to use. Default= file connection_string str - (blob only) The Azure Storage connection string. container_name str - (blob only) The Azure Storage container name. base_dir str - The base directory to write reports to, relative to the root. storage_account_blob_url str - The storage account blob URL to use. entity_extraction Fields llm (see LLM top-level config) parallelization (see Parallelization top-level config) async_mode (see Async Mode top-level config) prompt str - The prompt file to use. entity_types list[str] - The entity types to identify. max_gleanings int - The maximum number of gleaning cycles to use. encoding_model str - The text encoding model to use. By default, this will use the top-level encoding model. strategy dict - Fully override the entity extraction strategy. summarize_descriptions Fields llm (see LLM top-level config) parallelization (see Parallelization top-level config) async_mode (see Async Mode top-level config) prompt str - The prompt file to use. max_length int - The maximum number of output tokens per summarization. strategy dict - Fully override the summarize description strategy. claim_extraction Fields enabled bool - Whether to enable claim extraction. default=False llm (see LLM top-level config) parallelization (see Parallelization top-level config) async_mode (see Async Mode top-level config) prompt str - The prompt file to use. description str - Describes the types of claims we want to extract. max_gleanings int - The maximum number of gleaning cycles to use. encoding_model str - The text encoding model to use. By default, this will use the top-level encoding model. strategy dict - Fully override the claim extraction strategy. community_reports Fields llm (see LLM top-level config) parallelization (see Parallelization top-level config) async_mode (see Async Mode top-level config) prompt str - The prompt file to use. max_length int - The maximum number of output tokens per report. max_input_length int - The maximum number of input tokens to use when generating reports. strategy dict - Fully override the community reports strategy. cluster_graph Fields max_cluster_size int - The maximum cluster size to emit. strategy dict - Fully override the cluster_graph strategy. embed_graph Fields enabled bool - Whether to enable graph embeddings. num_walks int - The node2vec number of walks. walk_length int - The node2vec walk length. window_size int - The node2vec window size. iterations int - The node2vec number of iterations. random_seed int - The node2vec random seed. strategy dict - Fully override the embed graph strategy. umap Fields enabled bool - Whether to enable UMAP layouts. snapshots Fields graphml bool - Emit graphml snapshots. raw_entities bool - Emit raw entity snapshots. top_level_nodes bool - Emit top-level-node snapshots. encoding_model str - The text encoding model to use. Default is cl100k_base . skip_workflows list[str] - Which workflow names to skip. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/index/0-architecture/", "text": "Indexing Architecture GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Indexing Architecture Key Concepts Knowledge Model In order to support the GraphRAG system, the outputs of the indexing engine (in the Default Configuration Mode) are aligned to a knowledge model we call the GraphRAG Knowledge Model .\nThis model is designed to be an abstraction over the underlying data storage technology, and to provide a common interface for the GraphRAG system to interact with.\nIn normal use-cases the outputs of the GraphRAG Indexer would be loaded into a database system, and the GraphRAG's Query Engine would interact with the database using the knowledge model data-store types. DataShaper Workflows GraphRAG's Indexing Pipeline is built on top of our open-source library, DataShaper .\nDataShaper is a data processing library that allows users to declaratively express data pipelines, schemas, and related assets using well-defined schemas.\nDataShaper has implementations in JavaScript and Python, and is designed to be extensible to other languages. One of the core resource types within DataShaper is a Workflow .\nWorkflows are expressed as sequences of steps, which we call verbs .\nEach step has a verb name and a configuration object.\nIn DataShaper, these verbs model relational concepts such as SELECT, DROP, JOIN, etc.. Each verb transforms an input data table, and that table is passed down the pipeline. ---\ntitle: Sample Workflow\n---\nflowchart LR\n    input[Input Table] --> select[SELECT] --> join[JOIN] --> binarize[BINARIZE] --> output[Output Table] LLM-based Workflow Steps GraphRAG's Indexing Pipeline implements a handful of custom verbs on top of the standard, relational verbs that our DataShaper library provides. These verbs give us the ability to augment text documents with rich, structured data using the power of LLMs such as GPT-4. We utilize these verbs in our standard workflow to extract entities, relationships, claims, community structures, and community reports and summaries. This behavior is customizable and can be extended to support many kinds of AI-based data enrichment and extraction tasks. Workflow Graphs Because of the complexity of our data indexing tasks, we needed to be able to express our data pipeline as series of multiple, interdependent workflows.\nIn the GraphRAG Indexing Pipeline, each workflow may define dependencies on other workflows, effectively forming a directed acyclic graph (DAG) of workflows, which is then used to schedule processing. ---\ntitle: Sample Workflow DAG\n---\nstateDiagram-v2\n    [*] --> Prepare\n    Prepare --> Chunk\n    Chunk --> ExtractGraph\n    Chunk --> EmbedDocuments\n    ExtractGraph --> GenerateReports\n    ExtractGraph --> EmbedGraph\n    EntityResolution --> EmbedGraph\n    EntityResolution --> GenerateReports\n    ExtractGraph --> EntityResolution Dataframe Message Format The primary unit of communication between workflows, and between workflow steps is an instance of pandas.DataFrame .\nAlthough side-effects are possible, our goal is to be data-centric and table-centric in our approach to data processing.\nThis allows us to easily reason about our data, and to leverage the power of dataframe-based ecosystems.\nOur underlying dataframe technology may change over time, but our primary goal is to support the DataShaper workflow schema while retaining single-machine ease of use and developer ergonomics. LLM Caching The GraphRAG library was designed with LLM interactions in mind, and a common setback when working with LLM APIs is various errors errors due to network latency, throttling, etc..\nBecause of these potential error cases, we've added a cache layer around LLM interactions.\nWhen completion requests are made using the same input set (prompt and tuning parameters), we return a cached result if one exists.\nThis allows our indexer to be more resilient to network issues, to act idempotently, and to provide a more efficient end-user experience. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/index/overview/", "text": "GraphRAG Indexing ü§ñ GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search GraphRAG Indexing ü§ñ The GraphRAG indexing package is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using LLMs. Indexing Pipelines are configurable. They are composed of workflows, standard and custom steps, prompt templates, and input/output adapters. Our standard pipeline is designed to: extract entities, relationships and claims from raw text perform community detection in entities generate community summaries and reports at multiple levels of granularity embed entities into a graph vector space embed text chunks into a textual vector space The outputs of the pipeline can be stored in a variety of formats, including JSON and Parquet - or they can be handled manually via the Python API. Getting Started Requirements See the requirements section in Get Started for details on setting up a development environment. The Indexing Engine can be used in either a default configuration mode or with a custom pipeline.\nTo configure GraphRAG, see the configuration documentation.\nAfter you have a config file you can run the pipeline using the CLI or the Python API. Usage CLI # Via Poetry poetry run poe cli --root < data_root > # default config mode poetry run poe cli --config your_pipeline.yml # custom config mode # Via Node yarn run:index --root < data_root > # default config mode yarn run:index --config your_pipeline.yml # custom config mode Python API from graphrag . index import run_pipeline from graphrag . index . config import PipelineWorkflowReference\n\nworkflows : list [ PipelineWorkflowReference ] = [ PipelineWorkflowReference ( steps = [ { # built-in verb \"verb\" : \"derive\" , # https://github.com/microsoft/datashaper/blob/main/python/datashaper/datashaper/engine/verbs/derive.py \"args\" : { \"column1\" : \"col1\" , # from above \"column2\" : \"col2\" , # from above \"to\" : \"col_multiplied\" , # new column name \"operator\" : \"*\" , # multiply the two columns } , # Since we're trying to act on the default input, we don't need explicitly to specify an input } ] ) , ] dataset = pd . DataFrame ( [ { \"col1\" : 2 , \"col2\" : 4 } , { \"col1\" : 5 , \"col2\" : 10 } ] ) outputs = [ ] async for output in await run_pipeline ( dataset = dataset , workflows = workflows ) : outputs . append ( output ) pipeline_result = outputs [ - 1 ] print ( pipeline_result ) Further Reading To start developing within the GraphRAG project, see getting started To understand the underlying concepts and execution model of the indexing library, see the architecture documentation To get running with a series of examples, see the examples documentation To read more about configuring the indexing engine, see the configuration documentation Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://microsoft.github.io/graphrag/posts/config/init/", "text": "Configuring GraphRAG Indexing GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Configuring GraphRAG Indexing To start using GraphRAG, you need to configure the system. The init command is the easiest way to get started. It will create a .env and settings.yaml files in the specified directory with the necessary configuration settings. It will also output the default LLM prompts used by GraphRAG. Usage python -m graphrag.index [ --init ] [ --root PATH ] Options --init - Initialize the directory with the necessary configuration files. --root PATH - The root directory to initialize. Default is the current directory. Example python -m graphrag.index --init --root ./ragtest Output The init command will create the following files in the specified directory: settings.yaml - The configuration settings file. This file contains the configuration settings for GraphRAG. .env - The environment variables file. These are referenced in the settings.yaml file. prompts/ - The LLM prompts folder. This contains the default prompts used by GraphRAG, you can modify them or run the Auto Prompt Tuning command to generate new prompts adapted to your data. Next Steps After initializing your workspace, you can either run the Prompt Tuning command to adapt the prompts to your data or even start running the Indexing Pipeline to index your data. For more information on configuring GraphRAG, see the Configuration documentation. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://github.com/microsoft/graphrag", "text": "GitHub - microsoft/graphrag: A modular graph-based Retrieval-Augmented Generation (RAG) system Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k A modular graph-based Retrieval-Augmented Generation (RAG) system microsoft.github.io/graphrag/ License MIT license 14.2k stars 1.2k forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights microsoft/graphrag This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. main Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit History 85 Commits .github .github .semversioner .semversioner .vscode .vscode docsite docsite examples examples examples_notebooks examples_notebooks graphrag graphrag scripts scripts tests tests .gitignore .gitignore .vsts-ci.yml .vsts-ci.yml CHANGELOG.md CHANGELOG.md CODEOWNERS CODEOWNERS CODE_OF_CONDUCT.md CODE_OF_CONDUCT.md CONTRIBUTING.md CONTRIBUTING.md DEVELOPING.md DEVELOPING.md LICENSE LICENSE RAI_TRANSPARENCY.md RAI_TRANSPARENCY.md README.md README.md SECURITY.md SECURITY.md SUPPORT.md SUPPORT.md cspell.config.yaml cspell.config.yaml dictionary.txt dictionary.txt poetry.lock poetry.lock pyproject.toml pyproject.toml View all files Repository files navigation README Code of conduct MIT license Security GraphRAG üëâ Use the GraphRAG Accelerator solution üëâ Microsoft Research Blog Post üëâ Read the docs üëâ GraphRAG Arxiv Overview The GraphRAG project is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using the power of LLMs. To learn more about GraphRAG and how it can be used to enhance your LLM's ability to reason about your private data, please visit the Microsoft Research Blog Post. Quickstart To get started with the GraphRAG system we recommend trying the Solution Accelerator package. This provides a user-friendly end-to-end experience with Azure resources. Repository Guidance This repository presents a methodology for using knowledge graph memory structures to enhance LLM outputs. Please note that the provided code serves as a demonstration and is not an officially supported Microsoft offering. ‚ö†Ô∏è Warning: GraphRAG indexing can be an expensive operation, please read all of the documentation to understand the process and costs involved, and start small. Diving Deeper To learn about our contribution guidelines, see CONTRIBUTING.md To start developing GraphRAG , see DEVELOPING.md Join the conversation and provide feedback in the GitHub Discussions tab! Prompt Tuning Using GraphRAG with your data out of the box may not yield the best possible results.\nWe strongly recommend to fine-tune your prompts following the Prompt Tuning Guide in our documentation. Responsible AI FAQ See RAI_TRANSPARENCY.md What is GraphRAG? What can GraphRAG do? What are GraphRAG‚Äôs intended use(s)? How was GraphRAG evaluated? What metrics are used to measure performance? What are the limitations of GraphRAG? How can users minimize the impact of GraphRAG‚Äôs limitations when using the system? What operational factors and settings allow for effective and responsible use of GraphRAG? Trademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines .\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies. Privacy Microsoft Privacy Statement About A modular graph-based Retrieval-Augmented Generation (RAG) system microsoft.github.io/graphrag/ Topics gpt rag gpt-4 gpt4 llm llms graphrag Resources Readme License MIT license Code of conduct Code of conduct Security policy Security policy Activity Custom properties Stars 14.2k stars Watchers 87 watching Forks 1.2k forks Report repository Releases 3 v0.2.1 Latest Aug 6, 2024 + 2 releases Packages 0 No packages published Contributors 27 + 13 contributors Languages Python 95.3% Jupyter Notebook 3.7% Nunjucks 0.5% Jinja 0.2% CSS 0.1% JavaScript 0.1% Shell 0.1% Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/Azure-Samples/graphrag-accelerator", "text": "GitHub - Azure-Samples/graphrag-accelerator: One-click deploy of a Knowledge Graph powered RAG (GraphRAG) in Azure Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert Azure-Samples / graphrag-accelerator Public Notifications You must be signed in to change notification settings Fork 205 Star 1.4k One-click deploy of a Knowledge Graph powered RAG (GraphRAG) in Azure github.com/microsoft/graphrag License MIT license 1.4k stars 205 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 12 Pull requests 8 Discussions Actions Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Security Insights Azure-Samples/graphrag-accelerator This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. main Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit History 36 Commits .azdo .azdo .devcontainer .devcontainer .github .github backend backend docker docker docs docs frontend frontend infra infra notebooks notebooks .dockerignore .dockerignore .editorconfig .editorconfig .gitattributes .gitattributes .gitignore .gitignore .pre-commit-config.yaml .pre-commit-config.yaml .secrets.baseline .secrets.baseline CODEOWNERS CODEOWNERS CODE_OF_CONDUCT.md CODE_OF_CONDUCT.md CONTRIBUTING.md CONTRIBUTING.md LICENSE LICENSE README.md README.md SECURITY.md SECURITY.md TRANSPARENCY.md TRANSPARENCY.md poetry.lock poetry.lock pyproject.toml pyproject.toml View all files Repository files navigation README Code of conduct MIT license Security GraphRAG Accelerator Welcome to the GraphRAG solution accelerator! This accelerator builds on top of the graphrag python package and exposes API endpoints hosted on Azure, which can be used to trigger indexing pipelines and enable querying of the graphrag knowledge graph. For FAQ, access instructions, and our roadmap, please visit aka.ms/graphrag Getting Started with GraphRAG on Azure Deployment Guide To deploy the solution accelerator, see the deployment guide . This will result in a full deployment of graphrag as an API.\nAfterwards, check out the Quickstart notebook for a demonstration of various API calls. Development Guide Interested in contributing? Check out the development guide . How to file issues and get help This project uses GitHub Issues to track bugs and feature requests. Please search the existing issues before filing new issues to avoid duplicates. For new issues, file your bug or feature request as a new Issue. Contributing This project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to,\nand actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com . When you submit a pull request, a CLA-bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the\ninstructions provided by the bot. You will only need to do this once across all repositories using our CLA. This project has adopted the Microsoft Open Source Code of Conduct .\nFor more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Trademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft‚Äôs Trademark & Brand Guidelines . Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies. About One-click deploy of a Knowledge Graph powered RAG (GraphRAG) in Azure github.com/microsoft/graphrag Resources Readme License MIT license Code of conduct Code of conduct Security policy Security policy Activity Custom properties Stars 1.4k stars Watchers 29 watching Forks 205 forks Report repository Releases 1 v0.1.1 Latest Jul 2, 2024 Contributors 15 Languages Python 64.1% Bicep 14.5% Jupyter Notebook 12.5% Shell 6.6% CSS 0.8% Smarty 0.8% Dockerfile 0.7% Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main//graphrag/query/structured_search/global_search/reduce_system_prompt.py", "text": "graphrag/graphrag/query/structured_search/global_search/reduce_system_prompt.py at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main//graphrag/query/structured_search/local_search/system_prompt.py", "text": "graphrag/graphrag/query/structured_search/local_search/system_prompt.py at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Files main Breadcrumbs graphrag / graphrag / query / structured_search / local_search / system_prompt.py Blame Blame Latest commit History History 69 lines (34 loc) ¬∑ 2.64 KB main Breadcrumbs graphrag / graphrag / query / structured_search / local_search / system_prompt.py Top File metadata and controls Code Blame 69 lines (34 loc) ¬∑ 2.64 KB Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License \"\"\"Local search system prompts.\"\"\" LOCAL_SEARCH_SYSTEM_PROMPT = \"\"\" ---Role--- You are a helpful assistant responding to questions about data in the tables provided. ---Goal--- Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge. If you don't know the answer, just say so. Do not make anything up. Points supported by data should list their data references as follows: \"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\" Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more. For example: \"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16), Reports (1), Entities (5, 7); Relationships (23); Claims (2, 7, 34, 46, 64, +more)].\" where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record. Do not include information where the supporting evidence for it is not provided. ---Target response length and format--- {response_type} ---Data tables--- {context_data} ---Goal--- Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge. If you don't know the answer, just say so. Do not make anything up. Points supported by data should list their data references as follows: \"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\" Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more. For example: \"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16), Reports (1), Entities (5, 7); Relationships (23); Claims (2, 7, 34, 46, 64, +more)].\" where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record. Do not include information where the supporting evidence for it is not provided. ---Target response length and format--- {response_type} Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown. \"\"\" Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main//graphrag/query/structured_search/local_search/mixed_context.py", "text": "graphrag/graphrag/query/structured_search/local_search/mixed_context.py at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Files main Breadcrumbs graphrag / graphrag / query / structured_search / local_search / mixed_context.py Blame Blame Latest commit History History 497 lines (457 loc) ¬∑ 21.2 KB main Breadcrumbs graphrag / graphrag / query / structured_search / local_search / mixed_context.py Top File metadata and controls Code Blame 497 lines (457 loc) ¬∑ 21.2 KB Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License \"\"\"Algorithms to build context data for local search prompt.\"\"\" import logging from typing import Any import pandas as pd import tiktoken from graphrag . model import ( CommunityReport , Covariate , Entity , Relationship , TextUnit , ) from graphrag . query . context_builder . community_context import ( build_community_context , ) from graphrag . query . context_builder . conversation_history import ( ConversationHistory , ) from graphrag . query . context_builder . entity_extraction import ( EntityVectorStoreKey , map_query_to_entities , ) from graphrag . query . context_builder . local_context import ( build_covariates_context , build_entity_context , build_relationship_context , get_candidate_context , ) from graphrag . query . context_builder . source_context import ( build_text_unit_context , count_relationships , ) from graphrag . query . input . retrieval . community_reports import ( get_candidate_communities , ) from graphrag . query . input . retrieval . text_units import get_candidate_text_units from graphrag . query . llm . base import BaseTextEmbedding from graphrag . query . llm . text_utils import num_tokens from graphrag . query . structured_search . base import LocalContextBuilder from graphrag . vector_stores import BaseVectorStore log = logging . getLogger ( __name__ ) class LocalSearchMixedContext ( LocalContextBuilder ): \"\"\"Build data context for local search prompt combining community reports and entity/relationship/covariate tables.\"\"\" def __init__ ( self , entities : list [ Entity ], entity_text_embeddings : BaseVectorStore , text_embedder : BaseTextEmbedding , text_units : list [ TextUnit ] | None = None , community_reports : list [ CommunityReport ] | None = None , relationships : list [ Relationship ] | None = None , covariates : dict [ str , list [ Covariate ]] | None = None , token_encoder : tiktoken . Encoding | None = None , embedding_vectorstore_key : str = EntityVectorStoreKey . ID , ): if community_reports is None : community_reports = [] if relationships is None : relationships = [] if covariates is None : covariates = {} if text_units is None : text_units = [] self . entities = { entity . id : entity for entity in entities } self . community_reports = { community . id : community for community in community_reports } self . text_units = { unit . id : unit for unit in text_units } self . relationships = { relationship . id : relationship for relationship in relationships } self . covariates = covariates self . entity_text_embeddings = entity_text_embeddings self . text_embedder = text_embedder self . token_encoder = token_encoder self . embedding_vectorstore_key = embedding_vectorstore_key def filter_by_entity_keys ( self , entity_keys : list [ int ] | list [ str ]): \"\"\"Filter entity text embeddings by entity keys.\"\"\" self . entity_text_embeddings . filter_by_id ( entity_keys ) def build_context ( self , query : str , conversation_history : ConversationHistory | None = None , include_entity_names : list [ str ] | None = None , exclude_entity_names : list [ str ] | None = None , conversation_history_max_turns : int | None = 5 , conversation_history_user_turns_only : bool = True , max_tokens : int = 8000 , text_unit_prop : float = 0.5 , community_prop : float = 0.25 , top_k_mapped_entities : int = 10 , top_k_relationships : int = 10 , include_community_rank : bool = False , include_entity_rank : bool = False , rank_description : str = \"number of relationships\" , include_relationship_weight : bool = False , relationship_ranking_attribute : str = \"rank\" , return_candidate_context : bool = False , use_community_summary : bool = False , min_community_rank : int = 0 , community_context_name : str = \"Reports\" , column_delimiter : str = \"|\" , ** kwargs : dict [ str , Any ], ) -> tuple [ str | list [ str ], dict [ str , pd . DataFrame ]]: \"\"\" Build data context for local search prompt. Build a context by combining community reports and entity/relationship/covariate tables, and text units using a predefined ratio set by summary_prop. \"\"\" if include_entity_names is None : include_entity_names = [] if exclude_entity_names is None : exclude_entity_names = [] if community_prop + text_unit_prop > 1 : value_error = ( \"The sum of community_prop and text_unit_prop should not exceed 1.\" ) raise ValueError ( value_error ) # map user query to entities # if there is conversation history, attached the previous user questions to the current query if conversation_history : pre_user_questions = \" \\n \" . join ( conversation_history . get_user_turns ( conversation_history_max_turns ) ) query = f\" { query } \\n { pre_user_questions } \" selected_entities = map_query_to_entities ( query = query , text_embedding_vectorstore = self . entity_text_embeddings , text_embedder = self . text_embedder , all_entities = list ( self . entities . values ()), embedding_vectorstore_key = self . embedding_vectorstore_key , include_entity_names = include_entity_names , exclude_entity_names = exclude_entity_names , k = top_k_mapped_entities , oversample_scaler = 2 , ) # build context final_context = list [ str ]() final_context_data = dict [ str , pd . DataFrame ]() if conversation_history : # build conversation history context ( conversation_history_context , conversation_history_context_data , ) = conversation_history . build_context ( include_user_turns_only = conversation_history_user_turns_only , max_qa_turns = conversation_history_max_turns , column_delimiter = column_delimiter , max_tokens = max_tokens , recency_bias = False , ) if conversation_history_context . strip () != \"\" : final_context . append ( conversation_history_context ) final_context_data = conversation_history_context_data max_tokens = max_tokens - num_tokens ( conversation_history_context , self . token_encoder ) # build community context community_tokens = max ( int ( max_tokens * community_prop ), 0 ) community_context , community_context_data = self . _build_community_context ( selected_entities = selected_entities , max_tokens = community_tokens , use_community_summary = use_community_summary , column_delimiter = column_delimiter , include_community_rank = include_community_rank , min_community_rank = min_community_rank , return_candidate_context = return_candidate_context , context_name = community_context_name , ) if community_context . strip () != \"\" : final_context . append ( community_context ) final_context_data = { ** final_context_data , ** community_context_data } # build local (i.e. entity-relationship-covariate) context local_prop = 1 - community_prop - text_unit_prop local_tokens = max ( int ( max_tokens * local_prop ), 0 ) local_context , local_context_data = self . _build_local_context ( selected_entities = selected_entities , max_tokens = local_tokens , include_entity_rank = include_entity_rank , rank_description = rank_description , include_relationship_weight = include_relationship_weight , top_k_relationships = top_k_relationships , relationship_ranking_attribute = relationship_ranking_attribute , return_candidate_context = return_candidate_context , column_delimiter = column_delimiter , ) if local_context . strip () != \"\" : final_context . append ( str ( local_context )) final_context_data = { ** final_context_data , ** local_context_data } # build text unit context text_unit_tokens = max ( int ( max_tokens * text_unit_prop ), 0 ) text_unit_context , text_unit_context_data = self . _build_text_unit_context ( selected_entities = selected_entities , max_tokens = text_unit_tokens , return_candidate_context = return_candidate_context , ) if text_unit_context . strip () != \"\" : final_context . append ( text_unit_context ) final_context_data = { ** final_context_data , ** text_unit_context_data } return ( \" \\n \\n \" . join ( final_context ), final_context_data ) def _build_community_context ( self , selected_entities : list [ Entity ], max_tokens : int = 4000 , use_community_summary : bool = False , column_delimiter : str = \"|\" , include_community_rank : bool = False , min_community_rank : int = 0 , return_candidate_context : bool = False , context_name : str = \"Reports\" , ) -> tuple [ str , dict [ str , pd . DataFrame ]]: \"\"\"Add community data to the context window until it hits the max_tokens limit.\"\"\" if len ( selected_entities ) == 0 or len ( self . community_reports ) == 0 : return ( \"\" , { context_name . lower (): pd . DataFrame ()}) community_matches = {} for entity in selected_entities : # increase count of the community that this entity belongs to if entity . community_ids : for community_id in entity . community_ids : community_matches [ community_id ] = ( community_matches . get ( community_id , 0 ) + 1 ) # sort communities by number of matched entities and rank selected_communities = [ self . community_reports [ community_id ] for community_id in community_matches if community_id in self . community_reports ] for community in selected_communities : if community . attributes is None : community . attributes = {} community . attributes [ \"matches\" ] = community_matches [ community . id ] selected_communities . sort ( key = lambda x : ( x . attributes [ \"matches\" ], x . rank ), # type: ignore reverse = True , # type: ignore ) for community in selected_communities : del community . attributes [ \"matches\" ] # type: ignore context_text , context_data = build_community_context ( community_reports = selected_communities , token_encoder = self . token_encoder , use_community_summary = use_community_summary , column_delimiter = column_delimiter , shuffle_data = False , include_community_rank = include_community_rank , min_community_rank = min_community_rank , max_tokens = max_tokens , single_batch = True , context_name = context_name , ) if isinstance ( context_text , list ) and len ( context_text ) > 0 : context_text = \" \\n \\n \" . join ( context_text ) if return_candidate_context : candidate_context_data = get_candidate_communities ( selected_entities = selected_entities , community_reports = list ( self . community_reports . values ()), use_community_summary = use_community_summary , include_community_rank = include_community_rank , ) context_key = context_name . lower () if context_key not in context_data : context_data [ context_key ] = candidate_context_data context_data [ context_key ][ \"in_context\" ] = False else : if ( \"id\" in candidate_context_data . columns and \"id\" in context_data [ context_key ]. columns ): candidate_context_data [ \"in_context\" ] = candidate_context_data [ \"id\" ]. isin ( # cspell:disable-line context_data [ context_key ][ \"id\" ] ) context_data [ context_key ] = candidate_context_data else : context_data [ context_key ][ \"in_context\" ] = True return ( str ( context_text ), context_data ) def _build_text_unit_context ( self , selected_entities : list [ Entity ], max_tokens : int = 8000 , return_candidate_context : bool = False , column_delimiter : str = \"|\" , context_name : str = \"Sources\" , ) -> tuple [ str , dict [ str , pd . DataFrame ]]: \"\"\"Rank matching text units and add them to the context window until it hits the max_tokens limit.\"\"\" if len ( selected_entities ) == 0 or len ( self . text_units ) == 0 : return ( \"\" , { context_name . lower (): pd . DataFrame ()}) selected_text_units = list [ TextUnit ]() # for each matching text unit, rank first by the order of the entities that match it, then by the number of matching relationships # that the text unit has with the matching entities for index , entity in enumerate ( selected_entities ): if entity . text_unit_ids : for text_id in entity . text_unit_ids : if ( text_id not in [ unit . id for unit in selected_text_units ] and text_id in self . text_units ): selected_unit = self . text_units [ text_id ] num_relationships = count_relationships ( selected_unit , entity , self . relationships ) if selected_unit . attributes is None : selected_unit . attributes = {} selected_unit . attributes [ \"entity_order\" ] = index selected_unit . attributes [ \"num_relationships\" ] = ( num_relationships ) selected_text_units . append ( selected_unit ) # sort selected text units by ascending order of entity order and descending order of number of relationships selected_text_units . sort ( key = lambda x : ( x . attributes [ \"entity_order\" ], # type: ignore - x . attributes [ \"num_relationships\" ], # type: ignore ) ) for unit in selected_text_units : del unit . attributes [ \"entity_order\" ] # type: ignore del unit . attributes [ \"num_relationships\" ] # type: ignore context_text , context_data = build_text_unit_context ( text_units = selected_text_units , token_encoder = self . token_encoder , max_tokens = max_tokens , shuffle_data = False , context_name = context_name , column_delimiter = column_delimiter , ) if return_candidate_context : candidate_context_data = get_candidate_text_units ( selected_entities = selected_entities , text_units = list ( self . text_units . values ()), ) context_key = context_name . lower () if context_key not in context_data : context_data [ context_key ] = candidate_context_data context_data [ context_key ][ \"in_context\" ] = False else : if ( \"id\" in candidate_context_data . columns and \"id\" in context_data [ context_key ]. columns ): candidate_context_data [ \"in_context\" ] = candidate_context_data [ \"id\" ]. isin ( # cspell:disable-line context_data [ context_key ][ \"id\" ] ) context_data [ context_key ] = candidate_context_data else : context_data [ context_key ][ \"in_context\" ] = True return ( str ( context_text ), context_data ) def _build_local_context ( self , selected_entities : list [ Entity ], max_tokens : int = 8000 , include_entity_rank : bool = False , rank_description : str = \"relationship count\" , include_relationship_weight : bool = False , top_k_relationships : int = 10 , relationship_ranking_attribute : str = \"rank\" , return_candidate_context : bool = False , column_delimiter : str = \"|\" , ) -> tuple [ str , dict [ str , pd . DataFrame ]]: \"\"\"Build data context for local search prompt combining entity/relationship/covariate tables.\"\"\" # build entity context entity_context , entity_context_data = build_entity_context ( selected_entities = selected_entities , token_encoder = self . token_encoder , max_tokens = max_tokens , column_delimiter = column_delimiter , include_entity_rank = include_entity_rank , rank_description = rank_description , context_name = \"Entities\" , ) entity_tokens = num_tokens ( entity_context , self . token_encoder ) # build relationship-covariate context added_entities = [] final_context = [] final_context_data = {} # gradually add entities and associated metadata to the context until we reach limit for entity in selected_entities : current_context = [] current_context_data = {} added_entities . append ( entity ) # build relationship context ( relationship_context , relationship_context_data , ) = build_relationship_context ( selected_entities = added_entities , relationships = list ( self . relationships . values ()), token_encoder = self . token_encoder , max_tokens = max_tokens , column_delimiter = column_delimiter , top_k_relationships = top_k_relationships , include_relationship_weight = include_relationship_weight , relationship_ranking_attribute = relationship_ranking_attribute , context_name = \"Relationships\" , ) current_context . append ( relationship_context ) current_context_data [ \"relationships\" ] = relationship_context_data total_tokens = entity_tokens + num_tokens ( relationship_context , self . token_encoder ) # build covariate context for covariate in self . covariates : covariate_context , covariate_context_data = build_covariates_context ( selected_entities = added_entities , covariates = self . covariates [ covariate ], token_encoder = self . token_encoder , max_tokens = max_tokens , column_delimiter = column_delimiter , context_name = covariate , ) total_tokens += num_tokens ( covariate_context , self . token_encoder ) current_context . append ( covariate_context ) current_context_data [ covariate . lower ()] = covariate_context_data if total_tokens > max_tokens : log . info ( \"Reached token limit - reverting to previous context state\" ) break final_context = current_context final_context_data = current_context_data # attach entity context to final context final_context_text = entity_context + \" \\n \\n \" + \" \\n \\n \" . join ( final_context ) final_context_data [ \"entities\" ] = entity_context_data if return_candidate_context : # we return all the candidate entities/relationships/covariates (not only those that were fitted into the context window) # and add a tag to indicate which records were included in the context window candidate_context_data = get_candidate_context ( selected_entities = selected_entities , entities = list ( self . entities . values ()), relationships = list ( self . relationships . values ()), covariates = self . covariates , include_entity_rank = include_entity_rank , entity_rank_description = rank_description , include_relationship_weight = include_relationship_weight , ) for key in candidate_context_data : candidate_df = candidate_context_data [ key ] if key not in final_context_data : final_context_data [ key ] = candidate_df final_context_data [ key ][ \"in_context\" ] = False else : in_context_df = final_context_data [ key ] if \"id\" in in_context_df . columns and \"id\" in candidate_df . columns : candidate_df [ \"in_context\" ] = candidate_df [ \"id\" ]. isin ( # cspell:disable-line in_context_df [ \"id\" ] ) final_context_data [ key ] = candidate_df else : final_context_data [ key ][ \"in_context\" ] = True else : for key in final_context_data : final_context_data [ key ][ \"in_context\" ] = True return ( final_context_text , final_context_data ) Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main//graphrag/query/structured_search/global_search/search.py", "text": "graphrag/graphrag/query/structured_search/global_search/search.py at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Files main Breadcrumbs graphrag / graphrag / query / structured_search / global_search / search.py Blame Blame Latest commit History History 359 lines (322 loc) ¬∑ 13.5 KB main Breadcrumbs graphrag / graphrag / query / structured_search / global_search / search.py Top File metadata and controls Code Blame 359 lines (322 loc) ¬∑ 13.5 KB Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License \"\"\"The GlobalSearch Implementation.\"\"\" import asyncio import json import logging import time from dataclasses import dataclass from typing import Any import pandas as pd import tiktoken from graphrag . llm . openai . utils import try_parse_json_object from graphrag . query . context_builder . builders import GlobalContextBuilder from graphrag . query . context_builder . conversation_history import ( ConversationHistory , ) from graphrag . query . llm . base import BaseLLM from graphrag . query . llm . text_utils import num_tokens from graphrag . query . structured_search . base import BaseSearch , SearchResult from graphrag . query . structured_search . global_search . callbacks import ( GlobalSearchLLMCallback , ) from graphrag . query . structured_search . global_search . map_system_prompt import ( MAP_SYSTEM_PROMPT , ) from graphrag . query . structured_search . global_search . reduce_system_prompt import ( GENERAL_KNOWLEDGE_INSTRUCTION , NO_DATA_ANSWER , REDUCE_SYSTEM_PROMPT , ) DEFAULT_MAP_LLM_PARAMS = { \"max_tokens\" : 1000 , \"temperature\" : 0.0 , } DEFAULT_REDUCE_LLM_PARAMS = { \"max_tokens\" : 2000 , \"temperature\" : 0.0 , } log = logging . getLogger ( __name__ ) @ dataclass class GlobalSearchResult ( SearchResult ): \"\"\"A GlobalSearch result.\"\"\" map_responses : list [ SearchResult ] reduce_context_data : str | list [ pd . DataFrame ] | dict [ str , pd . DataFrame ] reduce_context_text : str | list [ str ] | dict [ str , str ] class GlobalSearch ( BaseSearch ): \"\"\"Search orchestration for global search mode.\"\"\" def __init__ ( self , llm : BaseLLM , context_builder : GlobalContextBuilder , token_encoder : tiktoken . Encoding | None = None , map_system_prompt : str = MAP_SYSTEM_PROMPT , reduce_system_prompt : str = REDUCE_SYSTEM_PROMPT , response_type : str = \"multiple paragraphs\" , allow_general_knowledge : bool = False , general_knowledge_inclusion_prompt : str = GENERAL_KNOWLEDGE_INSTRUCTION , json_mode : bool = True , callbacks : list [ GlobalSearchLLMCallback ] | None = None , max_data_tokens : int = 8000 , map_llm_params : dict [ str , Any ] = DEFAULT_MAP_LLM_PARAMS , reduce_llm_params : dict [ str , Any ] = DEFAULT_REDUCE_LLM_PARAMS , context_builder_params : dict [ str , Any ] | None = None , concurrent_coroutines : int = 32 , ): super (). __init__ ( llm = llm , context_builder = context_builder , token_encoder = token_encoder , context_builder_params = context_builder_params , ) self . map_system_prompt = map_system_prompt self . reduce_system_prompt = reduce_system_prompt self . response_type = response_type self . allow_general_knowledge = allow_general_knowledge self . general_knowledge_inclusion_prompt = general_knowledge_inclusion_prompt self . callbacks = callbacks self . max_data_tokens = max_data_tokens self . map_llm_params = map_llm_params self . reduce_llm_params = reduce_llm_params if json_mode : self . map_llm_params [ \"response_format\" ] = { \"type\" : \"json_object\" } else : # remove response_format key if json_mode is False self . map_llm_params . pop ( \"response_format\" , None ) self . semaphore = asyncio . Semaphore ( concurrent_coroutines ) async def asearch ( self , query : str , conversation_history : ConversationHistory | None = None , ** kwargs : Any , ) -> GlobalSearchResult : \"\"\" Perform a global search. Global search mode includes two steps: - Step 1: Run parallel LLM calls on communities' short summaries to generate answer for each batch - Step 2: Combine the answers from step 2 to generate the final answer \"\"\" # Step 1: Generate answers for each batch of community short summaries start_time = time . time () context_chunks , context_records = self . context_builder . build_context ( conversation_history = conversation_history , ** self . context_builder_params ) if self . callbacks : for callback in self . callbacks : callback . on_map_response_start ( context_chunks ) # type: ignore map_responses = await asyncio . gather ( * [ self . _map_response_single_batch ( context_data = data , query = query , ** self . map_llm_params ) for data in context_chunks ]) if self . callbacks : for callback in self . callbacks : callback . on_map_response_end ( map_responses ) map_llm_calls = sum ( response . llm_calls for response in map_responses ) map_prompt_tokens = sum ( response . prompt_tokens for response in map_responses ) # Step 2: Combine the intermediate answers from step 2 to generate the final answer reduce_response = await self . _reduce_response ( map_responses = map_responses , query = query , ** self . reduce_llm_params , ) return GlobalSearchResult ( response = reduce_response . response , context_data = context_records , context_text = context_chunks , map_responses = map_responses , reduce_context_data = reduce_response . context_data , reduce_context_text = reduce_response . context_text , completion_time = time . time () - start_time , llm_calls = map_llm_calls + reduce_response . llm_calls , prompt_tokens = map_prompt_tokens + reduce_response . prompt_tokens , ) def search ( self , query : str , conversation_history : ConversationHistory | None = None , ** kwargs : Any , ) -> GlobalSearchResult : \"\"\"Perform a global search synchronously.\"\"\" return asyncio . run ( self . asearch ( query , conversation_history )) async def _map_response_single_batch ( self , context_data : str , query : str , ** llm_kwargs , ) -> SearchResult : \"\"\"Generate answer for a single chunk of community reports.\"\"\" start_time = time . time () search_prompt = \"\" try : search_prompt = self . map_system_prompt . format ( context_data = context_data ) search_messages = [ { \"role\" : \"system\" , \"content\" : search_prompt }, { \"role\" : \"user\" , \"content\" : query }, ] async with self . semaphore : search_response = await self . llm . agenerate ( messages = search_messages , streaming = False , ** llm_kwargs ) log . info ( \"Map response: %s\" , search_response ) try : # parse search response json processed_response = self . parse_search_response ( search_response ) except ValueError : # Clean up and retry parse try : # parse search response json processed_response = self . parse_search_response ( search_response ) except ValueError : log . warning ( \"Warning: Error parsing search response json - skipping this batch\" ) processed_response = [] return SearchResult ( response = processed_response , context_data = context_data , context_text = context_data , completion_time = time . time () - start_time , llm_calls = 1 , prompt_tokens = num_tokens ( search_prompt , self . token_encoder ), ) except Exception : log . exception ( \"Exception in _map_response_single_batch\" ) return SearchResult ( response = [{ \"answer\" : \"\" , \"score\" : 0 }], context_data = context_data , context_text = context_data , completion_time = time . time () - start_time , llm_calls = 1 , prompt_tokens = num_tokens ( search_prompt , self . token_encoder ), ) def parse_search_response ( self , search_response : str ) -> list [ dict [ str , Any ]]: \"\"\"Parse the search response json and return a list of key points. Parameters ---------- search_response: str The search response json string Returns ------- list[dict[str, Any]] A list of key points, each key point is a dictionary with \"answer\" and \"score\" keys \"\"\" search_response , _j = try_parse_json_object ( search_response ) if _j == {}: return [{ \"answer\" : \"\" , \"score\" : 0 }] parsed_elements = json . loads ( search_response ). get ( \"points\" ) if not parsed_elements or not isinstance ( parsed_elements , list ): return [{ \"answer\" : \"\" , \"score\" : 0 }] return [ { \"answer\" : element [ \"description\" ], \"score\" : int ( element [ \"score\" ]), } for element in parsed_elements if \"description\" in element and \"score\" in element ] async def _reduce_response ( self , map_responses : list [ SearchResult ], query : str , ** llm_kwargs , ) -> SearchResult : \"\"\"Combine all intermediate responses from single batches into a final answer to the user query.\"\"\" text_data = \"\" search_prompt = \"\" start_time = time . time () try : # collect all key points into a single list to prepare for sorting key_points = [] for index , response in enumerate ( map_responses ): if not isinstance ( response . response , list ): continue for element in response . response : if not isinstance ( element , dict ): continue if \"answer\" not in element or \"score\" not in element : continue key_points . append ({ \"analyst\" : index , \"answer\" : element [ \"answer\" ], \"score\" : element [ \"score\" ], }) # filter response with score = 0 and rank responses by descending order of score filtered_key_points = [ point for point in key_points if point [ \"score\" ] > 0 # type: ignore ] if len ( filtered_key_points ) == 0 and not self . allow_general_knowledge : # return no data answer if no key points are found log . warning ( \"Warning: All map responses have score 0 (i.e., no relevant information found from the dataset), returning a canned 'I do not know' answer. You can try enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations.\" ) return SearchResult ( response = NO_DATA_ANSWER , context_data = \"\" , context_text = \"\" , completion_time = time . time () - start_time , llm_calls = 0 , prompt_tokens = 0 , ) filtered_key_points = sorted ( filtered_key_points , key = lambda x : x [ \"score\" ], # type: ignore reverse = True , # type: ignore ) data = [] total_tokens = 0 for point in filtered_key_points : formatted_response_data = [] formatted_response_data . append ( f'----Analyst { point [ \"analyst\" ] + 1 } ----' ) formatted_response_data . append ( f'Importance Score: { point [ \"score\" ] } ' # type: ignore ) formatted_response_data . append ( point [ \"answer\" ]) # type: ignore formatted_response_text = \" \\n \" . join ( formatted_response_data ) if ( total_tokens + num_tokens ( formatted_response_text , self . token_encoder ) > self . max_data_tokens ): break data . append ( formatted_response_text ) total_tokens += num_tokens ( formatted_response_text , self . token_encoder ) text_data = \" \\n \\n \" . join ( data ) search_prompt = self . reduce_system_prompt . format ( report_data = text_data , response_type = self . response_type ) if self . allow_general_knowledge : search_prompt += \" \\n \" + self . general_knowledge_inclusion_prompt search_messages = [ { \"role\" : \"system\" , \"content\" : search_prompt }, { \"role\" : \"user\" , \"content\" : query }, ] search_response = await self . llm . agenerate ( search_messages , streaming = True , callbacks = self . callbacks , # type: ignore ** llm_kwargs , # type: ignore ) return SearchResult ( response = search_response , context_data = text_data , context_text = text_data , completion_time = time . time () - start_time , llm_calls = 1 , prompt_tokens = num_tokens ( search_prompt , self . token_encoder ), ) except Exception : log . exception ( \"Exception in reduce_response\" ) return SearchResult ( response = \"\" , context_data = text_data , context_text = text_data , completion_time = time . time () - start_time , llm_calls = 1 , prompt_tokens = num_tokens ( search_prompt , self . token_encoder ), ) Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main//graphrag/query/structured_search/local_search/search.py", "text": "graphrag/graphrag/query/structured_search/local_search/search.py at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Files main Breadcrumbs graphrag / graphrag / query / structured_search / local_search / search.py Blame Blame Latest commit History History 159 lines (141 loc) ¬∑ 5.35 KB main Breadcrumbs graphrag / graphrag / query / structured_search / local_search / search.py Top File metadata and controls Code Blame 159 lines (141 loc) ¬∑ 5.35 KB Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License \"\"\"LocalSearch implementation.\"\"\" import logging import time from typing import Any import tiktoken from graphrag . query . context_builder . builders import LocalContextBuilder from graphrag . query . context_builder . conversation_history import ( ConversationHistory , ) from graphrag . query . llm . base import BaseLLM , BaseLLMCallback from graphrag . query . llm . text_utils import num_tokens from graphrag . query . structured_search . base import BaseSearch , SearchResult from graphrag . query . structured_search . local_search . system_prompt import ( LOCAL_SEARCH_SYSTEM_PROMPT , ) DEFAULT_LLM_PARAMS = { \"max_tokens\" : 1500 , \"temperature\" : 0.0 , } log = logging . getLogger ( __name__ ) class LocalSearch ( BaseSearch ): \"\"\"Search orchestration for local search mode.\"\"\" def __init__ ( self , llm : BaseLLM , context_builder : LocalContextBuilder , token_encoder : tiktoken . Encoding | None = None , system_prompt : str = LOCAL_SEARCH_SYSTEM_PROMPT , response_type : str = \"multiple paragraphs\" , callbacks : list [ BaseLLMCallback ] | None = None , llm_params : dict [ str , Any ] = DEFAULT_LLM_PARAMS , context_builder_params : dict | None = None , ): super (). __init__ ( llm = llm , context_builder = context_builder , token_encoder = token_encoder , llm_params = llm_params , context_builder_params = context_builder_params or {}, ) self . system_prompt = system_prompt self . callbacks = callbacks self . response_type = response_type async def asearch ( self , query : str , conversation_history : ConversationHistory | None = None , ** kwargs , ) -> SearchResult : \"\"\"Build local search context that fits a single context window and generate answer for the user query.\"\"\" start_time = time . time () search_prompt = \"\" context_text , context_records = self . context_builder . build_context ( query = query , conversation_history = conversation_history , ** kwargs , ** self . context_builder_params , ) log . info ( \"GENERATE ANSWER: %s. QUERY: %s\" , start_time , query ) try : search_prompt = self . system_prompt . format ( context_data = context_text , response_type = self . response_type ) search_messages = [ { \"role\" : \"system\" , \"content\" : search_prompt }, { \"role\" : \"user\" , \"content\" : query }, ] response = await self . llm . agenerate ( messages = search_messages , streaming = True , callbacks = self . callbacks , ** self . llm_params , ) return SearchResult ( response = response , context_data = context_records , context_text = context_text , completion_time = time . time () - start_time , llm_calls = 1 , prompt_tokens = num_tokens ( search_prompt , self . token_encoder ), ) except Exception : log . exception ( \"Exception in _asearch\" ) return SearchResult ( response = \"\" , context_data = context_records , context_text = context_text , completion_time = time . time () - start_time , llm_calls = 1 , prompt_tokens = num_tokens ( search_prompt , self . token_encoder ), ) def search ( self , query : str , conversation_history : ConversationHistory | None = None , ** kwargs , ) -> SearchResult : \"\"\"Build local search context that fits a single context window and generate answer for the user question.\"\"\" start_time = time . time () search_prompt = \"\" context_text , context_records = self . context_builder . build_context ( query = query , conversation_history = conversation_history , ** kwargs , ** self . context_builder_params , ) log . info ( \"GENERATE ANSWER: %d. QUERY: %s\" , start_time , query ) try : search_prompt = self . system_prompt . format ( context_data = context_text , response_type = self . response_type ) search_messages = [ { \"role\" : \"system\" , \"content\" : search_prompt }, { \"role\" : \"user\" , \"content\" : query }, ] response = self . llm . generate ( messages = search_messages , streaming = True , callbacks = self . callbacks , ** self . llm_params , ) return SearchResult ( response = response , context_data = context_records , context_text = context_text , completion_time = time . time () - start_time , llm_calls = 1 , prompt_tokens = num_tokens ( search_prompt , self . token_encoder ), ) except Exception : log . exception ( \"Exception in _map_response_single_batch\" ) return SearchResult ( response = \"\" , context_data = context_records , context_text = context_text , completion_time = time . time () - start_time , llm_calls = 1 , prompt_tokens = num_tokens ( search_prompt , self . token_encoder ), ) Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main//graphrag/query/structured_search/global_search/community_context.py", "text": "graphrag/graphrag/query/structured_search/global_search/community_context.py at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Files main Breadcrumbs graphrag / graphrag / query / structured_search / global_search / community_context.py Blame Blame Latest commit History History 99 lines (89 loc) ¬∑ 3.78 KB main Breadcrumbs graphrag / graphrag / query / structured_search / global_search / community_context.py Top File metadata and controls Code Blame 99 lines (89 loc) ¬∑ 3.78 KB Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License \"\"\"Contains algorithms to build context data for global search prompt.\"\"\" from typing import Any import pandas as pd import tiktoken from graphrag . model import CommunityReport , Entity from graphrag . query . context_builder . community_context import ( build_community_context , ) from graphrag . query . context_builder . conversation_history import ( ConversationHistory , ) from graphrag . query . structured_search . base import GlobalContextBuilder class GlobalCommunityContext ( GlobalContextBuilder ): \"\"\"GlobalSearch community context builder.\"\"\" def __init__ ( self , community_reports : list [ CommunityReport ], entities : list [ Entity ] | None = None , token_encoder : tiktoken . Encoding | None = None , random_state : int = 86 , ): self . community_reports = community_reports self . entities = entities self . token_encoder = token_encoder self . random_state = random_state def build_context ( self , conversation_history : ConversationHistory | None = None , use_community_summary : bool = True , column_delimiter : str = \"|\" , shuffle_data : bool = True , include_community_rank : bool = False , min_community_rank : int = 0 , community_rank_name : str = \"rank\" , include_community_weight : bool = True , community_weight_name : str = \"occurrence\" , normalize_community_weight : bool = True , max_tokens : int = 8000 , context_name : str = \"Reports\" , conversation_history_user_turns_only : bool = True , conversation_history_max_turns : int | None = 5 , ** kwargs : Any , ) -> tuple [ str | list [ str ], dict [ str , pd . DataFrame ]]: \"\"\"Prepare batches of community report data table as context data for global search.\"\"\" conversation_history_context = \"\" final_context_data = {} if conversation_history : # build conversation history context ( conversation_history_context , conversation_history_context_data , ) = conversation_history . build_context ( include_user_turns_only = conversation_history_user_turns_only , max_qa_turns = conversation_history_max_turns , column_delimiter = column_delimiter , max_tokens = max_tokens , recency_bias = False , ) if conversation_history_context != \"\" : final_context_data = conversation_history_context_data community_context , community_context_data = build_community_context ( community_reports = self . community_reports , entities = self . entities , token_encoder = self . token_encoder , use_community_summary = use_community_summary , column_delimiter = column_delimiter , shuffle_data = shuffle_data , include_community_rank = include_community_rank , min_community_rank = min_community_rank , community_rank_name = community_rank_name , include_community_weight = include_community_weight , community_weight_name = community_weight_name , normalize_community_weight = normalize_community_weight , max_tokens = max_tokens , single_batch = False , context_name = context_name , random_state = self . random_state , ) if isinstance ( community_context , list ): final_context = [ f\" { conversation_history_context } \\n \\n { context } \" for context in community_context ] else : final_context = f\" { conversation_history_context } \\n \\n { community_context } \" final_context_data . update ( community_context_data ) return ( final_context , final_context_data ) Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main//graphrag/query/structured_search/global_search/map_system_prompt.py", "text": "graphrag/graphrag/query/structured_search/global_search/map_system_prompt.py at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Files main Breadcrumbs graphrag / graphrag / query / structured_search / global_search / map_system_prompt.py Blame Blame Latest commit History History 82 lines (53 loc) ¬∑ 3.92 KB main Breadcrumbs graphrag / graphrag / query / structured_search / global_search / map_system_prompt.py Top File metadata and controls Code Blame 82 lines (53 loc) ¬∑ 3.92 KB Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License \"\"\"System prompts for global search.\"\"\" MAP_SYSTEM_PROMPT = \"\"\" ---Role--- You are a helpful assistant responding to questions about data in the tables provided. ---Goal--- Generate a response consisting of a list of key points that responds to the user's question, summarizing all relevant information in the input data tables. You should use the data provided in the data tables below as the primary context for generating the response. If you don't know the answer or if the input data tables do not contain sufficient information to provide an answer, just say so. Do not make anything up. Each key point in the response should have the following element: - Description: A comprehensive description of the point. - Importance Score: An integer score between 0-100 that indicates how important the point is in answering the user's question. An 'I don't know' type of response should have a score of 0. The response should be JSON formatted as follows: {{ \"points\": [ {{\"description\": \"Description of point 1 [Data: Reports (report ids)]\", \"score\": score_value}}, {{\"description\": \"Description of point 2 [Data: Reports (report ids)]\", \"score\": score_value}} ] }} The response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\". Points supported by data should list the relevant reports as references as follows: \"This is an example sentence supported by data references [Data: Reports (report ids)]\" **Do not list more than 5 record ids in a single reference**. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more. For example: \"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2, 7, 64, 46, 34, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\" where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data report in the provided tables. Do not include information where the supporting evidence for it is not provided. ---Data tables--- {context_data} ---Goal--- Generate a response consisting of a list of key points that responds to the user's question, summarizing all relevant information in the input data tables. You should use the data provided in the data tables below as the primary context for generating the response. If you don't know the answer or if the input data tables do not contain sufficient information to provide an answer, just say so. Do not make anything up. Each key point in the response should have the following element: - Description: A comprehensive description of the point. - Importance Score: An integer score between 0-100 that indicates how important the point is in answering the user's question. An 'I don't know' type of response should have a score of 0. The response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\". Points supported by data should list the relevant reports as references as follows: \"This is an example sentence supported by data references [Data: Reports (report ids)]\" **Do not list more than 5 record ids in a single reference**. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more. For example: \"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2, 7, 64, 46, 34, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\" where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data report in the provided tables. Do not include information where the supporting evidence for it is not provided. The response should be JSON formatted as follows: {{ \"points\": [ {{\"description\": \"Description of point 1 [Data: Reports (report ids)]\", \"score\": score_value}}, {{\"description\": \"Description of point 2 [Data: Reports (report ids)]\", \"score\": score_value}} ] }} \"\"\" Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main/examples/README.md", "text": "graphrag/examples/README.md at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Files main Breadcrumbs graphrag / examples / README.md Blame Blame Latest commit History History 19 lines (14 loc) ¬∑ 700 Bytes main Breadcrumbs graphrag / examples / README.md Top File metadata and controls Preview Code Blame 19 lines (14 loc) ¬∑ 700 Bytes Raw Indexing Engine Examples This directory contains several examples of how to use the indexing engine. Most examples include two different forms of running the pipeline, both are contained in the examples run.py Using mostly the Python API Using mostly the a pipeline configuration file Running an Example First run poetry shell to activate a virtual environment with the required dependencies. Then run PYTHONPATH=\"$(pwd)\" python examples/path_to_example/run.py from the python/graphrag directory. For example to run the single_verb example, you would run the following commands: cd python/graphrag\npoetry shell\nPYTHONPATH= \" $( pwd ) \" python examples/single_verb/run.py Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/datashaper/blob/main/javascript/schema/src/workflow/WorkflowSchema.ts", "text": "datashaper/javascript/schema/src/workflow/WorkflowSchema.ts at main ¬∑ microsoft/datashaper ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / datashaper Public Notifications You must be signed in to change notification settings Fork 17 Star 133 Code Issues 24 Pull requests 13 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights Files main Breadcrumbs datashaper / javascript / schema / src / workflow / WorkflowSchema.ts Blame Blame Latest commit History History 23 lines (21 loc) ¬∑ 572 Bytes main Breadcrumbs datashaper / javascript / schema / src / workflow / WorkflowSchema.ts Top File metadata and controls Code Blame 23 lines (21 loc) ¬∑ 572 Bytes Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 /*! * Copyright (c) Microsoft. All rights reserved. * Licensed under the MIT license. See LICENSE file in the project. */ import type { ResourceSchema } from '../datapackage/ResourceSchema.js' import type { Step } from './Step.js' /** * The root wrangling workflow specification. * resource profile: 'workflow' */ export interface WorkflowSchema extends ResourceSchema { profile: 'workflow' /** * The workflow steps */ steps?: Step[] /** * A list of input names that are expected to be provided in addition to the workflow steps */ input?: string[] } Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks", "text": "Trademark and Brand Guidelines | Microsoft Legal This is the Trace Id: 0cf6b9c4b7f7276a5af1b85c23cfdf12 Skip to main content Microsoft Legal Legal Legal Home Diversity & inclusion Microsoft Legal diversity D&I at Microsoft CELA Diversifying the legal profession Law firm D&I partnerships Compliance & ethics Our approach Our approach Microsoft compliance program Standards of Business Conduct Standards of Business Conduct Our Trust Code Upholding our code of conduct Report a concern Anti-corruption & anti-bribery Anti-corruption & anti-bribery Anti-corruption & anti-bribery commitments Trustworthy representatives State-owned entity criteria Environmental compliance Environmental compliance Devices compliance and sustainability Cloud responsible packaging Responsible supplier sourcing Intellectual property Patents & licensing Patents & licensing Microsoft intellectual property About technology licensing Tech licensing programs Microsoft patents Azure IP Advantage Copyrights & trademarks Copyrights & trademarks About copyright law Use of Microsoft copyrighted content Microsoft Trademark and Brand Guidelines Report copyright & trademark infringement Open innovation Open innovation Open source Open data Licensing terms Licensing terms License terms lookup Software & hardware piracy Microsoft Terms of Use Third party notices More resources Antitrust & interoperability Antitrust & interoperability Antitrust information European Commission Decision of 2004 Interoperability efforts Arbitration & dispute resolution Arbitration & dispute resolution US arbitration and dispute resolution EU arbitration and dispute resolution Pro Bono Pro Bono Microsoft pro bono programs Microsoft pro bono report Settlement & policy information Settlement & policy information Consumer Class Action Settlement Legal supplier policies and procedures More All Microsoft Global Microsoft 365 Teams Copilot Windows Surface Xbox Deals Small Business Support Software Software Windows Apps AI Outlook OneDrive Microsoft Teams OneNote Microsoft Edge Skype PCs & Devices PCs & Devices Computers Shop Xbox Accessories VR & mixed reality Certified Refurbished Trade-in for cash Entertainment Entertainment Xbox Game Pass Ultimate PC Game Pass Xbox games PC and Windows games Movies & TV Business Business Microsoft Cloud Microsoft Security Dynamics 365 Microsoft 365 for business Microsoft Power Platform Windows 365 Microsoft Industry Small Business Developer & IT Developer & IT Azure Developer Center Documentation Microsoft Learn Microsoft Tech Community Azure Marketplace AppSource Visual Studio Other Other Microsoft Rewards Free downloads & security Education Gift cards Licensing Unlocked stories View Sitemap Search Search Microsoft Legal Resources No results Cancel 0 Cart 0 items in shopping cart Sign in Microsoft Trademark and Brand Guidelines Thank you for helping us protect our trademarks and brand assets Microsoft is grateful for the trust that people place in our products, services, and experiences. These Trademark and Brand Guidelines (‚ÄúTrademark Guidelines‚Äù) detail how you can help us protect Microsoft‚Äôs brand assets, including logos, names, app and product icons, and the trust that they represent. We have created these Trademark Guidelines to help clarify proper usage of our brand assets. Microsoft reserves the right to take action as necessary to protect them and, as a result, protect its customers and the public. Microsoft‚Äôs brand assets‚Äîincluding the Microsoft Trademarks (PDF) (as updated from time to time), logos, icons, designs, trade dress, fonts, names of Microsoft software, products, services, sounds, emojis, and any other brand features and elements, whether registered or unregistered (‚ÄúBrand Assets‚Äù)‚Äîare proprietary assets owned exclusively by Microsoft and its group of companies. These Trademark Guidelines, which may be updated from time to time, detail how our Brand Assets can be used under specific circumstances. To help illustrate use cases, Contoso is used as the fictitious partner brand. Many uses, including our logos, app and product icons, and other designs, will require a license first . Unless you have an express license from Microsoft, these Trademark Guidelines will exclusively govern your use of our Brand Assets. Examples of uses governed by these Trademark Guidelines Don‚Äôt miss Contoso‚Äôs Memorial Day sale featuring a limited time discount on Microsoft Excel software. The Contoso Game can be played on Xbox gaming consoles. The Contoso App works with Microsoft Teams . Examples of Brand Assets requiring authorization from Microsoft Microsoft logo See Specific Brand and Product Guidelines section for more information on use requirements. Microsoft logo lockups These include logo lockups for flagship offerings, which are offerings that are aligned to and signal Microsoft‚Äôs long-term strategic intent. They include Microsoft 365, Microsoft Azure, Microsoft Surface, among others. See Specific Brand and Product Guidelines for more information. Microsoft product icons These include app icons for products such as Microsoft 365, Microsoft Dynamics, Microsoft Azure, and others. See Specific Brand and Product Guidelines section for more information. Badges This includes badges to show designations or relationships between your product and Microsoft. Dos and don‚Äôts To help you understand what you can and can‚Äôt do with Microsoft‚Äôs Brand Assets, we have created this non-exhaustive list of Dos and Don‚Äôts. In general, wordmarks can be used to truthfully convey information about your product or service, as long as customers and the public will not be confused into believing Microsoft is affiliated with or endorses your product or service. However, our logos, app and product icons, illustrations, photographs, videos, and designs can never be used without an express license . Please review these additional tips to better understand how you can more clearly communicate the role that our Brand Assets convey in your communications. Dos In general, you may do the following with Microsoft‚Äôs wordmarks and names of software, products, or services: Note if your product, service, or solution is interoperable or compatible with a Microsoft product, service, or solution. Communicate if your product, service, or solution goes through the approved certification process. Contoso headphones are compatible with Microsoft Surface . Contoso app is certified for Microsoft Teams. If your product, service, or solution integrates with a Microsoft technology, clearly communicate the relationship.¬†For more information on this integration scenario, visit the guidelines for partner-led marketing under the Resources section below. Contoso software works with Microsoft OneDrive Contoso app is built on Microsoft Azure Contoso is a new device designed for Microsoft Teams Rooms Contoso is integrated with Microsoft 365 Truthfully and accurately refer to Microsoft and its products and services. Contoso is a three-dimensional model city created by students using Minecraft gaming software. Used Xbox gaming console for sale! Join us for a seminar about mixed reality in the real estate industry, including the use of the Microsoft HoloLens technology. Use without alteration in text to refer to Microsoft and its products and services. Follow these steps to format mail merge letters using Microsoft Word software. CRM is easy with Microsoft Dynamics 365 applications. Microsoft Azure provides enterprise-level cloud computing solutions. Note when a product or service has met Microsoft‚Äôs accessibility requirements . The Contoso case has met Microsoft‚Äôs accessibility requirements for Microsoft Surface products. Use in the title of news articles, when truthful and not misleading. Microsoft releases new Xbox console Microsoft Azure is adding new features on July 1st Microsoft partners with Contoso to add new AI capabilities Use less prominently than your own brand or company name unless you have a strategic partnership agreement.¬† Visit the Microsoft partner website for more information on partner branding. Contoso Inc. Providing Microsoft software installation and customization services since 1999. Contoso software Our software is compatible with a number of email applications, including Microsoft Outlook. Use as adjectives before nouns. Xbox gaming consoles Excel spreadsheets Word documents HoloLens virtual reality equipment OneDrive cloud storage Use our Brand Assets for publication, seminars, or conferences in compliance with Microsoft‚Äôs Publications, seminars, and conferences guidelines . Include a trademark footnote to give notice that you are using Microsoft‚Äôs trademarks, e.g., ‚ÄúMicrosoft, (list additional trademarks in alphabetical order) are trademarks of the Microsoft group of companies.‚Äù Don‚Äôts Without a written license or express permission under one of Microsoft's other published guidelines (e.g., Specific Brand and Product Guidelines below), don‚Äôt do the following: Don‚Äôt use Microsoft‚Äôs Brand Assets in the name of your business, product, service, app, domain name, social media account, other offering, or business indicator. Contoso OneDrive software. Contoso Surface headphones. www. azure virtualmachine.com @ xbox superfans Don‚Äôt use Microsoft‚Äôs logos, icons, or designs, in any manner. Don‚Äôt imply an affiliation, endorsement, sponsorship, or approval with or by Microsoft. Minecraft Builder LLC builds three-dimensional models. Join us for Contoso‚Äôs Xbox gaming event Contoso offers premium Microsoft 365 installation & support services Microsoft Teams has certified the Contoso CP1 headset. Don‚Äôt alter, animate, distort, or misappropriate Microsoft‚Äôs Brand Assets, for example, by combining Microsoft‚Äôs Brand Assets with other terms, misspellings, or incorporating them into a tagline or slogan, etc. Microsoft Words mithing is Easy! Use Microsoft Power ful Platform Rest Azured that your data is safe with Microsoft Contoso‚Ä¶Your one stop shop for all things Xbox . Contoso Dynamics Contoso Excels at Spreadsheet Development! Bing ing you quality search results since 2005 Don‚Äôt use Microsoft‚Äôs Brand Assets as nouns or verbs. Xbox your way to rewards You can Excel with Microsoft We‚Äôre selling new Microsoft Surfaces Don‚Äôt use Microsoft‚Äôs Brand Assets in entertainment titles (including book, films, and magazines). PowerPoint alism Beauty and the Bing Azure Daily Don‚Äôt use Microsoft‚Äôs Brand Assets more prominently than your own brand(s) or company name. MICROSOFT SOFTWARE Contoso installs Microsoft software ‚Äì call us today! Don‚Äôt use Microsoft‚Äôs Brand Assets in the name of a user group, fan group, tech communities, or other organization name, irrespective of whether the group or organization is a non-profit. Dynamics User Group Microsoft 365 Technology Group Xbox Game Club Don‚Äôt use Microsoft‚Äôs Brand Assets on any tangible goods or packaging, including any promotional, marketing, swag, or other items. T-shirt with Microsoft logo Xbox branded headphones Surface mouse pad Microsoft Teams branded headset Don‚Äôt file any application or recordation to register terms or logos identical or similar to Microsoft‚Äôs Brand Assets as a trademark, service mark, trade name, doing business as designation, or any other proprietary right. App guidelines Without a license arrangement with Microsoft, everything about your app (including developer name, app name, logo, description, screenshots, and other app collateral) must be unique to you and free of Microsoft‚Äôs Brand Assets. The only exception is that you may truthfully state whether your app is compatible or interoperable with a Microsoft product or service within the text description about your app. If your app is integrated with a Microsoft technology, follow the guidelines for partner-led marketing under the Resources section below. Examples of approved app details Developer: Contoso Developer, Inc. App name: Contoso App Image/screenshots: <No Microsoft Content Featured> Description: Contoso App allows you to access Microsoft OneDrive content and share it with your CRM contacts at the push of a button. Examples of unapproved app details Developer: Azure Development Co. App name: Azure To Go Image/screenshots: <Microsoft Logo, Azure App Icon Featured> Description: Azure To Go brings the Azure cloud services to your mobile phone. You may not use Microsoft‚Äôs Brand Assets in a manner that implies Microsoft published, developed, endorsed, is affiliated with, or is otherwise connected with your app. Furthermore, Microsoft‚Äôs logos, designs, and icons can never be used as your app icon and can only be used in your app advertisements with a license agreement in place. Specific Brand and Product Guidelines Certain Brand Assets have Specific Brand and Product Guidelines providing guidance on how to use them. Several of these Specific Brand and Product Guidelines are published below and others may be provided to you by your relevant Microsoft contact in the context of your relationship with Microsoft. All Microsoft‚Äôs Specific Brand and Product Guidelines are incorporated in these Trademark Guidelines. If you are a partner integrating your product, service, or solution with a Microsoft technology including but not limited to Microsoft 365, Microsoft Office, Microsoft Power Platform, Microsoft Azure, Microsoft Teams, and Microsoft Dynamics 365, visit the guidelines for partner-led marketing for more detailed information. Microsoft Windows Microsoft 365 Microsoft Edge Skype Bing Microsoft Surface Copyrighted content Visit Use of Microsoft copyrighted content to learn how to use photography, box shots, and screenshots. Legal notice Any use of Microsoft‚Äôs Brand Assets inures solely to Microsoft‚Äôs benefit and all use must comply with these Trademark Guidelines, or other licensing/contractual arrangements with Microsoft. Third parties, including licensees, may never claim ownership rights in Microsoft‚Äôs Brand Assets, or brands that are confusingly similar to Microsoft‚Äôs Brand Assets, in any manner, including without limitation as a trademark, service mark, company name or designation, domain name, social media profile/handle, or in any other manner. Microsoft expressly reserves the right in its sole discretion to terminate, revoke, modify, or otherwise change permission to use its Brand Assets at any time and expressly reserves the right to object to any use or misuse of its Brand Assets in any jurisdiction worldwide. Resources If your proposed use falls outside of these Trademark Guidelines or the Specific Brand and Product Guidelines linked above, the use is not permitted unless you seek approval from an appropriate contact within Microsoft. If you have a question, please contact your legal counsel, Microsoft business contact, or trademarks@microsoft.com . You may find the following helpful if you didn‚Äôt find what you were looking for above. Business Support Press Tool Kit Device Partner Center Partner-led Marketing Guidelines (form) Microsoft Partner Brand Guidelines Xbox Fansite Guidelines Intellectual Property Innovation is at the heart of Microsoft as a company. Explore Microsoft IP Follow Microsoft What's new Surface Pro Surface Laptop Surface Laptop Studio 2 Surface Laptop Go 3 Microsoft Copilot AI in Windows Explore Microsoft products Windows 11 apps Microsoft Store Account profile Download Center Microsoft Store support Returns Order tracking Certified Refurbished Microsoft Store Promise Flexible Payments Education Microsoft in education Devices for education Microsoft Teams for Education Microsoft 365 Education How to buy for your school Educator training and development Deals for students and parents Azure for students Business Microsoft Cloud Microsoft Security Dynamics 365 Microsoft 365 Microsoft Power Platform Microsoft Teams Copilot for Microsoft 365 Small Business Developer & IT Azure Developer Center Documentation Microsoft Learn Microsoft Tech Community Azure Marketplace AppSource Visual Studio Company Careers About Microsoft Company news Privacy at Microsoft Investors Diversity and inclusion Accessibility Sustainability English (United States) Your Privacy Choices Opt-Out Icon Your Privacy Choices Your Privacy Choices Opt-Out Icon Your Privacy Choices Consumer Health Privacy Sitemap Contact Microsoft Privacy Manage cookies Terms of use Trademarks Safety & eco Recycling About our ads ¬© Microsoft 2024"},
{"url": "https://www.microsoft.com/en-ca/", "text": "Microsoft ‚Äì Cloud, Computers, Apps & Gaming Trace Id is missing Skip to main content Microsoft Microsoft 365 Teams Copilot Windows Surface Xbox Deals Small Business Support More All Microsoft Global Microsoft 365 Teams Copilot Windows Surface Xbox Deals Small Business Support Software Software Windows Apps AI OneDrive Outlook Skype OneNote Microsoft Teams PCs & Devices PCs & Devices Computers Shop Xbox Accessories VR & mixed reality Entertainment Entertainment Xbox Game Pass Ultimate PC Game Pass Xbox games PC games Windows digital games Movies & TV Business Business Microsoft Cloud Microsoft Security Azure Dynamics 365 Microsoft 365 for business Microsoft Industry Microsoft Power Platform Windows 365 Developer & IT Developer & IT Developer Center Documentation Microsoft Learn Microsoft Tech Community Azure Marketplace AppSource Visual Studio Other Other Microsoft Rewards Free downloads & security Education Virtual workshops and training Gift cards Licensing View Sitemap Search Search Microsoft.com No results Cancel 0 Cart 0 items in shopping cart Sign in Buy now, enjoy price protection for 60 days from delivery. Shop Back to School Sale now Slide %{start} of %{total}. %{slideTitle} Skip featured products and announcements slideshow: navigate using the previous and next: navigate using the slide tabs Pause Previous Next Back to School Surface Pro, Copilot+ PC Bundle Save up to $79 on the most flexible 2-in-1 out there with Microsoft 365, a protection plan, and 20% off select accessories. Shop the bundle Back to School Surface Laptop, Copilot+ PC Bundle Save up to $79 on an AI-powered PC with Microsoft 365 and a protection plan‚Äîplus, get 20% off select accessories. Shop the bundle End of featured products and announcements slideshow: navigate using the previous and next: navigate using the slide tabs section Shop Surface devices Shop Xbox games and consoles Find your next PC Shop for your business Choose your Microsoft 365 Back to School deals Back to School Up to $500 off Surface Laptop Studio 2 Ace the college pace with next-level deals on the ultimate all-in-one laptop. Select devices on sale now. Offer ends 8/8. Shop Surface Laptop Studio 2 Maximize the everyday with Microsoft 365 Get online protection, secure cloud storage, and innovative apps designed to fit your needs‚Äîall in one plan. For 1 person For up to 6 people Xbox Series X The fastest, most powerful Xbox ever. Shop Xbox Series X Shop all Xbox consoles Xbox Series S Next-gen performance in the smallest Xbox ever. Shop Xbox Series S Unlock your potential with Microsoft Copilot Get things done faster and unleash your creativity with the power of AI anywhere you go. Download the Copilot app For business New Copilot+ PCs for Business Surface Pro for Business and Surface Laptop for Business unlock new AI-accelerated features like Windows Studio Effects. Pre-order Surface Pro for Business Pre-order Surface Laptop for Business New Surface for Business PCs Boost productivity with the latest Intel processors and AI-powered tools in the Surface Pro 10 for Business and Surface Laptop 6 for Business. Shop Surface Pro 10 for Business Shop Surface Laptop 6 for Business Join the era of AI Create, communicate, and code with the latest Microsoft AI solutions. Explore AI solutions Copilot for Microsoft 365 Save time and focus on the things that matter most with AI in Microsoft 365 for business. Learn more Follow Microsoft Accessibility feedback Back to top What's new Surface Pro Surface Laptop Surface Laptop Studio 2 Surface Laptop Go 3 Microsoft Copilot AI in Windows Explore Microsoft products Windows 11 apps Microsoft Store Account profile Download Center Microsoft Store Support Returns Order tracking Virtual workshops and training Microsoft Store Promise Education Microsoft in education Devices for education Microsoft Teams for Education Microsoft 365 Education Office Education Educator training and development Deals for students and parents Azure for students Business Microsoft Cloud Microsoft Security Azure Dynamics 365 Microsoft 365 Microsoft Advertising Copilot for Microsoft 365 Microsoft Teams Developer & IT Developer Center Documentation Microsoft Learn Microsoft Tech Community Azure Marketplace AppSource Microsoft Power Platform Visual Studio Company Careers About Microsoft Company news Privacy at Microsoft Investors Diversity and inclusion Accessibility Sustainability English (Canada) Your Privacy Choices Opt-Out Icon Your Privacy Choices Your Privacy Choices Opt-Out Icon Your Privacy Choices Consumer Health Privacy Contact Microsoft Privacy Manage cookies Terms of use Trademarks About our ads ¬© Microsoft 2024"},
{"url": "https://microsoft.github.io/graphrag/posts/developing/", "text": "Developing GraphRAG GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Developing GraphRAG Requirements Name Installation Purpose Python 3.10-3.12 Download The library is Python-based. Poetry Instructions Poetry is used for package management and virtualenv management in Python codebases Getting Started Install Dependencies # Install Python dependencies. poetry install Execute the Indexing Engine poetry run poe index < .. .args > Executing Queries poetry run poe query < .. .args > Azurite Some unit and smoke tests use Azurite to emulate Azure resources. This can be started by running: ./scripts/start-azurite.sh or by simply running azurite in the terminal if already installed globally. See the Azurite documentation for more information about how to install and use Azurite. Lifecycle Scripts Our Python package utilizes Poetry to manage dependencies and poethepoet to manage build scripts. Available scripts are: poetry run poe index - Run the Indexing CLI poetry run poe query - Run the Query CLI poetry build - This invokes poetry build , which will build a wheel file and other distributable artifacts. poetry run poe test - This will execute all tests. poetry run poe test_unit - This will execute unit tests. poetry run poe test_integration - This will execute integration tests. poetry run poe test_smoke - This will execute smoke tests. poetry run poe check - This will perform a suite of static checks across the package, including: formatting documentation formatting linting security patterns type-checking poetry run poe fix - This will apply any available auto-fixes to the package. Usually this is just formatting fixes. poetry run poe fix_unsafe - This will apply any available auto-fixes to the package, including those that may be unsafe. poetry run poe format - Explicitly run the formatter across the package. Troubleshooting \"RuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config\" when running poetry install Make sure llvm-9 and llvm-9-dev are installed: sudo apt-get install llvm-9 llvm-9-dev and then in your bashrc, add export LLVM_CONFIG=/usr/bin/llvm-config-9 \"numba/_pymodule.h:6:10: fatal error: Python.h: No such file or directory\" when running poetry install Make sure you have python3.10-dev installed or more generally python<version>-dev sudo apt-get install python3.10-dev LLM call constantly exceeds TPM, RPM or time limits GRAPHRAG_LLM_THREAD_COUNT and GRAPHRAG_EMBEDDING_THREAD_COUNT are both set to 50 by default. You can modify this values\nto reduce concurrency. Please refer to the Configuration Documents Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://github.com/microsoft/datashaper/blob/main/javascript/schema/src/workflow/verbs.ts", "text": "datashaper/javascript/schema/src/workflow/verbs.ts at main ¬∑ microsoft/datashaper ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / datashaper Public Notifications You must be signed in to change notification settings Fork 17 Star 133 Code Issues 24 Pull requests 13 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights Files main Breadcrumbs datashaper / javascript / schema / src / workflow / verbs.ts Blame Blame Latest commit History History 728 lines (660 loc) ¬∑ 15.7 KB main Breadcrumbs datashaper / javascript / schema / src / workflow / verbs.ts Top File metadata and controls Code Blame 728 lines (660 loc) ¬∑ 15.7 KB Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 /*! * Copyright (c) Microsoft. All rights reserved. * Licensed under the MIT license. See LICENSE file in the project. */ import type { CodebookSchema } from '../codebook/CodebookSchema.js' import type { Value } from '../data.js' import type { SortDirection } from '../enums/index.js' import type { WorkflowSchema } from './WorkflowSchema.js' export enum Verb { Aggregate = 'aggregate', Bin = 'bin', Binarize = 'binarize', Boolean = 'boolean', Concat = 'concat', Convert = 'convert', Copy = 'copy', Dedupe = 'dedupe', Derive = 'derive', Difference = 'difference', Decode = 'decode', Destructure = 'destructure', Drop = 'drop', Encode = 'encode', Erase = 'erase', Fill = 'fill', Filter = 'filter', Fold = 'fold', Groupby = 'groupby', Impute = 'impute', Intersect = 'intersect', Join = 'join', Lookup = 'lookup', Merge = 'merge', Print = 'print', Onehot = 'onehot', Orderby = 'orderby', Pivot = 'pivot', Recode = 'recode', Rename = 'rename', Rollup = 'rollup', Sample = 'sample', Select = 'select', Spread = 'spread', Unfold = 'unfold', Ungroup = 'ungroup', Unhot = 'unhot', Union = 'union', Unorder = 'unorder', Unroll = 'unroll', Window = 'window', Workflow = 'workflow', StringsReplace = 'strings.replace', StringsLower = 'strings.lower', StringsUpper = 'strings.upper', } export interface InputColumnArgs { /** * Name of the input column for columnar operations */ column: string } /** * Base interface for a number of operations that work on a column list. */ export interface InputColumnListArgs { /** * List of input columns for operations that work across multiple columns */ columns: string[] } export interface InputColumnRecordArgs { /** * Map of old column to new column names */ columns: Record<string, string> } export interface InputKeyValueArgs { /** * Key column for the operation */ key: string /** * Value column for the operation */ value: string } export interface OutputColumnArgs { /** * Name of the output column to receive the operation's result. */ to: string } export interface SourcePreservingArgs { /** * Indicates whether source column(s) should be preserved on operations that normally drop the source(s). * E.g., merge, fold, spread, etc. */ preserveSource?: boolean } export interface Criteria { /** * Comparison value for the column. This is either a direct value to compare if strategy is * ComparisonStrategy.Value, or the name of another column to compare if strategy is ComparisonStrategy.Column. * Not required if the operator is self-defining (e.g., 'is empty') */ value?: Value /** * Indicates whether the filter should be directly against a value, * or against the value of another column */ strategy: ComparisonStrategy /** * Filter operator to execute. Note the correct operator for the column data type must be used. */ operator: | NumericComparisonOperator | StringComparisonOperator | BooleanComparisonOperator | DateComparisonOperator } /** * This is a subset of data types available for parsing operations. */ export enum ParseType { /** * Type is a binary boolean (true/false) */ Boolean = 'boolean', /** * Type is a date */ Date = 'date', /** * Type is an integer (whole number) */ Integer = 'int', /** * Type is a decimal (floating point number). * Note that in JavaScript integers and decimals are both represented as \"number\", * but the distinction is helpful for formatting/display and understanding user intent. */ Decimal = 'float', /** * Type is a string of text. */ String = 'string', /** * Type is an array of values. */ Array = 'array', } export enum MathOperator { Add = '+', Subtract = '-', Multiply = '*', Divide = '/', } export enum NumericComparisonOperator { Equals = '=', NotEqual = '!=', LessThan = '<', LessThanOrEqual = '<=', GreaterThan = '>', GreaterThanOrEqual = '>=', IsEmpty = 'is empty', IsNotEmpty = 'is not empty', } export enum DateComparisonOperator { Equals = 'equals', NotEqual = 'is not equal', Before = 'before', After = 'after', IsEmpty = 'is empty', IsNotEmpty = 'is not empty', } export enum StringComparisonOperator { Equals = 'equals', NotEqual = 'is not equal', Contains = 'contains', StartsWith = 'starts with', EndsWith = 'ends with', IsEmpty = 'is empty', IsNotEmpty = 'is not empty', RegularExpression = 'regex', } export enum BooleanComparisonOperator { Equals = 'equals', NotEqual = 'is not equal', IsTrue = 'is true', IsFalse = 'is false', IsEmpty = 'is empty', IsNotEmpty = 'is not empty', } export enum BooleanOperator { /** * Any match sets the result to true */ OR = 'or', /** * All conditions must match for the result to be true */ AND = 'and', /** * None of the conditions can match for the result to be true */ NOR = 'nor', /** * Any number of conditions can match but not all of them for the result to be true */ NAND = 'nand', /** * Every pairwise comparison must contain one true and one false value */ XOR = 'xor', /** * Every pairwise comparison must be two `true` or two `false` to be true */ XNOR = 'xnor', } /** * Indicates the type of set operation to perform across two collections. */ export enum SetOp { /** * Concatenate the two collections together */ Concat = 'concat', /** * Return the union of the two collections */ Union = 'union', /** * Return the intersection of the two collections */ Intersect = 'intersect', /** * Return the difference of the two collections */ Difference = 'difference', } /** * Indicates the comparison type used for a filter operation. * This is done on a row-by-row basis. */ export enum ComparisonStrategy { /** * The comparison value is a literal value */ Value = 'value', /** * The comparison value is the value from the same row in another column */ Column = 'column', } /** * This is the subset of aggregate functions that can operate * on a single field so we don't accommodate additional args. * See https://uwdata.github.io/arquero/api/op#aggregate-functions */ export enum FieldAggregateOperation { /** * Select any value. Implementation-dependent - this could be random, the first found, etc. */ Any = 'any', /** * Count the number of values */ Count = 'count', /** * Count the number of unique values */ CountDistinct = 'distinct', /** * Count only the valid (non-null, non-error) values */ Valid = 'valid', /** * Count only the valid values */ Invalid = 'invalid', /** * Find the max value */ Max = 'max', /** * Find the min value */ Min = 'min', /** * Sum the values */ Sum = 'sum', /** * Compute the product of the values */ Product = 'product', /** * Compute the mean of the values */ Mean = 'mean', /** * Compute the mode of the values */ Mode = 'mode', /** * Compute the median of the values */ Median = 'median', /** * Compute the standard deviation of the values */ StandardDeviation = 'stdev', /** * Compute the population standard deviation of the values */ StandardDeviationPopulation = 'stdevp', /** * Compute the variance of the values */ Variance = 'variance', /** * Collect all of the values into an array */ CreateArray = 'array_agg', /** * Collect all of the unique values into an array */ CreateArrayDistinct = 'array_agg_distinct', } /** * These are operations that perform windowed compute. * See https://uwdata.github.io/arquero/api/op#window-functions */ export enum WindowFunction { RowNumber = 'row_number', Rank = 'rank', PercentRank = 'percent_rank', CumulativeDistribution = 'cume_dist', FirstValue = 'first_value', LastValue = 'last_value', FillDown = 'fill_down', FillUp = 'fill_up', UUID = 'uuid' } /** * These are the available formats for the snapshot verb. */ export enum FileType { Csv = 'csv', Json = 'json', Parquet = 'parquet' } export interface AggregateArgs extends RollupArgs { /** * Column to group by */ groupby: string } /** * Describes the binning technique to use. * See numpy for detailed definitions: https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html */ export enum BinStrategy { Auto = 'auto', Fd = 'fd', Doane = 'doane', Scott = 'scott', Rice = 'rice', Sturges = 'sturges', Sqrt = 'sqrt', FixedCount = 'fixed count', FixedWidth = 'fixed width', } export interface BinArgs extends InputColumnArgs, OutputColumnArgs { /** * Binning technique to use. */ strategy: BinStrategy /** * Indicates whether bins should be rounded in a readable human-friendly way. */ nice?: boolean /** * Fixed number of bins. * Note that the bin placements are inclusive of the bottom boundary and exclusive of the top boundary - * this means there is always one extra bin for the max value when using fixed count. */ fixedcount?: number /** * Exact step size between bins */ fixedwidth?: number /** * Min boundary to categorize values into. * If cell values are below this, they will default to -Infinity unless clamped. */ min?: number /** * Max boundary to categorize values into. * If cell values are above this, they will default to +Infinity unless clamped. */ max?: number /** * If true, values outside of the min/max boundaries will be clamped to those * boundaries rather than +/-Infinity. */ clamped?: boolean /** * If true, the range for each bin will be printed as the cell value instead of the truncated numeric value. */ printRange?: boolean } export interface BinarizeArgs extends FilterArgs, OutputColumnArgs {} export interface BooleanArgs extends InputColumnListArgs, OutputColumnArgs { /** * Boolean comparison type to apply across the list of input column values */ operator: BooleanOperator } export interface ConvertArgs extends InputColumnArgs, OutputColumnArgs { /** * Output type to convert the column values to. */ type: ParseType /** * Radix to use for parsing strings into ints */ radix?: number /** * Delimiter to use for identifying decimals when converting strings to numbers. */ delimiter?: string /** * Format string to use when converting strings to dates. Follows strptime format. */ formatPattern?: string } export type DedupeArgs = Partial<InputColumnListArgs> export interface DeriveArgs extends OutputColumnArgs { /** * Column on the left side of the operation */ column1: string /** * Column on the right side of the operation */ column2: string /** * Math operation to perform row-by-row on the two columns */ operator: MathOperator } export interface EraseArgs extends InputColumnArgs { /** * Value to match and erase (set to undefined) in the column */ value: Value } export interface DestructureArgs extends InputColumnArgs, SourcePreservingArgs { keys?: string[] } export interface EncodeDecodeArgs { /** * Strategy for applying the codebook to the table. */ strategy: CodebookStrategy /** * Codebook to apply to the table. */ codebook: CodebookSchema } export interface FillArgs extends OutputColumnArgs { /** * Value to fill in the new column. All rows will receive this value. */ value: Value } export interface FilterArgs extends Criteria, InputColumnArgs {} export interface FoldArgs extends InputColumnListArgs, SourcePreservingArgs { /** * Two-element array of names for the output [key, value] */ to?: [string, string] } export type GroupbyArgs = InputColumnListArgs export interface ImputeArgs extends InputColumnArgs { /** * Value to fill in empty cells */ value: Value } export interface JoinArgsBase { /** * Column names to join with. * If only one is specified, it will use for both tables. * If none are specified, all matching column names will be used. */ on?: string[] } export interface JoinArgs extends JoinArgsBase { /** * Type of join to perform */ strategy?: JoinStrategy } export enum JoinStrategy { Inner = 'inner', LeftOuter = 'left outer', RightOuter = 'right outer', FullOuter = 'full outer', Cross = 'cross', SemiJoin = 'semi join', AntiJoin = 'anti join', } export enum CodebookStrategy { /** * Only parse data types for each column, per the codebook definition */ DataTypeOnly = 'data type only', /** * Only apply mappings for each column, per the codebook definition */ MappingOnly = 'mapping only', /** * Apply both data type and mapping for each column, per the codebook definition */ DataTypeAndMapping = 'data type and mapping', } export interface LookupArgs extends JoinArgsBase, InputColumnListArgs {} export enum MergeStrategy { /** * Use the first valid value found in the list */ FirstOneWins = 'first one wins', /** * Use the last valid value found in the list */ LastOneWins = 'last one wins', /** * Concat all values into a string */ Concat = 'concat', /** * Concat all values into an array */ CreateArray = 'array', } export interface MergeArgs extends InputColumnListArgs, OutputColumnArgs, SourcePreservingArgs { /** * Strategy to use for merging the input columns */ strategy: MergeStrategy /** * Delimiter to use when merging columns into a string. * This is only necessary if MergeStrategy.Concat is used. * If it is not supplied, the values are just mashed together. */ delimiter?: string } export interface CopyArgs extends InputColumnArgs, OutputColumnArgs {} export interface OnehotArgs extends InputColumnArgs, SourcePreservingArgs { /** * Optional prefixes for the output column names */ prefix?: string } export interface OrderbyArgs { /** * List of ordering instructions to apply */ orders: OrderbyInstruction[] } export interface OrderbyInstruction { /** * Name of the column to order by */ column: string /** * Direction to order by */ direction?: SortDirection } export interface PivotArgs extends InputKeyValueArgs { /** * Aggregate/rollup operation to perform when doing the pivot. */ operation: FieldAggregateOperation } export interface RecodeArgs extends InputColumnArgs, OutputColumnArgs { /** * Mapping of old value to new for the recoding. * Note that the key must be coercible to a string for map lookup. */ mapping: Record<Value, Value> } export type RenameArgs = InputColumnRecordArgs export interface RollupArgs extends InputColumnArgs, OutputColumnArgs { /** * Aggregate/rollup operation */ operation: FieldAggregateOperation } export interface SampleArgs { /** * Number of rows to sample from the table. * This takes precedence over proportion. */ size?: number /** * If table size is unknown ahead of time, specify a proportion of rows to sample. * If size is specified, it will be used instead, otherwise computed from this * proportion using the table.numRows() */ proportion?: number /** * The randomization seed to use for sampling to ensure stable sampling. */ seed?: number /** * Whether to preserve and emit the non-sampled records via the 'remainder' output port. */ emitRemainder?: boolean } export type SelectArgs = InputColumnListArgs export type DropArgs = InputColumnListArgs export interface SpreadArgs extends InputColumnArgs, SourcePreservingArgs { to: string[] } export interface StringsArgs extends InputColumnArgs, OutputColumnArgs {} export interface PrintArgs { message?: string limit?: number } export interface SnapshotArgs { name: string fileType: FileType } export interface StringsReplaceArgs extends StringsArgs { pattern: string replacement: string globalSearch?: boolean caseInsensitive?: boolean } export type UnfoldArgs = InputKeyValueArgs export interface UnhotArgs extends InputColumnListArgs, OutputColumnArgs, SourcePreservingArgs { prefix?: string } export type UnrollArgs = InputColumnArgs export interface WindowArgs extends InputColumnArgs, OutputColumnArgs { /** * Window function to apply to the column. */ operation: WindowFunction } export interface WorkflowArgs { /** * The workflow configuration. */ workflow: WorkflowSchema } Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://microsoft.github.io/graphrag/posts/get_started/", "text": "Get Started GraphRAG Welcome Get Started Developing Indexing Architecture Dataflow CLI Configuration Init command Using Env Vars Using JSON or YAML Fully Custom Template Prompt Tuning Automatic Templating Manual Prompt Tuning Query Local Search Question Generation Global Search CLI Notebooks Global Search Local Search Get Started Requirements Python 3.10-3.12 To get started with the GraphRAG system, you have a few options: üëâ Use the GraphRAG Accelerator solution üëâ Install from pypi . üëâ Use it from source Quickstart To get started with the GraphRAG system we recommend trying the Solution Accelerator package. This provides a user-friendly end-to-end experience with Azure resources. Top-Level Modules Indexing Pipeline Overview Query Engine Overview Overview The following is a simple end-to-end example for using the GraphRAG system.\nIt shows how to use the system to index some text, and then use the indexed data to answer questions about the documents. Install GraphRAG pip install graphrag Running the Indexer Now we need to set up a data project and some initial configuration. Let's set that up. We're using the default configuration mode , which you can customize as needed using a config file , which we recommend, or environment variables . First let's get a sample dataset ready: mkdir -p ./ragtest/input Now let's get a copy of A Christmas Carol by Charles Dickens from a trusted source curl https://www.gutenberg.org/cache/epub/24022/pg24022.txt > ./ragtest/input/book.txt Next we'll inject some required config variables: Set Up Your Workspace Variables First let's make sure to setup the required environment variables. For details on these environment variables, and what environment variables are available, see the variables documentation . To initialize your workspace, let's first run the graphrag.index --init command.\nSince we have already configured a directory named .ragtest` in the previous step, we can run the following command: python -m graphrag.index --init --root ./ragtest This will create two files: .env and settings.yaml in the ./ragtest directory. .env contains the environment variables required to run the GraphRAG pipeline. If you inspect the file, you'll see a single environment variable defined, GRAPHRAG_API_KEY=<API_KEY> . This is the API key for the OpenAI API or Azure OpenAI endpoint. You can replace this with your own API key. settings.yaml contains the settings for the pipeline. You can modify this file to change the settings for the pipeline. OpenAI and Azure OpenAI To run in OpenAI mode, just make sure to update the value of GRAPHRAG_API_KEY in the .env file with your OpenAI API key. Azure OpenAI In addition, Azure OpenAI users should set the following variables in the settings.yaml file. To find the appropriate sections, just search for the llm: configuration, you should see two sections, one for the chat endpoint and one for the embeddings endpoint. Here is an example of how to configure the chat endpoint: type : azure_openai_chat # Or azure_openai_embedding for embeddings api_base : https : //<instance > .openai.azure.com api_version : 2024 - 02 - 15 - preview # You can customize this for other versions deployment_name : <azure_model_deployment_name > For more details about configuring GraphRAG, see the configuration documentation . To learn more about Initialization, refer to the Initialization documentation . For more details about using the CLI, refer to the CLI documentation . Running the Indexing pipeline Finally we'll run the pipeline! python -m graphrag.index --root ./ragtest This process will take some time to run. This depends on the size of your input data, what model you're using, and the text chunk size being used (these can be configured in your .env file).\nOnce the pipeline is complete, you should see a new folder called ./ragtest/output/<timestamp>/artifacts with a series of parquet files. Using the Query Engine Running the Query Engine Now let's ask some questions using this dataset. Here is an example using Global search to ask a high-level question: python -m graphrag.query \\ --root ./ragtest \\ --method global \\ \"What are the top themes in this story?\" Here is an example using Local search to ask a more specific question about a particular character: python -m graphrag.query \\ --root ./ragtest \\ --method local \\ \"Who is Scrooge, and what are his main relationships?\" Please refer to Query Engine docs for detailed information about how to leverage our Local and Global search mechanisms for extracting meaningful insights from data after the Indexer has wrapped up execution. Privacy | Consumer Health Privacy | Cookies | Terms of Use | Trademarks | | GitHub | Solution Accelerator"},
{"url": "https://github.com/microsoft/datashaper", "text": "GitHub - microsoft/datashaper: Processing engine and React components for constructing configuration-based data transformation and processing pipelines. Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / datashaper Public Notifications You must be signed in to change notification settings Fork 17 Star 133 Processing engine and React components for constructing configuration-based data transformation and processing pipelines. microsoft.github.io/datashaper/ License MIT license 133 stars 17 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 24 Pull requests 13 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights microsoft/datashaper This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. main Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit History 3,917 Commits .devcontainer .devcontainer .github .github .vscode .vscode .yarn .yarn docs docs javascript javascript python python schema schema scripts scripts .eslintignore .eslintignore .eslintrc .eslintrc .gitattributes .gitattributes .gitignore .gitignore .vsts-ci.yml .vsts-ci.yml .yarnrc.yml .yarnrc.yml CODEOWNERS CODEOWNERS CODE_OF_CONDUCT.md CODE_OF_CONDUCT.md LICENSE LICENSE README.md README.md SECURITY.md SECURITY.md SUPPORT.md SUPPORT.md biome.json biome.json cspell.config.yaml cspell.config.yaml dictionary.txt dictionary.txt package.json package.json turbo.json turbo.json yarn.lock yarn.lock View all files Repository files navigation README Code of conduct MIT license Security DataShaper This project provides a collection of components for executing processing pipelines, particularly oriented to data wrangling. Detailed documentation is provided in subfolders, with an overview of high-level goals and concepts here. Most of the documentation within individual packages is tailored to developers needing to understand how the code is organized and executed. Higher-level concepts for the project as a whole, constructing workflows, etc. are in the root docs folder. Motivation There are four primary goals of the project: Create a shareable client/server schema for describing data processing steps. This is in the schema folder. TypeScript types and JSONSchema generation is in javascript/schema , and published schemas are copied out to schema along with test cases that are executed by JavaScript and Python builds to ensure parity. Stable released versions of DataShaper schemas are hosted on github.io for permanent reference (described below). Maintain an implementation of a basic client-side wrangling engine (largely based on Arquero ). This is in the javascript/workflow folder. This contains a reactive execution engine, along with individual verb implementations. Maintain a python implementation using common wrangling libraries (e.g., pandas ) for backend or data science deployments. This is in the python folder. The execution engine is less complete than in JavaScript, but has complete verb implementations and test suite parity. A fuller-featured generalized pipeline execution engine is forthcoming. Provide an application framework along with some reusable React components so wrangling operations can be incorporated into web applications easily. This is in the javascript/app-framework and javascript/react folders. Individual documentation for the JavaScript and Python implementations can be found in their respective folders. Broad documentation about building pipelines and the available verbs is available in the docs folder. We currently have seven primary JavaScript packages: app-framework - this provides web application infrastructure for creating data-driven apps with minimal boilerplate. react - this is a set of React components for each verb that you can include in web apps that enable transformation pipeline building. schema - this is a set of core types and associated JSONSchema definitions for formalizing our data package and resource models (including the definitions for table parsing, Codebooks, and Workflows). tables - this is the primary set of functions for loading and parsing data tables, using Arquero under the hood. utilities - this is a set of helpers for working with files, etc., to ease building data wrangling applications. webapp - this is the deployable DataShaper webapp that includes all of the verb components and allows creation, execution, and saving of pipeline JSON files. We also rely on this to demonstrate example code, including a TestApp profile. If you're wondering how to build an app with DataShaper components, start here! workflow - this is the primary engine for pipeline execution. It includes low-level operational primitives to execute a wide variety of relational algebra transformations over Arquero tables. Also note that each JavaScript package has a generated docs folder containing Markdown API documentation extracted from code comments using api-extractor . The Python packages are much simpler, because there is no associated web application and component code. engine - contains the core verb implementations. workflow.py - this is the primary execution engine that loads and interprets pipelines, and iterates through the steps to produce outputs. Schema management We generate JSONSchema for formal project artifacts including resource definitions and workflow specifications. This allows validation by any consumer and/or implementor. Schema versions are published on github.io for permanent reference. Each variant of a schema is hosted in perpetuity with semantic versioning. Aliases to the most recent (unversioned latest) and major revisions are also published. Here are direct links to the latest versions of our primary schemas: Bundle ( types ) ( published schema ) Codebook ( types ) ( published schema ) Data Package ( types ) ( published schema ) Data Table ( types ) ( published schema ) Table Bundle ( types ) ( published schema ) Workflow ( types ) ( published schema ) Note that for the purposes of pipeline development, the workflow schema is primary. The rest are largely used for package management and table bundling in the web application. Creating new verbs For new verbs within the DataShaper toolkit, you must first determine if JavaScript and Python parity is desired. For operations that should be configurable via a UX, a JavaScript implementation is necessary. However, if the verb is primarily useful for data science workflows and has potentially complicated parameters, a Python-only implementation may be fine. We have a preference for parity to reduce confusion and allow for cross-platform execution of any pipelines created with the tool, but also recognize the value of the Python-based execution engine for configuring data science and ETL workflows that will only ever be run server-side. Core verbs Core verbs are built into the toolkit, and should generally have JavaScript and Python parity. Creating these verbs involves the following steps: Schema definition - this is done by authoring TypeScript types in the javascript/schema folder, which are then generated as JSONSchema during a build step. Cross-platform tests - these are defined in schema/fixtures , primarily in the workflow folder. Each fixture includes a workflow.json and an expected output csv file. Executors run in both JavaScript and Python to confirm that outputs match the expected table. JavaScript implementation - verbs are implemented in javascript/workflow/verbs Verb UX - individual verb UX components are in javascript/react Python implementation Verbs are implemented in python/verbs Create a verb file following the json schema as package structure, for example, if in the schema the verbs is defined as: \"verb\" : { \"const\" : \" strings.upper \" , \"type\" : \" string \" } The location of the verb must be in datashaper.engine.verbs.strings.upper . Create a function that replicates the same functionality as the javascript version and use the @verb decorator to make it available to the Workflow engine. The name parameter of the decorator must match the package name defined in the schema. For example: @ verb ( name = \"my_package.upper\" ) def upper ( input : VerbInput , column : str , to : str ):\n    ... Important Note: If a verb already exists with the same name you will get a ValueError , pick a unique name for each verb. For example if you try to create a new \"strings.upper\" you will get a ValueError if you want to create a custom version of this verb you could use \"my_package.upper\" like the example above. Custom verbs The Python implementation supports the use of custom verbs supplied by your application - this allows arbitrary processing pipelines to be built that contain custom logic and processing steps. TODO: document custom verb format Build and test JavaScript You need node and yarn installed Operate from project root Run: yarn Then: yarn build Run the webapp locally: yarn start Python You need Python and poetry installed Operate from python/datashaper folder Run: poetry install Then: poetry run poe test Contributing This project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com . When you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct .\nFor more information see the Code of Conduct FAQ or\ncontact opencode@microsoft.com with any additional questions or comments. Trademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines .\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies. About Processing engine and React components for constructing configuration-based data transformation and processing pipelines. microsoft.github.io/datashaper/ Resources Readme License MIT license Code of conduct Code of conduct Security policy Security policy Activity Custom properties Stars 133 stars Watchers 9 watching Forks 17 forks Report repository Releases No releases published Packages 0 No packages published Contributors 13 Languages TypeScript 64.6% Jupyter Notebook 25.6% Python 9.5% Other 0.3% Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/claims/prompts.py", "text": "graphrag/graphrag/index/graph/extractors/claims/prompts.py at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Files main Breadcrumbs graphrag / graphrag / index / graph / extractors / claims / prompts.py Blame Blame Latest commit History History 61 lines (47 loc) ¬∑ 5.36 KB main Breadcrumbs graphrag / graphrag / index / graph / extractors / claims / prompts.py Top File metadata and controls Code Blame 61 lines (47 loc) ¬∑ 5.36 KB Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License \"\"\"A file containing prompts definition.\"\"\" CLAIM_EXTRACTION_PROMPT = \"\"\" -Target activity- You are an intelligent assistant that helps a human analyst to analyze claims against certain entities presented in a text document. -Goal- Given a text document that is potentially relevant to this activity, an entity specification, and a claim description, extract all entities that match the entity specification and all claims against those entities. -Steps- 1. Extract all named entities that match the predefined entity specification. Entity specification can either be a list of entity names or a list of entity types. 2. For each entity identified in step 1, extract all claims associated with the entity. Claims need to match the specified claim description, and the entity should be the subject of the claim. For each claim, extract the following information: - Subject: name of the entity that is subject of the claim, capitalized. The subject entity is one that committed the action described in the claim. Subject needs to be one of the named entities identified in step 1. - Object: name of the entity that is object of the claim, capitalized. The object entity is one that either reports/handles or is affected by the action described in the claim. If object entity is unknown, use **NONE**. - Claim Type: overall category of the claim, capitalized. Name it in a way that can be repeated across multiple text inputs, so that similar claims share the same claim type - Claim Status: **TRUE**, **FALSE**, or **SUSPECTED**. TRUE means the claim is confirmed, FALSE means the claim is found to be False, SUSPECTED means the claim is not verified. - Claim Description: Detailed description explaining the reasoning behind the claim, together with all the related evidence and references. - Claim Date: Period (start_date, end_date) when the claim was made. Both start_date and end_date should be in ISO-8601 format. If the claim was made on a single date rather than a date range, set the same date for both start_date and end_date. If date is unknown, return **NONE**. - Claim Source Text: List of **all** quotes from the original text that are relevant to the claim. Format each claim as (<subject_entity>{tuple_delimiter}<object_entity>{tuple_delimiter}<claim_type>{tuple_delimiter}<claim_status>{tuple_delimiter}<claim_start_date>{tuple_delimiter}<claim_end_date>{tuple_delimiter}<claim_description>{tuple_delimiter}<claim_source>) 3. Return output in English as a single list of all the claims identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter. 4. When finished, output {completion_delimiter} -Examples- Example 1: Entity specification: organization Claim description: red flags associated with an entity Text: According to an article on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B. The company is owned by Person C who was suspected of engaging in corruption activities in 2015. Output: (COMPANY A{tuple_delimiter}GOVERNMENT AGENCY B{tuple_delimiter}ANTI-COMPETITIVE PRACTICES{tuple_delimiter}TRUE{tuple_delimiter}2022-01-10T00:00:00{tuple_delimiter}2022-01-10T00:00:00{tuple_delimiter}Company A was found to engage in anti-competitive practices because it was fined for bid rigging in multiple public tenders published by Government Agency B according to an article published on 2022/01/10{tuple_delimiter}According to an article published on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B.) {completion_delimiter} Example 2: Entity specification: Company A, Person C Claim description: red flags associated with an entity Text: According to an article on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B. The company is owned by Person C who was suspected of engaging in corruption activities in 2015. Output: (COMPANY A{tuple_delimiter}GOVERNMENT AGENCY B{tuple_delimiter}ANTI-COMPETITIVE PRACTICES{tuple_delimiter}TRUE{tuple_delimiter}2022-01-10T00:00:00{tuple_delimiter}2022-01-10T00:00:00{tuple_delimiter}Company A was found to engage in anti-competitive practices because it was fined for bid rigging in multiple public tenders published by Government Agency B according to an article published on 2022/01/10{tuple_delimiter}According to an article published on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B.) {record_delimiter} (PERSON C{tuple_delimiter}NONE{tuple_delimiter}CORRUPTION{tuple_delimiter}SUSPECTED{tuple_delimiter}2015-01-01T00:00:00{tuple_delimiter}2015-12-30T00:00:00{tuple_delimiter}Person C was suspected of engaging in corruption activities in 2015{tuple_delimiter}The company is owned by Person C who was suspected of engaging in corruption activities in 2015) {completion_delimiter} -Real Data- Use the following input for your answer. Entity specification: {entity_specs} Claim description: {claim_description} Text: {input_text} Output:\"\"\" CONTINUE_PROMPT = \"MANY entities were missed in the last extraction.  Add them below using the same format: \\n \" LOOP_PROMPT = \"It appears some entities may have still been missed.  Answer YES {tuple_delimiter} NO if there are still entities that need to be added. \\n \" Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/graph/prompts.py", "text": "graphrag/graphrag/index/graph/extractors/graph/prompts.py at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Files main Breadcrumbs graphrag / graphrag / index / graph / extractors / graph / prompts.py Blame Blame Latest commit History History 129 lines (113 loc) ¬∑ 8.67 KB main Breadcrumbs graphrag / graphrag / index / graph / extractors / graph / prompts.py Top File metadata and controls Code Blame 129 lines (113 loc) ¬∑ 8.67 KB Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License \"\"\"A file containing prompts definition.\"\"\" GRAPH_EXTRACTION_PROMPT = \"\"\" -Goal- Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities. -Steps- 1. Identify all entities. For each identified entity, extract the following information: - entity_name: Name of the entity, capitalized - entity_type: One of the following types: [{entity_types}] - entity_description: Comprehensive description of the entity's attributes and activities Format each entity as (\"entity\"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>) 2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other. For each pair of related entities, extract the following information: - source_entity: name of the source entity, as identified in step 1 - target_entity: name of the target entity, as identified in step 1 - relationship_description: explanation as to why you think the source entity and the target entity are related to each other - relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity Format each relationship as (\"relationship\"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_strength>) 3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter. 4. When finished, output {completion_delimiter} ###################### -Examples- ###################### Example 1: Entity_types: ORGANIZATION,PERSON Text: The Verdantis's Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%. ###################### Output: (\"entity\"{tuple_delimiter}CENTRAL INSTITUTION{tuple_delimiter}ORGANIZATION{tuple_delimiter}The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday) {record_delimiter} (\"entity\"{tuple_delimiter}MARTIN SMITH{tuple_delimiter}PERSON{tuple_delimiter}Martin Smith is the chair of the Central Institution) {record_delimiter} (\"entity\"{tuple_delimiter}MARKET STRATEGY COMMITTEE{tuple_delimiter}ORGANIZATION{tuple_delimiter}The Central Institution committee makes key decisions about interest rates and the growth of Verdantis's money supply) {record_delimiter} (\"relationship\"{tuple_delimiter}MARTIN SMITH{tuple_delimiter}CENTRAL INSTITUTION{tuple_delimiter}Martin Smith is the Chair of the Central Institution and will answer questions at a press conference{tuple_delimiter}9) {completion_delimiter} ###################### Example 2: Entity_types: ORGANIZATION Text: TechGlobal's (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation's debut on the public markets isn't indicative of how other newly listed companies may perform. TechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones. ###################### Output: (\"entity\"{tuple_delimiter}TECHGLOBAL{tuple_delimiter}ORGANIZATION{tuple_delimiter}TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones) {record_delimiter} (\"entity\"{tuple_delimiter}VISION HOLDINGS{tuple_delimiter}ORGANIZATION{tuple_delimiter}Vision Holdings is a firm that previously owned TechGlobal) {record_delimiter} (\"relationship\"{tuple_delimiter}TECHGLOBAL{tuple_delimiter}VISION HOLDINGS{tuple_delimiter}Vision Holdings formerly owned TechGlobal from 2014 until present{tuple_delimiter}5) {completion_delimiter} ###################### Example 3: Entity_types: ORGANIZATION,GEO,PERSON Text: Five Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia. The swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara. The exchange initiated in Firuzabad's capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara. They were welcomed by senior Aurelian officials and are now on their way to Aurelia's capital, Cashion. The Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia's Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality. ###################### Output: (\"entity\"{tuple_delimiter}FIRUZABAD{tuple_delimiter}GEO{tuple_delimiter}Firuzabad held Aurelians as hostages) {record_delimiter} (\"entity\"{tuple_delimiter}AURELIA{tuple_delimiter}GEO{tuple_delimiter}Country seeking to release hostages) {record_delimiter} (\"entity\"{tuple_delimiter}QUINTARA{tuple_delimiter}GEO{tuple_delimiter}Country that negotiated a swap of money in exchange for hostages) {record_delimiter} {record_delimiter} (\"entity\"{tuple_delimiter}TIRUZIA{tuple_delimiter}GEO{tuple_delimiter}Capital of Firuzabad where the Aurelians were being held) {record_delimiter} (\"entity\"{tuple_delimiter}KROHAARA{tuple_delimiter}GEO{tuple_delimiter}Capital city in Quintara) {record_delimiter} (\"entity\"{tuple_delimiter}CASHION{tuple_delimiter}GEO{tuple_delimiter}Capital city in Aurelia) {record_delimiter} (\"entity\"{tuple_delimiter}SAMUEL NAMARA{tuple_delimiter}PERSON{tuple_delimiter}Aurelian who spent time in Tiruzia's Alhamia Prison) {record_delimiter} (\"entity\"{tuple_delimiter}ALHAMIA PRISON{tuple_delimiter}GEO{tuple_delimiter}Prison in Tiruzia) {record_delimiter} (\"entity\"{tuple_delimiter}DURKE BATAGLANI{tuple_delimiter}PERSON{tuple_delimiter}Aurelian journalist who was held hostage) {record_delimiter} (\"entity\"{tuple_delimiter}MEGGIE TAZBAH{tuple_delimiter}PERSON{tuple_delimiter}Bratinas national and environmentalist who was held hostage) {record_delimiter} (\"relationship\"{tuple_delimiter}FIRUZABAD{tuple_delimiter}AURELIA{tuple_delimiter}Firuzabad negotiated a hostage exchange with Aurelia{tuple_delimiter}2) {record_delimiter} (\"relationship\"{tuple_delimiter}QUINTARA{tuple_delimiter}AURELIA{tuple_delimiter}Quintara brokered the hostage exchange between Firuzabad and Aurelia{tuple_delimiter}2) {record_delimiter} (\"relationship\"{tuple_delimiter}QUINTARA{tuple_delimiter}FIRUZABAD{tuple_delimiter}Quintara brokered the hostage exchange between Firuzabad and Aurelia{tuple_delimiter}2) {record_delimiter} (\"relationship\"{tuple_delimiter}SAMUEL NAMARA{tuple_delimiter}ALHAMIA PRISON{tuple_delimiter}Samuel Namara was a prisoner at Alhamia prison{tuple_delimiter}8) {record_delimiter} (\"relationship\"{tuple_delimiter}SAMUEL NAMARA{tuple_delimiter}MEGGIE TAZBAH{tuple_delimiter}Samuel Namara and Meggie Tazbah were exchanged in the same hostage release{tuple_delimiter}2) {record_delimiter} (\"relationship\"{tuple_delimiter}SAMUEL NAMARA{tuple_delimiter}DURKE BATAGLANI{tuple_delimiter}Samuel Namara and Durke Bataglani were exchanged in the same hostage release{tuple_delimiter}2) {record_delimiter} (\"relationship\"{tuple_delimiter}MEGGIE TAZBAH{tuple_delimiter}DURKE BATAGLANI{tuple_delimiter}Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release{tuple_delimiter}2) {record_delimiter} (\"relationship\"{tuple_delimiter}SAMUEL NAMARA{tuple_delimiter}FIRUZABAD{tuple_delimiter}Samuel Namara was a hostage in Firuzabad{tuple_delimiter}2) {record_delimiter} (\"relationship\"{tuple_delimiter}MEGGIE TAZBAH{tuple_delimiter}FIRUZABAD{tuple_delimiter}Meggie Tazbah was a hostage in Firuzabad{tuple_delimiter}2) {record_delimiter} (\"relationship\"{tuple_delimiter}DURKE BATAGLANI{tuple_delimiter}FIRUZABAD{tuple_delimiter}Durke Bataglani was a hostage in Firuzabad{tuple_delimiter}2) {completion_delimiter} ###################### -Real Data- ###################### Entity_types: {entity_types} Text: {input_text} ###################### Output:\"\"\" CONTINUE_PROMPT = \"MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format: \\n \" LOOP_PROMPT = \"It appears some entities and relationships may have still been missed.  Answer YES | NO if there are still entities or relationships that need to be added. \\n \" Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/community_reports/prompts.py", "text": "graphrag/graphrag/index/graph/extractors/community_reports/prompts.py at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Files main Breadcrumbs graphrag / graphrag / index / graph / extractors / community_reports / prompts.py Blame Blame Latest commit History History 150 lines (115 loc) ¬∑ 8.65 KB main Breadcrumbs graphrag / graphrag / index / graph / extractors / community_reports / prompts.py Top File metadata and controls Code Blame 150 lines (115 loc) ¬∑ 8.65 KB Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License \"\"\"A file containing prompts definition.\"\"\" COMMUNITY_REPORT_PROMPT = \"\"\" You are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network. # Goal Write a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community's key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims. # Report Structure The report should include the following sections: - TITLE: community's name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title. - SUMMARY: An executive summary of the community's overall structure, how its entities are related to each other, and significant information associated with its entities. - IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community. - RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating. - DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive. Return output as a well-formed JSON-formatted string with the following format: {{ \"title\": <report_title>, \"summary\": <executive_summary>, \"rating\": <impact_severity_rating>, \"rating_explanation\": <rating_explanation>, \"findings\": [ {{ \"summary\":<insight_1_summary>, \"explanation\": <insight_1_explanation> }}, {{ \"summary\":<insight_2_summary>, \"explanation\": <insight_2_explanation> }} ] }} # Grounding Rules Points supported by data should list their data references as follows: \"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\" Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more. For example: \"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)].\" where 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record. Do not include information where the supporting evidence for it is not provided. # Example Input ----------- Text: Entities id,entity,description 5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March 6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza Relationships id,source,target,description 37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March 38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza 39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza 40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza 41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march 43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March Output: {{ \"title\": \"Verdant Oasis Plaza and Unity March\", \"summary\": \"The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.\", \"rating\": 5.0, \"rating_explanation\": \"The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.\", \"findings\": [ {{ \"summary\": \"Verdant Oasis Plaza as the central location\", \"explanation\": \"Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza's association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]\" }}, {{ \"summary\": \"Harmony Assembly's role in the community\", \"explanation\": \"Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]\" }}, {{ \"summary\": \"Unity March as a significant event\", \"explanation\": \"The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community's dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]\" }}, {{ \"summary\": \"Role of Tribune Spotlight\", \"explanation\": \"Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]\" }} ] }} # Real Data Use the following text for your answer. Do not make anything up in your answer. Text: {input_text} The report should include the following sections: - TITLE: community's name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title. - SUMMARY: An executive summary of the community's overall structure, how its entities are related to each other, and significant information associated with its entities. - IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community. - RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating. - DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive. Return output as a well-formed JSON-formatted string with the following format: {{ \"title\": <report_title>, \"summary\": <executive_summary>, \"rating\": <impact_severity_rating>, \"rating_explanation\": <rating_explanation>, \"findings\": [ {{ \"summary\":<insight_1_summary>, \"explanation\": <insight_1_explanation> }}, {{ \"summary\":<insight_2_summary>, \"explanation\": <insight_2_explanation> }} ] }} # Grounding Rules Points supported by data should list their data references as follows: \"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\" Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more. For example: \"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)].\" where 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record. Do not include information where the supporting evidence for it is not provided. Output:\"\"\" Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/summarize/prompts.py", "text": "graphrag/graphrag/index/graph/extractors/summarize/prompts.py at main ¬∑ microsoft/graphrag ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert microsoft / graphrag Public Notifications You must be signed in to change notification settings Fork 1.2k Star 14.2k Code Issues 104 Pull requests 33 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Files main Breadcrumbs graphrag / graphrag / index / graph / extractors / summarize / prompts.py Blame Blame Latest commit History History 19 lines (16 loc) ¬∑ 823 Bytes main Breadcrumbs graphrag / graphrag / index / graph / extractors / summarize / prompts.py Top File metadata and controls Code Blame 19 lines (16 loc) ¬∑ 823 Bytes Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Copyright (c) 2024 Microsoft Corporation. # Licensed under the MIT License \"\"\"A file containing prompts definition.\"\"\" SUMMARIZE_PROMPT = \"\"\" You are a helpful assistant responsible for generating a comprehensive summary of the data provided below. Given one or two entities, and a list of descriptions, all related to the same entity or group of entities. Please concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions. If the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary. Make sure it is written in third person, and include the entity names so we the have full context. ####### -Data- Entities: {entity_name} Description List: {description_list} ####### Output: \"\"\" Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://learn.microsoft.com/en-us/azure/ai-services/openai/reference", "text": "Azure OpenAI Service REST API reference - Azure OpenAI | Microsoft Learn Skip to main content This browser is no longer supported. Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support. Download Microsoft Edge More info about Internet Explorer and Microsoft Edge Table of contents Exit focus mode Read in English Save Table of contents Read in English Save Add to Plan Edit Share via Facebook x.com LinkedIn Email Print Table of contents Azure OpenAI Service REST API reference Article 07/09/2024 24 contributors Feedback In this article This article provides details on the inference REST API endpoints for Azure OpenAI. API specs Managing and interacting with Azure OpenAI models and resources is divided across three primary API surfaces: Control plane Data plane - authoring Data plane - inference Each API surface/specification encapsulates a different set of Azure OpenAI capabilities. Each API has its own unique set of preview and stable/generally available (GA) API releases. Preview releases currently tend to follow a monthly cadence. API Latest preview release Latest GA release Specifications Description Control plane 2024-04-01-preview 2023-05-01 Spec files Azure OpenAI shares a common control plane with all other Azure AI Services. The control plane API is used for things like creating Azure OpenAI resources , model deployment , and other higher level resource management tasks. The control plane also governs what is possible to do with capabilities like Azure Resource Manager, Bicep, Terraform, and Azure CLI. Data plane - authoring 2024-05-01-preview 2024-06-01 Spec files The data plane authoring API controls fine-tuning , file-upload , ingestion jobs , and certain model level queries Data plane - inference 2024-05-01-preview 2024-06-01 Spec files The data plane inference API provides the inference capabilities/endpoints for features like completions, chat completions, embeddings, speech/whisper, on your data, Dall-e, assistants, etc. Authentication Azure OpenAI provides two methods for authentication. You can use  either API Keys or Microsoft Entra ID. API Key authentication : For this type of authentication, all API requests must include the API Key in the api-key HTTP header. The Quickstart provides guidance for how to make calls with this type of authentication. Microsoft Entra ID authentication : You can authenticate an API call using a Microsoft Entra token. Authentication tokens are included in a request as the Authorization header. The token provided must be preceded by Bearer , for example Bearer YOUR_AUTH_TOKEN . You can read our how-to guide on authenticating with Microsoft Entra ID . REST API versioning The service APIs are versioned using the api-version query parameter. All versions follow the YYYY-MM-DD date structure. For example: POST https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/completions?api-version=2024-06-01 Data plane inference The rest of the article covers the latest GA release of the Azure OpenAI data plane inference specification, 2024-06-01 . If you're looking for documentation on the latest preview API release, refer to the latest preview data plane inference API Completions POST https://{endpoint}/openai/deployments/{deployment-id}/completions?api-version=2024-06-01 Creates a completion for the provided prompt, parameters and chosen model. URI Parameters Name In Required Type Description endpoint path Yes string url Supported Azure OpenAI endpoints (protocol and hostname, for example: https://aoairesource.openai.azure.com . Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com deployment-id path Yes string Deployment id of the model which was deployed. api-version query Yes string API version Request Header Name Required Type Description api-key True string Provide Azure OpenAI API key here Request Body Content-Type : application/json Name Type Description Required Default prompt string or array The prompt(s) to generate completions for, encoded as a string or array of strings. Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt isn'tspecified the model will generate as if from the beginning of a new document. Maximum allowed size of string list is 2048. No max_tokens integer The token count of your prompt plus max_tokens can't exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Has minimum of 0. No 16 temperature number What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (arg max sampling) for ones with a well-defined answer. We generally recommend altering this or top_p but not both. No 1 top_p number An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. No 1 logit_bias object Defaults to null. Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {\"50256\":-100} to prevent the <|endoftext|> token from being generated. No user string A unique identifier representing your end-user, which can help monitoring and detecting abuse No n integer How many completions to generate for each prompt. Minimum of 1 and maximum of 128 allowed. Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. No 1 stream boolean Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. No False logprobs integer Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response. Minimum of 0 and maximum of 5 allowed. No None suffix string The suffix that comes after a completion of inserted text. No echo boolean Echo back the prompt in addition to the completion No False stop string or array Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. No completion_config string No presence_penalty number Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. No 0 frequency_penalty number Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. No 0 best_of integer Generates best_of completions server-side and returns the \"best\" (defined as the one with the highest log probability per token). Results can't be streamed. When used with n, best_of controls the number of candidate completions and n specifies how many to return - best_of must be greater than n. Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. Has maximum value of 128. No Responses Name Type Description Required Default id string Yes object string Yes created integer Yes model string Yes prompt_filter_results promptFilterResults Content filtering results for zero or more prompts in the request. In a streaming request, results for different prompts may arrive at different times or in different orders. No choices array Yes usage object No Properties for usage completion_tokens Name Type Description Default completion_tokens number prompt_tokens number total_tokens number Status Code: 200 Description : OK Content-Type Type Description application/json object Status Code: default Description : Service unavailable Content-Type Type Description application/json errorResponse Examples Example Creates a completion for the provided prompt, parameters and chosen model. POST https://{endpoint}/openai/deployments/{deployment-id}/completions?api-version=2024-06-01\n\n{\n \"prompt\": [\n  \"tell me a joke about mango\"\n ],\n \"max_tokens\": 32,\n \"temperature\": 1.0,\n \"n\": 1\n} Responses :\nStatus Code: 200 {\n  \"body\": {\n    \"id\": \"cmpl-7QmVI15qgYVllxK0FtxVGG6ywfzaq\",\n    \"created\": 1686617332,\n    \"choices\": [\n      {\n        \"text\": \"es\\n\\nWhat do you call a mango who's in charge?\\n\\nThe head mango.\",\n        \"index\": 0,\n        \"finish_reason\": \"stop\",\n        \"logprobs\": null\n      }\n    ],\n    \"usage\": {\n      \"completion_tokens\": 20,\n      \"prompt_tokens\": 6,\n      \"total_tokens\": 26\n    }\n  }\n} Embeddings POST https://{endpoint}/openai/deployments/{deployment-id}/embeddings?api-version=2024-06-01 Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms. URI Parameters Name In Required Type Description endpoint path Yes string url Supported Azure OpenAI endpoints (protocol and hostname, for example: https://aoairesource.openai.azure.com . Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com deployment-id path Yes string api-version query Yes string API version Request Header Name Required Type Description api-key True string Provide Azure OpenAI API key here Request Body Content-Type : application/json Name Type Description Required Default input string or array Input text to get embeddings for, encoded as a string. To get embeddings for multiple inputs in a single request, pass an array of strings. Each input must not exceed 2048 tokens in length. Unless you're embedding code, we suggest replacing newlines (\\n) in your input with a single space, as we have observed inferior results when newlines are present. Yes user string A unique identifier representing your end-user, which can help monitoring and detecting abuse. No input_type string input type of embedding search to use No encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . No dimensions integer The number of dimensions the resulting output embeddings should have. Only supported in text-embedding-3 and later models. No Responses Name Type Description Required Default object string Yes model string Yes data array Yes usage object Yes Properties for usage prompt_tokens Name Type Description Default prompt_tokens integer total_tokens Name Type Description Default total_tokens integer Status Code: 200 Description : OK Content-Type Type Description application/json object Examples Example Return the embeddings for a given prompt. POST https://{endpoint}/openai/deployments/{deployment-id}/embeddings?api-version=2024-06-01\n\n{\n \"input\": [\n  \"this is a test\"\n ]\n} Responses :\nStatus Code: 200 {\n  \"body\": {\n    \"data\": [\n      {\n        \"index\": 0,\n        \"embedding\": [\n          -0.012838088,\n          -0.007421397,\n          -0.017617522,\n          -0.028278312,\n          -0.018666342,\n          0.01737855,\n          -0.01821495,\n          -0.006950092,\n          -0.009937238,\n          -0.038580645,\n          0.010674067,\n          0.02412286,\n          -0.013647936,\n          0.013189907,\n          0.0021125758,\n          0.012406612,\n          0.020790534,\n          0.00074595667,\n          0.008397198,\n        ]\n      }\n    ],\n    \"usage\": {\n      \"prompt_tokens\": 4,\n      \"total_tokens\": 4\n    }\n  }\n} Chat completions POST https://{endpoint}/openai/deployments/{deployment-id}/chat/completions?api-version=2024-06-01 Creates a completion for the chat message URI Parameters Name In Required Type Description endpoint path Yes string url Supported Azure OpenAI endpoints (protocol and hostname, for example: https://aoairesource.openai.azure.com . Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com deployment-id path Yes string Deployment id of the model which was deployed. api-version query Yes string API version Request Header Name Required Type Description api-key True string Provide Azure OpenAI API key here Request Body Content-Type : application/json Name Type Description Required Default temperature number What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both. No 1 top_p number An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. No 1 stream boolean If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. No False stop string or array Up to 4 sequences where the API will stop generating further tokens. No max_tokens integer The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens). No 4096 presence_penalty number Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. No 0 frequency_penalty number Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. No 0 logit_bias object Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. No user string A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse. No messages array A list of messages comprising the conversation so far. Example Python code . No data_sources array The configuration entries for Azure OpenAI chat extensions that use them. This additional specification is only compatible with Azure OpenAI. No n integer How many chat completion choices to generate for each input message. No 1 seed integer If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism isn'tguaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend. No 0 logprobs boolean Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . This option is currently not available on the gpt-4-vision-preview model. No False top_logprobs integer An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. No response_format object An object specifying the format that the model must output. Used to enable JSON mode. No tools array A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. No tool_choice chatCompletionToolChoiceOption Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that function. No functions array Deprecated in favor of tools . A list of functions the model may generate JSON inputs for. No function_call string or object Deprecated in favor of tool_choice . Controls how the model responds to function calls. \"none\" means the model doesn't call a function, and responds to the end-user. \"auto\" means the model can pick between an end-user or calling a function.  Specifying a particular function via {\"name\":\\ \"my_function\"} forces the model to call that function. \"none\" is the default when no functions are present. \"auto\" is the default if functions are present. No Properties for response_format Type Name Type Description Default type chatCompletionResponseFormat Setting to json_object enables JSON mode. This guarantees that the message the model generates is valid JSON. text Responses Status Code: 200 Description : OK Content-Type Type Description application/json createChatCompletionResponse Status Code: default Description : Service unavailable Content-Type Type Description application/json errorResponse Examples Example Creates a completion for the provided prompt, parameters and chosen model. POST https://{endpoint}/openai/deployments/{deployment-id}/chat/completions?api-version=2024-06-01\n\n{\n \"messages\": [\n  {\n   \"role\": \"system\",\n   \"content\": \"you're a helpful assistant that talks like a pirate\"\n  },\n  {\n   \"role\": \"user\",\n   \"content\": \"can you tell me how to care for a parrot?\"\n  }\n ]\n} Responses :\nStatus Code: 200 {\n  \"body\": {\n    \"id\": \"chatcmpl-7R1nGnsXO8n4oi9UPz2f3UHdgAYMn\",\n    \"created\": 1686676106,\n    \"choices\": [\n      {\n        \"index\": 0,\n        \"finish_reason\": \"stop\",\n        \"message\": {\n          \"role\": \"assistant\",\n          \"content\": \"Ahoy matey! So ye be wantin' to care for a fine squawkin' parrot, eh? Well, shiver me timbers, let ol' Cap'n Assistant share some wisdom with ye! Here be the steps to keepin' yer parrot happy 'n healthy:\\n\\n1. Secure a sturdy cage: Yer parrot be needin' a comfortable place to lay anchor! Be sure ye get a sturdy cage, at least double the size of the bird's wingspan, with enough space to spread their wings, yarrrr!\\n\\n2. Perches 'n toys: Aye, parrots need perches of different sizes, shapes, 'n textures to keep their feet healthy. Also, a few toys be helpin' to keep them entertained 'n their minds stimulated, arrrh!\\n\\n3. Proper grub: Feed yer feathered friend a balanced diet of high-quality pellets, fruits, 'n veggies to keep 'em strong 'n healthy. Give 'em fresh water every day, or ye\\u00e2\\u20ac\\u2122ll have a scurvy bird on yer hands!\\n\\n4. Cleanliness: Swab their cage deck! Clean their cage on a regular basis: fresh water 'n food daily, the floor every couple of days, 'n a thorough scrubbing ev'ry few weeks, so the bird be livin' in a tidy haven, arrhh!\\n\\n5. Socialize 'n train: Parrots be a sociable lot, arrr! Exercise 'n interact with 'em daily to create a bond 'n maintain their mental 'n physical health. Train 'em with positive reinforcement, treat 'em kindly, yarrr!\\n\\n6. Proper rest: Yer parrot be needin' \\u00e2\\u20ac\\u2122bout 10-12 hours o' sleep each night. Cover their cage 'n let them slumber in a dim, quiet quarter for a proper night's rest, ye scallywag!\\n\\n7. Keep a weather eye open for illness: Birds be hidin' their ailments, arrr! Be watchful for signs of sickness, such as lethargy, loss of appetite, puffin' up, or change in droppings, and make haste to a vet if need be.\\n\\n8. Provide fresh air 'n avoid toxins: Parrots be sensitive to draft and pollutants. Keep yer quarters well ventilated, but no drafts, arrr! Be mindful of toxins like Teflon fumes, candles, or air fresheners.\\n\\nSo there ye have it, me hearty! With proper care 'n commitment, yer parrot will be squawkin' \\\"Yo-ho-ho\\\" for many years to come! Good luck, sailor, and may the wind be at yer back!\"\n        }\n      }\n    ],\n    \"usage\": {\n      \"completion_tokens\": 557,\n      \"prompt_tokens\": 33,\n      \"total_tokens\": 590\n    }\n  }\n} Example Creates a completion based on Azure Search data and system-assigned managed identity. POST https://{endpoint}/openai/deployments/{deployment-id}/chat/completions?api-version=2024-06-01\n\n{\n \"messages\": [\n  {\n   \"role\": \"user\",\n   \"content\": \"can you tell me how to care for a dog?\"\n  }\n ],\n \"data_sources\": [\n  {\n   \"type\": \"azure_search\",\n   \"parameters\": {\n    \"endpoint\": \"https://your-search-endpoint.search.windows.net/\",\n    \"index_name\": \"{index name}\",\n    \"authentication\": {\n     \"type\": \"system_assigned_managed_identity\"\n    }\n   }\n  }\n ]\n} Responses :\nStatus Code: 200 {\n  \"body\": {\n    \"id\": \"chatcmpl-7R1nGnsXO8n4oi9UPz2f3UHdgAYMn\",\n    \"created\": 1686676106,\n    \"choices\": [\n      {\n        \"index\": 0,\n        \"finish_reason\": \"stop\",\n        \"message\": {\n          \"role\": \"assistant\",\n          \"content\": \"Content of the completion [doc1].\",\n          \"context\": {\n            \"citations\": [\n              {\n                \"content\": \"Citation content.\",\n                \"title\": \"Citation Title\",\n                \"filepath\": \"contoso.txt\",\n                \"url\": \"https://contoso.blob.windows.net/container/contoso.txt\",\n                \"chunk_id\": \"0\"\n              }\n            ],\n            \"intent\": \"dog care\"\n          }\n        }\n      }\n    ],\n    \"usage\": {\n      \"completion_tokens\": 557,\n      \"prompt_tokens\": 33,\n      \"total_tokens\": 590\n    }\n  }\n} Example Creates a completion based on Azure Search vector data, previous assistant message and user-assigned managed identity. POST https://{endpoint}/openai/deployments/{deployment-id}/chat/completions?api-version=2024-06-01\n\n{\n \"messages\": [\n  {\n   \"role\": \"user\",\n   \"content\": \"can you tell me how to care for a cat?\"\n  },\n  {\n   \"role\": \"assistant\",\n   \"content\": \"Content of the completion [doc1].\",\n   \"context\": {\n    \"intent\": \"cat care\"\n   }\n  },\n  {\n   \"role\": \"user\",\n   \"content\": \"how about dog?\"\n  }\n ],\n \"data_sources\": [\n  {\n   \"type\": \"azure_search\",\n   \"parameters\": {\n    \"endpoint\": \"https://your-search-endpoint.search.windows.net/\",\n    \"authentication\": {\n     \"type\": \"user_assigned_managed_identity\",\n     \"managed_identity_resource_id\": \"/subscriptions/{subscription-id}/resourceGroups/{resource-group}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{resource-name}\"\n    },\n    \"index_name\": \"{index name}\",\n    \"query_type\": \"vector\",\n    \"embedding_dependency\": {\n     \"type\": \"deployment_name\",\n     \"deployment_name\": \"{embedding deployment name}\"\n    },\n    \"in_scope\": true,\n    \"top_n_documents\": 5,\n    \"strictness\": 3,\n    \"role_information\": \"you're an AI assistant that helps people find information.\",\n    \"fields_mapping\": {\n     \"content_fields_separator\": \"\\\\n\",\n     \"content_fields\": [\n      \"content\"\n     ],\n     \"filepath_field\": \"filepath\",\n     \"title_field\": \"title\",\n     \"url_field\": \"url\",\n     \"vector_fields\": [\n      \"contentvector\"\n     ]\n    }\n   }\n  }\n ]\n} Responses :\nStatus Code: 200 {\n  \"body\": {\n    \"id\": \"chatcmpl-7R1nGnsXO8n4oi9UPz2f3UHdgAYMn\",\n    \"created\": 1686676106,\n    \"choices\": [\n      {\n        \"index\": 0,\n        \"finish_reason\": \"stop\",\n        \"message\": {\n          \"role\": \"assistant\",\n          \"content\": \"Content of the completion [doc1].\",\n          \"context\": {\n            \"citations\": [\n              {\n                \"content\": \"Citation content 2.\",\n                \"title\": \"Citation Title 2\",\n                \"filepath\": \"contoso2.txt\",\n                \"url\": \"https://contoso.blob.windows.net/container/contoso2.txt\",\n                \"chunk_id\": \"0\"\n              }\n            ],\n            \"intent\": \"dog care\"\n          }\n        }\n      }\n    ],\n    \"usage\": {\n      \"completion_tokens\": 557,\n      \"prompt_tokens\": 33,\n      \"total_tokens\": 590\n    }\n  }\n} Example Creates a completion for the provided Azure Cosmos DB. POST https://{endpoint}/openai/deployments/{deployment-id}/chat/completions?api-version=2024-06-01\n\n{\n \"messages\": [\n  {\n   \"role\": \"user\",\n   \"content\": \"can you tell me how to care for a dog?\"\n  }\n ],\n \"data_sources\": [\n  {\n   \"type\": \"azure_cosmos_db\",\n   \"parameters\": {\n    \"authentication\": {\n     \"type\": \"connection_string\",\n     \"connection_string\": \"mongodb+srv://rawantest:{password}$@{cluster-name}.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000\"\n    },\n    \"database_name\": \"vectordb\",\n    \"container_name\": \"azuredocs\",\n    \"index_name\": \"azuredocindex\",\n    \"embedding_dependency\": {\n     \"type\": \"deployment_name\",\n     \"deployment_name\": \"{embedding deployment name}\"\n    },\n    \"fields_mapping\": {\n     \"content_fields\": [\n      \"content\"\n     ],\n     \"vector_fields\": [\n      \"contentvector\"\n     ]\n    }\n   }\n  }\n ]\n} Responses :\nStatus Code: 200 {\n  \"body\": {\n    \"id\": \"chatcmpl-7R1nGnsXO8n4oi9UPz2f3UHdgAYMn\",\n    \"created\": 1686676106,\n    \"choices\": [\n      {\n        \"index\": 0,\n        \"finish_reason\": \"stop\",\n        \"message\": {\n          \"role\": \"assistant\",\n          \"content\": \"Content of the completion [doc1].\",\n          \"context\": {\n            \"citations\": [\n              {\n                \"content\": \"Citation content.\",\n                \"title\": \"Citation Title\",\n                \"filepath\": \"contoso.txt\",\n                \"url\": \"https://contoso.blob.windows.net/container/contoso.txt\",\n                \"chunk_id\": \"0\"\n              }\n            ],\n            \"intent\": \"dog care\"\n          }\n        }\n      }\n    ],\n    \"usage\": {\n      \"completion_tokens\": 557,\n      \"prompt_tokens\": 33,\n      \"total_tokens\": 590\n    }\n  }\n} Transcriptions POST https://{endpoint}/openai/deployments/{deployment-id}/audio/transcriptions?api-version=2024-06-01 Transcribes audio into the input language. URI Parameters Name In Required Type Description endpoint path Yes string url Supported Azure OpenAI endpoints (protocol and hostname, for example: https://aoairesource.openai.azure.com . Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com deployment-id path Yes string Deployment id of the whisper model. api-version query Yes string API version Request Header Name Required Type Description api-key True string Provide Azure OpenAI API key here Request Body Content-Type : multipart/form-data Name Type Description Required Default file string The audio file object to transcribe. Yes prompt string An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. No response_format audioResponseFormat Defines the format of the output. No temperature number The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. No 0 language string The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency. No Responses Status Code: 200 Description : OK Content-Type Type Description application/json audioResponse or audioVerboseResponse text/plain string Transcribed text in the output format (when response_format was one of text, vtt or srt). Examples Example Gets transcribed text and associated metadata from provided spoken audio data. POST https://{endpoint}/openai/deployments/{deployment-id}/audio/transcriptions?api-version=2024-06-01 Responses :\nStatus Code: 200 {\n  \"body\": {\n    \"text\": \"A structured object when requesting json or verbose_json\"\n  }\n} Example Gets transcribed text and associated metadata from provided spoken audio data. POST https://{endpoint}/openai/deployments/{deployment-id}/audio/transcriptions?api-version=2024-06-01\n\n\"---multipart-boundary\\nContent-Disposition: form-data; name=\\\"file\\\"; filename=\\\"file.wav\\\"\\nContent-Type: application/octet-stream\\n\\nRIFF..audio.data.omitted\\n---multipart-boundary--\" Responses :\nStatus Code: 200 {\n  \"type\": \"string\",\n  \"example\": \"plain text when requesting text, srt, or vtt\"\n} Translations POST https://{endpoint}/openai/deployments/{deployment-id}/audio/translations?api-version=2024-06-01 Transcribes and translates input audio into English text. URI Parameters Name In Required Type Description endpoint path Yes string url Supported Azure OpenAI endpoints (protocol and hostname, for example: https://aoairesource.openai.azure.com . Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com deployment-id path Yes string Deployment id of the whisper model which was deployed. api-version query Yes string API version Request Header Name Required Type Description api-key True string Provide Azure OpenAI API key here Request Body Content-Type : multipart/form-data Name Type Description Required Default file string The audio file to translate. Yes prompt string An optional text to guide the model's style or continue a previous audio segment. The prompt should be in English. No response_format audioResponseFormat Defines the format of the output. No temperature number The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. No 0 Responses Status Code: 200 Description : OK Content-Type Type Description application/json audioResponse or audioVerboseResponse text/plain string Transcribed text in the output format (when response_format was one of text, vtt or srt). Examples Example Gets English language transcribed text and associated metadata from provided spoken audio data. POST https://{endpoint}/openai/deployments/{deployment-id}/audio/translations?api-version=2024-06-01\n\n\"---multipart-boundary\\nContent-Disposition: form-data; name=\\\"file\\\"; filename=\\\"file.wav\\\"\\nContent-Type: application/octet-stream\\n\\nRIFF..audio.data.omitted\\n---multipart-boundary--\" Responses :\nStatus Code: 200 {\n  \"body\": {\n    \"text\": \"A structured object when requesting json or verbose_json\"\n  }\n} Example Gets English language transcribed text and associated metadata from provided spoken audio data. POST https://{endpoint}/openai/deployments/{deployment-id}/audio/translations?api-version=2024-06-01\n\n\"---multipart-boundary\\nContent-Disposition: form-data; name=\\\"file\\\"; filename=\\\"file.wav\\\"\\nContent-Type: application/octet-stream\\n\\nRIFF..audio.data.omitted\\n---multipart-boundary--\" Responses :\nStatus Code: 200 {\n  \"type\": \"string\",\n  \"example\": \"plain text when requesting text, srt, or vtt\"\n} Image generation POST https://{endpoint}/openai/deployments/{deployment-id}/images/generations?api-version=2024-06-01 Generates a batch of images from a text caption on a given DALLE model deployment URI Parameters Name In Required Type Description endpoint path Yes string url Supported Azure OpenAI endpoints (protocol and hostname, for example: https://aoairesource.openai.azure.com . Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com deployment-id path Yes string Deployment id of the dall-e model which was deployed. api-version query Yes string API version Request Header Name Required Type Description api-key True string Provide Azure OpenAI API key here Request Body Content-Type : application/json Name Type Description Required Default prompt string A text description of the desired image(s). The maximum length is 4000 characters. Yes n integer The number of images to generate. No 1 size imageSize The size of the generated images. No 1024x1024 response_format imagesResponseFormat The format in which the generated images are returned. No url user string A unique identifier representing your end-user, which can help to monitor and detect abuse. No quality imageQuality The quality of the image that will be generated. No standard style imageStyle The style of the generated images. No vivid Responses Status Code: 200 Description : Ok Content-Type Type Description application/json generateImagesResponse Status Code: default Description : An error occurred. Content-Type Type Description application/json dalleErrorResponse Examples Example Creates images given a prompt. POST https://{endpoint}/openai/deployments/{deployment-id}/images/generations?api-version=2024-06-01\n\n{\n \"prompt\": \"In the style of WordArt, Microsoft Clippy wearing a cowboy hat.\",\n \"n\": 1,\n \"style\": \"natural\",\n \"quality\": \"standard\"\n} Responses :\nStatus Code: 200 {\n  \"body\": {\n    \"created\": 1698342300,\n    \"data\": [\n      {\n        \"revised_prompt\": \"A vivid, natural representation of Microsoft Clippy wearing a cowboy hat.\",\n        \"prompt_filter_results\": {\n          \"sexual\": {\n            \"severity\": \"safe\",\n            \"filtered\": false\n          },\n          \"violence\": {\n            \"severity\": \"safe\",\n            \"filtered\": false\n          },\n          \"hate\": {\n            \"severity\": \"safe\",\n            \"filtered\": false\n          },\n          \"self_harm\": {\n            \"severity\": \"safe\",\n            \"filtered\": false\n          },\n          \"profanity\": {\n            \"detected\": false,\n            \"filtered\": false\n          }\n        },\n        \"url\": \"https://dalletipusw2.blob.core.windows.net/private/images/e5451cc6-b1ad-4747-bd46-b89a3a3b8bc3/generated_00.png?se=2023-10-27T17%3A45%3A09Z&...\",\n        \"content_filter_results\": {\n          \"sexual\": {\n            \"severity\": \"safe\",\n            \"filtered\": false\n          },\n          \"violence\": {\n            \"severity\": \"safe\",\n            \"filtered\": false\n          },\n          \"hate\": {\n            \"severity\": \"safe\",\n            \"filtered\": false\n          },\n          \"self_harm\": {\n            \"severity\": \"safe\",\n            \"filtered\": false\n          }\n        }\n      }\n    ]\n  }\n} Components errorResponse Name Type Description Required Default error error No errorBase Name Type Description Required Default code string No message string No Error Name Type Description Required Default param string No type string No inner_error innerError Inner error with additional details. No innerError Inner error with additional details. Name Type Description Required Default code innerErrorCode Error codes for the inner error object. No content_filter_results contentFilterPromptResults Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id. No innerErrorCode Error codes for the inner error object. Description : Error codes for the inner error object. Type : string Default : Enum Name : InnerErrorCode Enum Values : Value Description ResponsibleAIPolicyViolation The prompt violated one of more content filter rules. dalleErrorResponse Name Type Description Required Default error dalleError No dalleError Name Type Description Required Default param string No type string No inner_error dalleInnerError Inner error with additional details. No dalleInnerError Inner error with additional details. Name Type Description Required Default code innerErrorCode Error codes for the inner error object. No content_filter_results dalleFilterResults Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id. No revised_prompt string The prompt that was used to generate the image, if there was any revision to the prompt. No contentFilterResultBase Name Type Description Required Default filtered boolean Yes contentFilterSeverityResult Name Type Description Required Default filtered boolean Yes severity string No contentFilterDetectedResult Name Type Description Required Default filtered boolean Yes detected boolean No contentFilterDetectedWithCitationResult Name Type Description Required Default citation object No Properties for citation URL Name Type Description Default URL string License Name Type Description Default license string contentFilterResultsBase Information about the content filtering results. Name Type Description Required Default sexual contentFilterSeverityResult No violence contentFilterSeverityResult No hate contentFilterSeverityResult No self_harm contentFilterSeverityResult No profanity contentFilterDetectedResult No error errorBase No contentFilterPromptResults Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id. Name Type Description Required Default sexual contentFilterSeverityResult No violence contentFilterSeverityResult No hate contentFilterSeverityResult No self_harm contentFilterSeverityResult No profanity contentFilterDetectedResult No error errorBase No jailbreak contentFilterDetectedResult No contentFilterChoiceResults Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about third-party text and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id. Name Type Description Required Default sexual contentFilterSeverityResult No violence contentFilterSeverityResult No hate contentFilterSeverityResult No self_harm contentFilterSeverityResult No profanity contentFilterDetectedResult No error errorBase No protected_material_text contentFilterDetectedResult No protected_material_code contentFilterDetectedWithCitationResult No promptFilterResult Content filtering results for a single prompt in the request. Name Type Description Required Default prompt_index integer No content_filter_results contentFilterPromptResults Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id. No promptFilterResults Content filtering results for zero or more prompts in the request. In a streaming request, results for different prompts may arrive at different times or in different orders. dalleContentFilterResults Information about the content filtering results. Name Type Description Required Default sexual contentFilterSeverityResult No violence contentFilterSeverityResult No hate contentFilterSeverityResult No self_harm contentFilterSeverityResult No dalleFilterResults Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id. Name Type Description Required Default sexual contentFilterSeverityResult No violence contentFilterSeverityResult No hate contentFilterSeverityResult No self_harm contentFilterSeverityResult No profanity contentFilterDetectedResult No jailbreak contentFilterDetectedResult No chatCompletionsRequestCommon Name Type Description Required Default temperature number What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both. No 1 top_p number An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. No 1 stream boolean If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. No False stop string or array Up to 4 sequences where the API will stop generating further tokens. No max_tokens integer The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens). No 4096 presence_penalty number Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. No 0 frequency_penalty number Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. No 0 logit_bias object Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. No user string A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse. No createChatCompletionRequest Name Type Description Required Default temperature number What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both. No 1 top_p number An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. No 1 stream boolean If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. No False stop string or array Up to 4 sequences where the API will stop generating further tokens. No max_tokens integer The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens). No 4096 presence_penalty number Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. No 0 frequency_penalty number Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. No 0 logit_bias object Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. No user string A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse. No messages array A list of messages comprising the conversation so far. Example Python code . No data_sources array The configuration entries for Azure OpenAI chat extensions that use them. This additional specification is only compatible with Azure OpenAI. No n integer How many chat completion choices to generate for each input message. No 1 seed integer If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism isn'tguaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend. No 0 logprobs boolean Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . This option is currently not available on the gpt-4-vision-preview model. No False top_logprobs integer An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. No response_format object An object specifying the format that the model must output. Used to enable JSON mode. No tools array A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. No tool_choice chatCompletionToolChoiceOption Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that function. No functions array Deprecated in favor of tools . A list of functions the model may generate JSON inputs for. No function_call string or object Deprecated in favor of tool_choice . Controls how the model responds to function calls. \"none\" means the model doesn't call a function, and responds to the end-user. \"auto\" means the model can pick between an end-user or calling a function.  Specifying a particular function via {\"name\":\\ \"my_function\"} forces the model to call that function. \"none\" is the default when no functions are present. \"auto\" is the default if functions are present. No Properties for response_format Type Name Type Description Default type chatCompletionResponseFormat Setting to json_object enables JSON mode. This guarantees that the message the model generates is valid JSON. text chatCompletionResponseFormat Setting to json_object enables JSON mode. This guarantees that the message the model generates is valid JSON. Description : Setting to json_object enables JSON mode. This guarantees that the message the model generates is valid JSON. Type : string Default : text Enum Name : ChatCompletionResponseFormat Enum Values : Value Description text Response format is a plain text string. json_object Response format is a JSON object. chatCompletionFunction Name Type Description Required Default name string The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64. Yes description string The description of what the function does. No parameters chatCompletionFunctionParameters The parameters the functions accepts, described as a JSON Schema object. See  the JSON Schema reference for documentation about the format. No chatCompletionFunctionParameters The parameters the functions accepts, described as a JSON Schema object. See the JSON Schema reference for documentation about the format. chatCompletionRequestMessage Name Type Description Required Default role chatCompletionRequestMessageRole The role of the messages author. Yes chatCompletionRequestMessageRole The role of the messages author. Description : The role of the messages author. Type : string Default : Enum Name : ChatCompletionRequestMessageRole Enum Values : Value Description system The message author role is system. user The message author role is user. assistant The message author role is assistant. tool The message author role is tool. function Deprecated. The message author role is function. chatCompletionRequestMessageSystem Name Type Description Required Default role chatCompletionRequestMessageRole The role of the messages author. Yes content string The contents of the message. No chatCompletionRequestMessageUser Name Type Description Required Default role chatCompletionRequestMessageRole The role of the messages author. Yes content string or array No chatCompletionRequestMessageContentPart Name Type Description Required Default type chatCompletionRequestMessageContentPartType The type of the content part. Yes chatCompletionRequestMessageContentPartType The type of the content part. Description : The type of the content part. Type : string Default : Enum Name : ChatCompletionRequestMessageContentPartType Enum Values : Value Description text The content part type is text. image_url The content part type is image_url. chatCompletionRequestMessageContentPartText Name Type Description Required Default type chatCompletionRequestMessageContentPartType The type of the content part. Yes text string The text content. No chatCompletionRequestMessageContentPartImage Name Type Description Required Default type chatCompletionRequestMessageContentPartType The type of the content part. Yes url string Either a URL of the image or the base64 encoded image data. No detail imageDetailLevel Specifies the detail level of the image. No auto imageDetailLevel Specifies the detail level of the image. Description : Specifies the detail level of the image. Type : string Default : auto Enum Name : ImageDetailLevel Enum Values : Value Description auto The image detail level is auto. low The image detail level is low. high The image detail level is high. chatCompletionRequestMessageAssistant Name Type Description Required Default role chatCompletionRequestMessageRole The role of the messages author. Yes content string The contents of the message. No tool_calls array The tool calls generated by the model, such as function calls. No context azureChatExtensionsMessageContext A representation of the additional context information available when Azure OpenAI chat extensions are involved in the generation of a corresponding chat completions response. This context information is only populated when using an Azure OpenAI request configured to use a matching extension. No azureChatExtensionConfiguration A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat\ncompletions request that should use Azure OpenAI chat extensions to augment the response behavior.\nThe use of this configuration is compatible only with Azure OpenAI. Name Type Description Required Default type azureChatExtensionType A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat completions request that should use Azure OpenAI chat extensions to augment the response behavior. The use of this configuration is compatible only with Azure OpenAI. Yes azureChatExtensionType A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat\ncompletions request that should use Azure OpenAI chat extensions to augment the response behavior.\nThe use of this configuration is compatible only with Azure OpenAI. Description :   A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat Completions request that should use Azure OpenAI chat extensions to augment the response behavior. The use of this configuration is compatible only with Azure OpenAI. Type : string Default : Enum Name : AzureChatExtensionType Enum Values : Value Description azure_search Represents the use of Azure Search as an Azure OpenAI chat extension. azure_cosmos_db Represents the use of Azure Cosmos DB as an Azure OpenAI chat extension. azureSearchChatExtensionConfiguration A specific representation of configurable options for Azure Search when using it as an Azure OpenAI chat\nextension. Name Type Description Required Default type azureChatExtensionType A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat completions request that should use Azure OpenAI chat extensions to augment the response behavior. The use of this configuration is compatible only with Azure OpenAI. Yes parameters azureSearchChatExtensionParameters Parameters for Azure Search when used as an Azure OpenAI chat extension. No azureSearchChatExtensionParameters Parameters for Azure Search when used as an Azure OpenAI chat extension. Name Type Description Required Default authentication onYourDataApiKeyAuthenticationOptions or onYourDataSystemAssignedManagedIdentityAuthenticationOptions or onYourDataUserAssignedManagedIdentityAuthenticationOptions Yes top_n_documents integer The configured top number of documents to feature for the configured query. No in_scope boolean Whether queries should be restricted to use of indexed data. No strictness integer The configured strictness of the search relevance filtering. The higher of strictness, the higher of the precision but lower recall of the answer. No role_information string Give the model instructions about how it should behave and any context it should reference when generating a response. You can describe the assistant's personality and tell it how to format responses. There's a 100 token limit for it, and it counts against the overall token limit. No endpoint string The absolute endpoint path for the Azure Search resource to use. Yes index_name string The name of the index to use as available in the referenced Azure Search resource. Yes fields_mapping azureSearchIndexFieldMappingOptions Optional settings to control how fields are processed when using a configured Azure Search resource. No query_type azureSearchQueryType The type of Azure Search retrieval query that should be executed when using it as an Azure OpenAI chat extension. No semantic_configuration string The additional semantic configuration for the query. No filter string Search filter. No embedding_dependency onYourDataEndpointVectorizationSource or onYourDataDeploymentNameVectorizationSource No azureSearchIndexFieldMappingOptions Optional settings to control how fields are processed when using a configured Azure Search resource. Name Type Description Required Default title_field string The name of the index field to use as a title. No url_field string The name of the index field to use as a URL. No filepath_field string The name of the index field to use as a filepath. No content_fields array The names of index fields that should be treated as content. No content_fields_separator string The separator pattern that content fields should use. No vector_fields array The names of fields that represent vector data. No azureSearchQueryType The type of Azure Search retrieval query that should be executed when using it as an Azure OpenAI chat extension. Description : The type of Azure Search retrieval query that should be executed when using it as an Azure OpenAI chat extension. Type : string Default : Enum Name : azureSearchQueryType Enum Values : Value Description simple Represents the default, simple query parser. semantic Represents the semantic query parser for advanced semantic modeling. vector Represents vector search over computed data. vector_simple_hybrid Represents a combination of the simple query strategy with vector data. vector_semantic_hybrid Represents a combination of semantic search and vector data querying. azureCosmosDBChatExtensionConfiguration A specific representation of configurable options for Azure Cosmos DB when using it as an Azure OpenAI chat\nextension. Name Type Description Required Default type azureChatExtensionType A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat completions request that should use Azure OpenAI chat extensions to augment the response behavior. The use of this configuration is compatible only with Azure OpenAI. Yes parameters azureCosmosDBChatExtensionParameters Parameters to use when configuring Azure OpenAI On Your Data chat extensions when using Azure Cosmos DB for MongoDB vCore. No azureCosmosDBChatExtensionParameters Parameters to use when configuring Azure OpenAI On Your Data chat extensions when using Azure Cosmos DB for\nMongoDB vCore. Name Type Description Required Default authentication onYourDataConnectionStringAuthenticationOptions The authentication options for Azure OpenAI On Your Data when using a connection string. Yes top_n_documents integer The configured top number of documents to feature for the configured query. No in_scope boolean Whether queries should be restricted to use of indexed data. No strictness integer The configured strictness of the search relevance filtering. The higher of strictness, the higher of the precision but lower recall of the answer. No role_information string Give the model instructions about how it should behave and any context it should reference when generating a response. You can describe the assistant's personality and tell it how to format responses. There's a 100 token limit for it, and it counts against the overall token limit. No database_name string The MongoDB vCore database name to use with Azure Cosmos DB. Yes container_name string The name of the Azure Cosmos DB resource container. Yes index_name string The MongoDB vCore index name to use with Azure Cosmos DB. Yes fields_mapping azureCosmosDBFieldMappingOptions Optional settings to control how fields are processed when using a configured Azure Cosmos DB resource. Yes embedding_dependency onYourDataEndpointVectorizationSource or onYourDataDeploymentNameVectorizationSource Yes azureCosmosDBFieldMappingOptions Optional settings to control how fields are processed when using a configured Azure Cosmos DB resource. Name Type Description Required Default title_field string The name of the index field to use as a title. No url_field string The name of the index field to use as a URL. No filepath_field string The name of the index field to use as a filepath. No content_fields array The names of index fields that should be treated as content. Yes content_fields_separator string The separator pattern that content fields should use. No vector_fields array The names of fields that represent vector data. Yes onYourDataAuthenticationOptions The authentication options for Azure OpenAI On Your Data. Name Type Description Required Default type onYourDataAuthenticationType The authentication types supported with Azure OpenAI On Your Data. Yes onYourDataAuthenticationType The authentication types supported with Azure OpenAI On Your Data. Description : The authentication types supported with Azure OpenAI On Your Data. Type : string Default : Enum Name : OnYourDataAuthenticationType Enum Values : Value Description api_key Authentication via API key. connection_string Authentication via connection string. system_assigned_managed_identity Authentication via system-assigned managed identity. user_assigned_managed_identity Authentication via user-assigned managed identity. onYourDataApiKeyAuthenticationOptions The authentication options for Azure OpenAI On Your Data when using an API key. Name Type Description Required Default type onYourDataAuthenticationType The authentication types supported with Azure OpenAI On Your Data. Yes key string The API key to use for authentication. No onYourDataConnectionStringAuthenticationOptions The authentication options for Azure OpenAI On Your Data when using a connection string. Name Type Description Required Default type onYourDataAuthenticationType The authentication types supported with Azure OpenAI On Your Data. Yes connection_string string The connection string to use for authentication. No onYourDataSystemAssignedManagedIdentityAuthenticationOptions The authentication options for Azure OpenAI On Your Data when using a system-assigned managed identity. Name Type Description Required Default type onYourDataAuthenticationType The authentication types supported with Azure OpenAI On Your Data. Yes onYourDataUserAssignedManagedIdentityAuthenticationOptions The authentication options for Azure OpenAI On Your Data when using a user-assigned managed identity. Name Type Description Required Default type onYourDataAuthenticationType The authentication types supported with Azure OpenAI On Your Data. Yes managed_identity_resource_id string The resource ID of the user-assigned managed identity to use for authentication. No onYourDataVectorizationSource An abstract representation of a vectorization source for Azure OpenAI On Your Data with vector search. Name Type Description Required Default type onYourDataVectorizationSourceType Represents the available sources Azure OpenAI On Your Data can use to configure vectorization of data for use with vector search. Yes onYourDataVectorizationSourceType Represents the available sources Azure OpenAI On Your Data can use to configure vectorization of data for use with\nvector search. Description : Represents the available sources Azure OpenAI On Your Data can use to configure vectorization of data for use with Vector search. Type : string Default : Enum Name : OnYourDataVectorizationSourceType Enum Values : Value Description endpoint Represents vectorization performed by public service calls to an Azure OpenAI embedding model. deployment_name Represents an Ada model deployment name to use. This model deployment must be in the same Azure OpenAI resource, but The on your data feature will use this model deployment via an internal call rather than a public one, which enables vector search even in private networks. onYourDataDeploymentNameVectorizationSource The details of a vectorization source, used by Azure OpenAI On Your Data when applying vector search, that is based\non an internal embeddings model deployment name in the same Azure OpenAI resource. Name Type Description Required Default type onYourDataVectorizationSourceType Represents the available sources Azure OpenAI On Your Data can use to configure vectorization of data for use with vector search. Yes deployment_name string Specifies the name of the model deployment to use for vectorization. This model deployment must be in the same Azure OpenAI resource, but On Your Data will use this model deployment via an internal call rather than a public one, which enables vector search even in private networks. No onYourDataEndpointVectorizationSource The details of a vectorization source, used by Azure OpenAI On Your Data when applying vector search, that is based\non public embeddings endpoint for Azure OpenAI. Name Type Description Required Default type onYourDataVectorizationSourceType Represents the available sources Azure OpenAI On Your Data can use to configure vectorization of data for use with vector search. Yes authentication onYourDataApiKeyAuthenticationOptions The authentication options for Azure OpenAI On Your Data when using an API key. No endpoint string Specifies the endpoint to use for vectorization. This endpoint must be in the same Azure OpenAI resource, but On Your Data will use this endpoint via an internal call rather than a public one, which enables vector search even in private networks. No azureChatExtensionsMessageContext A representation of the additional context information available when Azure OpenAI chat extensions are involved\nin the generation of a corresponding chat completions response. This context information is only populated when\nusing an Azure OpenAI request configured to use a matching extension. Name Type Description Required Default citations array The data source retrieval result, used to generate the assistant message in the response. No intent string The detected intent from the chat history, used to pass to the next turn to carry over the context. No Citation Citation information for a chat completions response message. Name Type Description Required Default content string The content of the citation. Yes title string The title of the citation. No url string The URL of the citation. No filepath string The file path of the citation. No chunk_id string The chunk ID of the citation. No chatCompletionMessageToolCall Name Type Description Required Default id string The ID of the tool call. Yes type toolCallType The type of the tool call, in this case function . Yes function object The function that the model called. Yes Properties for function Name Name Type Description Default name string The name of the function to call. Arguments Name Type Description Default arguments string The arguments to call the function with, as generated by the model in JSON format. Note that the model doesn't always generate valid JSON, and may fabricate parameters not defined by your function schema. Validate the arguments in your code before calling your function. toolCallType The type of the tool call, in this case function . Description : The type of the tool call, in this case function . Type : string Default : Enum Name : ToolCallType Enum Values : Value Description function The tool call type is function. chatCompletionRequestMessageTool Name Type Description Required Default role chatCompletionRequestMessageRole The role of the messages author. Yes tool_call_id string Tool call that this message is responding to. No content string The contents of the message. No chatCompletionRequestMessageFunction Name Type Description Required Default role enum The role of the messages author, in this case function . Possible values: function Yes name string The contents of the message. No content string The contents of the message. No createChatCompletionResponse Name Type Description Required Default id string A unique identifier for the chat completion. Yes object chatCompletionResponseObject The object type. Yes created integer The Unix timestamp (in seconds) of when the chat completion was created. Yes model string The model used for the chat completion. Yes usage completionUsage Usage statistics for the completion request. No system_fingerprint string Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism. No prompt_filter_results promptFilterResults Content filtering results for zero or more prompts in the request. In a streaming request, results for different prompts may arrive at different times or in different orders. No choices array No chatCompletionChoiceLogProbs Log probability information for the choice. Name Type Description Required Default content array A list of message content tokens with log probability information. Yes chatCompletionTokenLogprob Name Type Description Required Default token string The token. Yes logprob number The log probability of this token. Yes bytes array A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token. Yes top_logprobs array List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned. Yes chatCompletionResponseMessage A chat completion message generated by the model. Name Type Description Required Default role chatCompletionResponseMessageRole The role of the author of the response message. No content string The contents of the message. No tool_calls array The tool calls generated by the model, such as function calls. No function_call chatCompletionFunctionCall Deprecated and replaced by tool_calls . The name and arguments of a function that should be called, as generated by the model. No context azureChatExtensionsMessageContext A representation of the additional context information available when Azure OpenAI chat extensions are involved in the generation of a corresponding chat completions response. This context information is only populated when using an Azure OpenAI request configured to use a matching extension. No chatCompletionResponseMessageRole The role of the author of the response message. Description : The role of the author of the response message. Type : string Default : Enum Values : assistant chatCompletionToolChoiceOption Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that function. This component can be one of the following: chatCompletionNamedToolChoice Specifies a tool the model should use. Use to force the model to call a specific function. Name Type Description Required Default type enum The type of the tool. Currently, only function is supported. Possible values: function No function object No Properties for function Name Name Type Description Default name string The name of the function to call. chatCompletionFunctionCall Deprecated and replaced by tool_calls . The name and arguments of a function that should be called, as generated by the model. Name Type Description Required Default name string The name of the function to call. Yes arguments string The arguments to call the function with, as generated by the model in JSON format. Note that the model doesn't always generate valid JSON, and may fabricate parameters not defined by your function schema. Validate the arguments in your code before calling your function. Yes chatCompletionsResponseCommon Name Type Description Required Default id string A unique identifier for the chat completion. Yes object chatCompletionResponseObject The object type. Yes created integer The Unix timestamp (in seconds) of when the chat completion was created. Yes model string The model used for the chat completion. Yes usage completionUsage Usage statistics for the completion request. No system_fingerprint string Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism. No chatCompletionResponseObject The object type. Description : The object type. Type : string Default : Enum Name : ChatCompletionResponseObject Enum Values : Value Description chat.completion The object type is chat completion. completionUsage Usage statistics for the completion request. Name Type Description Required Default prompt_tokens integer Number of tokens in the prompt. Yes completion_tokens integer Number of tokens in the generated completion. Yes total_tokens integer Total number of tokens used in the request (prompt + completion). Yes chatCompletionTool Name Type Description Required Default type chatCompletionToolType The type of the tool. Currently, only function is supported. Yes function object Yes Properties for function Description Name Type Description Default description string A description of what the function does, used by the model to choose when and how to call the function. Name Name Type Description Default name string The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64. Parameters Name Type Description Default parameters chatCompletionFunctionParameters The parameters the functions accepts, described as a JSON Schema object. See the JSON Schema reference for documentation about the format. chatCompletionToolType The type of the tool. Currently, only function is supported. Description : The type of the tool. Currently, only function is supported. Type : string Default : Enum Name : ChatCompletionToolType Enum Values : Value Description function The tool type is function. chatCompletionChoiceCommon Name Type Description Required Default index integer No finish_reason string No createTranslationRequest Translation request. Name Type Description Required Default file string The audio file to translate. Yes prompt string An optional text to guide the model's style or continue a previous audio segment. The prompt should be in English. No response_format audioResponseFormat Defines the format of the output. No temperature number The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. No 0 audioResponse Translation or transcription response when response_format was json Name Type Description Required Default text string Translated or transcribed text. Yes audioVerboseResponse Translation or transcription response when response_format was verbose_json Name Type Description Required Default text string Translated or transcribed text. Yes task string Type of audio task. No language string Language. No duration number Duration. No segments array No audioResponseFormat Defines the format of the output. Description : Defines the format of the output. Type : string Default : Enum Values : json text srt verbose_json vtt createTranscriptionRequest Transcription request. Name Type Description Required Default file string The audio file object to transcribe. Yes prompt string An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. No response_format audioResponseFormat Defines the format of the output. No temperature number The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. No 0 language string The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency. No audioSegment Transcription or translation segment. Name Type Description Required Default id integer Segment identifier. No seek number Offset of the segment. No start number The segment start offset. No end number Segment end offset. No text string Segment text. No tokens array Tokens of the text. No temperature number Temperature. No avg_logprob number Average log probability. No compression_ratio number Compression ratio. No no_speech_prob number Probability of 'no speech'. No imageQuality The quality of the image that will be generated. Description : The quality of the image that will be generated. Type : string Default : standard Enum Name : Quality Enum Values : Value Description standard Standard quality creates images with standard quality. hd HD quality creates images with finer details and greater consistency across the image. imagesResponseFormat The format in which the generated images are returned. Description : The format in which the generated images are returned. Type : string Default : url Enum Name : ImagesResponseFormat Enum Values : Value Description url The URL that provides temporary access to download the generated images. b64_json The generated images are returned as base64 encoded string. imageSize The size of the generated images. Description : The size of the generated images. Type : string Default : 1024x1024 Enum Name : Size Enum Values : Value Description 1792x1024 The desired size of the generated image is 1792x1024 pixels. 1024x1792 The desired size of the generated image is 1024x1792 pixels. 1024x1024 The desired size of the generated image is 1024x1024 pixels. imageStyle The style of the generated images. Description : The style of the generated images. Type : string Default : vivid Enum Name : Style Enum Values : Value Description vivid Vivid creates images that are hyper-realistic and dramatic. natural Natural creates images that are more natural and less hyper-realistic. imageGenerationsRequest Name Type Description Required Default prompt string A text description of the desired image(s). The maximum length is 4000 characters. Yes n integer The number of images to generate. No 1 size imageSize The size of the generated images. No 1024x1024 response_format imagesResponseFormat The format in which the generated images are returned. No url user string A unique identifier representing your end-user, which can help to monitor and detect abuse. No quality imageQuality The quality of the image that will be generated. No standard style imageStyle The style of the generated images. No vivid generateImagesResponse Name Type Description Required Default created integer The unix timestamp when the operation was created. Yes data array The result data of the operation, if successful Yes imageResult The image url or encoded image if successful, and an error otherwise. Name Type Description Required Default url string The image url. No b64_json string The base64 encoded image No content_filter_results dalleContentFilterResults Information about the content filtering results. No revised_prompt string The prompt that was used to generate the image, if there was any revision to the prompt. No prompt_filter_results dalleFilterResults Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id. No Completions extensions Completions extensions aren't part of the latest GA version of the Azure OpenAI data plane inference spec. Chatmessage The Chat message object isn't part of the latest GA version of the Azure OpenAI data plane inference spec. Text to speech Is not currently part of the latest Azure OpenAI GA version of the Azure OpenAI data plane inference spec. Refer to the latest preview version for this capability. Next steps Learn about Models, and fine-tuning with the REST API .\nLearn more about the underlying models that power Azure OpenAI . Feedback Was this page helpful? Yes No Provide product feedback | Get help at Microsoft Q&A Feedback Coming soon: Throughout 2024 we will be phasing out GitHub Issues as the feedback mechanism for content and replacing it with a new feedback system. For more information see: https://aka.ms/ContentUserFeedback . Submit and view feedback for This product This page View all page feedback Additional resources California Consumer Privacy Act (CCPA) Opt-Out Icon Your Privacy Choices Theme Light Dark High contrast Previous Versions Blog Contribute Privacy Terms of Use Trademarks ¬© Microsoft 2024 Additional resources In this article California Consumer Privacy Act (CCPA) Opt-Out Icon Your Privacy Choices Theme Light Dark High contrast Previous Versions Blog Contribute Privacy Terms of Use Trademarks ¬© Microsoft 2024"},
{"url": "https://pypi.org/project/graphrag/", "text": "graphrag ¬∑ PyPI Skip to main content Switch to mobile version Warning Some features may not work without JavaScript. Please try enabling it if you encounter problems. Search PyPI Search Help Sponsors Log in Register Menu Help Sponsors Log in Register Search PyPI Search graphrag 0.2.1 pip install graphrag Copy PIP instructions Latest version Released: Aug 6, 2024 No project description provided Navigation Project description Release history Download files Verified details These details have been verified by PyPI Maintainers Alonso.G darthtrevino natoverse Unverified details These details have not been verified by PyPI Meta License: MIT License (MIT) Author: Alonso Guevara Fern√°ndez Requires: Python <3.13, >=3.10 Classifiers License OSI Approved :: MIT License Programming Language Python :: 3 Python :: 3.10 Python :: 3.11 Python :: 3.12 Project description Project details Release history Download files Project description GraphRAG üëâ Use the GraphRAG Accelerator solution üëâ Microsoft Research Blog Post üëâ Read the docs üëâ GraphRAG Arxiv Overview The GraphRAG project is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using the power of LLMs. To learn more about GraphRAG and how it can be used to enhance your LLMs ability to reason about your private data, please visit the Microsoft Research Blog Post. Quickstart To get started with the GraphRAG system we recommend trying the Solution Accelerator package. This provides a user-friendly end-to-end experience with Azure resources. Repository Guidance This repository presents a methodology for using knowledge graph memory structures to enhance LLM outputs. Please note that the provided code serves as a demonstration and is not an officially supported Microsoft offering. ‚ö†Ô∏è Warning: GraphRAG indexing can be an expensive operation, please read all of the documentation to understand the process and costs involved, and start small. Diving Deeper To learn about our contribution guidelines, see CONTRIBUTING.md To start developing GraphRAG , see DEVELOPING.md Join the conversation and provide feedback in the GitHub Discussions tab! Prompt Tuning Using GraphRAG with your data out of the box may not yield the best possible results.\nWe strongly recommend to fine-tune your prompts following the Prompt Tuning Guide in our documentation. Responsible AI FAQ See RAI_TRANSPARENCY.md What is GraphRAG? What can GraphRAG do? What are GraphRAG‚Äôs intended use(s)? How was GraphRAG evaluated? What metrics are used to measure performance? What are the limitations of GraphRAG? How can users minimize the impact of GraphRAG‚Äôs limitations when using the system? What operational factors and settings allow for effective and responsible use of GraphRAG? Trademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines .\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies. Privacy Microsoft Privacy Statement Project details Verified details These details have been verified by PyPI Maintainers Alonso.G darthtrevino natoverse Unverified details These details have not been verified by PyPI Meta License: MIT License (MIT) Author: Alonso Guevara Fern√°ndez Requires: Python <3.13, >=3.10 Classifiers License OSI Approved :: MIT License Programming Language Python :: 3 Python :: 3.10 Python :: 3.11 Python :: 3.12 Release history Release notifications | RSS feed This version 0.2.1 Aug 6, 2024 0.2.0 Jul 25, 2024 0.1.1 Jul 1, 2024 0.0.0 yanked Mar 26, 2024 Download files Download the file for your platform. If you're not sure which to choose, learn more about installing packages . Source Distribution graphrag-0.2.1.tar.gz (199.8 kB view hashes ) Uploaded Aug 6, 2024 Source Built Distribution graphrag-0.2.1-py3-none-any.whl (371.3 kB view hashes ) Uploaded Aug 6, 2024 Python 3 Close Hashes for graphrag-0.2.1.tar.gz Hashes for graphrag-0.2.1.tar.gz Algorithm Hash digest SHA256 74fe3adb97dd4c24d7b6860041fcb2309509cba224ec9241eb64bb44dbc56e0d Copy MD5 d9f6bc931307c0119f8ed2ee3f323b6f Copy BLAKE2b-256 e453d6d145bb21032717320773aec8b4a1112d96179007464fb942e1fd3bd616 Copy Close Close Hashes for graphrag-0.2.1-py3-none-any.whl Hashes for graphrag-0.2.1-py3-none-any.whl Algorithm Hash digest SHA256 862045be1cd8fc1e0543bc181034a84475724b4049ef81fdb599c26fbc895d54 Copy MD5 7728dc20ff3e636d9773a01fbd32107a Copy BLAKE2b-256 675f567acd52d5631033fd7ba6f38b46adbd32ab3e3429f90f28560ede961dc3 Copy Close Help Installing packages Uploading packages User guide Project name retention FAQs About PyPI PyPI Blog Infrastructure dashboard Statistics Logos & trademarks Our sponsors Contributing to PyPI Bugs and feedback Contribute on GitHub Translate PyPI Sponsor PyPI Development credits Using PyPI Code of conduct Report security issue Privacy policy Terms of Use Acceptable Use Policy Status: all systems operational Developed and maintained by the Python community, for the Python community. Donate today! \"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation . ¬© 2024 Python Software Foundation Site map Switch to desktop version English espa√±ol fran√ßais Êó•Êú¨Ë™û portugu√™s (Brasil) —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞ ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ Deutsch ‰∏≠Êñá (ÁÆÄ‰Ωì) ‰∏≠Êñá (ÁπÅÈ´î) —Ä—É—Å—Å–∫–∏–π ◊¢◊ë◊®◊ô◊™ Esperanto Supported by AWS Cloud computing and Security Sponsor Datadog Monitoring Fastly CDN Google Download Analytics Microsoft PSF Sponsor Pingdom Monitoring Sentry Error logging StatusPage Status page"},
{"url": "https://pypi.org/project/poethepoet/", "text": "poethepoet ¬∑ PyPI Skip to main content Switch to mobile version Warning Some features may not work without JavaScript. Please try enabling it if you encounter problems. Search PyPI Search Help Sponsors Log in Register Menu Help Sponsors Log in Register Search PyPI Search poethepoet 0.27.0 pip install poethepoet Copy PIP instructions Latest version Released: Jun 26, 2024 A task runner that works well with poetry. Navigation Project description Release history Download files Verified details These details have been verified by PyPI Maintainers natn Unverified details These details have not been verified by PyPI Project links Homepage Documentation Repository GitHub Statistics Stars: Forks: Open issues: Open PRs: Meta License: MIT License (MIT) Author: Nat Noordanus Tags automation, tasks, task-runner, plugin, poetry, poe Requires: Python >=3.8 Provides-Extra: poetry-plugin Classifiers License OSI Approved :: MIT License Programming Language Python :: 3 Python :: 3.8 Python :: 3.9 Python :: 3.10 Python :: 3.11 Python :: 3.12 Topic Software Development :: Build Tools Project description Project details Release history Download files Project description Poe the Poet A batteries included task runner that works well with poetry . üìñ Read the documentation üìñ Features ‚úÖ Straight forward declaration of project tasks in your pyproject.toml ‚úÖ Tasks are run in poetry's virtualenv ( or another env you specify) ‚úÖ Shell completion of task names (and global options too for zsh) ‚úÖ The poe CLI can be used standalone, or as a plugin for poetry ‚úÖ Tasks can be commands , shell scripts , python expressions , or references to python functions ‚úÖ Concise commands with extra arguments passed to the task poe [options] task [task_args] ‚úÖ Easily define CLI arguments for your tasks ‚úÖ Tasks can specify and reference environment variables , even without a shell ‚úÖ Tasks are self documenting , with optional help messages (just run poe with no arguments) ‚úÖ Tasks can be composed into sequences or DAGs ‚úÖ Works with .env files ‚úÖ Can be used as a library to embed in other tools Quick start Install the Poe the Poet via pipx or another method . pipx install poethepoet Define some tasks in your pyproject.toml [tool.poe.tasks] test = \"pytest --cov=my_app\" # a simple command task serve . script = \"my_app.service:run(debug=True)\" # python script based task tunnel . shell = \"ssh -N -L 0.0.0.0:8080:$PROD:8080 $PROD &\" # (posix) shell based task Run your tasks via the CLI $ poe test -v tests/unit # extra CLI arguments are appended to the underlying command Poe = > pytest --cov = my_app\n... If you're using poetry, then poe will automatically use CLI tools and libraries from your poetry managed virtualenv without you having to run poetry run or poetry shell Poe can also be used without poetry . Contributing There's plenty to do, come say hi in the discussions or open an issue ! üëã Also check out the CONTRIBUTING guide ü§ì License MIT Project details Verified details These details have been verified by PyPI Maintainers natn Unverified details These details have not been verified by PyPI Project links Homepage Documentation Repository GitHub Statistics Stars: Forks: Open issues: Open PRs: Meta License: MIT License (MIT) Author: Nat Noordanus Tags automation, tasks, task-runner, plugin, poetry, poe Requires: Python >=3.8 Provides-Extra: poetry-plugin Classifiers License OSI Approved :: MIT License Programming Language Python :: 3 Python :: 3.8 Python :: 3.9 Python :: 3.10 Python :: 3.11 Python :: 3.12 Topic Software Development :: Build Tools Release history Release notifications | RSS feed This version 0.27.0 Jun 26, 2024 0.26.1 Apr 30, 2024 0.26.0 Apr 27, 2024 0.25.1 Apr 13, 2024 0.25.0 Feb 25, 2024 0.24.4 Nov 18, 2023 0.24.3 Nov 12, 2023 0.24.2 Nov 4, 2023 0.24.1 Oct 7, 2023 0.24.0 Sep 30, 2023 0.23.0 Sep 23, 2023 0.22.1 Sep 3, 2023 0.22.0 Aug 14, 2023 0.21.1 Jul 12, 2023 0.21.0 Jul 9, 2023 0.20.0 May 3, 2023 0.19.0 Mar 22, 2023 0.18.2b2 pre-release Feb 17, 2023 0.18.2b1 pre-release Feb 4, 2023 0.18.1 Jan 15, 2023 0.18.0 Jan 9, 2023 0.17.1 Dec 26, 2022 0.17.0 Dec 26, 2022 0.16.5 Nov 27, 2022 0.16.4 Oct 9, 2022 0.16.3 Oct 8, 2022 0.16.2 Sep 3, 2022 0.16.1 Aug 30, 2022 0.16.0 Jul 17, 2022 0.15.0 Jul 3, 2022 0.14.0 Jun 18, 2022 0.13.1 Mar 1, 2022 0.13.0 Feb 27, 2022 0.12.3 Feb 9, 2022 0.12.2 Jan 16, 2022 0.12.1 Jan 13, 2022 0.12.0 Jan 8, 2022 0.11.0 Nov 14, 2021 0.11.0b6 pre-release Oct 26, 2021 0.11.0b5 pre-release Aug 24, 2021 0.11.0b4 pre-release Aug 22, 2021 0.11.0b3 pre-release Jun 13, 2021 0.11.0b2 pre-release Apr 28, 2021 0.11.0b1 pre-release Feb 14, 2021 0.10.0 Feb 7, 2021 0.9.0 Sep 29, 2020 0.8.0 Sep 2, 2020 0.7.0 Aug 16, 2020 0.6.0 Aug 9, 2020 0.5.0 Aug 2, 2020 0.4.0 Jul 11, 2020 0.3.0 Jun 20, 2020 0.2.0 Jun 17, 2020 0.1.0 Jun 4, 2020 0.0.3 May 31, 2020 0.0.2 May 29, 2020 0.0.1 May 28, 2020 Download files Download the file for your platform. If you're not sure which to choose, learn more about installing packages . Source Distribution poethepoet-0.27.0.tar.gz (57.0 kB view hashes ) Uploaded Jun 26, 2024 Source Built Distribution poethepoet-0.27.0-py3-none-any.whl (73.5 kB view hashes ) Uploaded Jun 26, 2024 Python 3 Close Hashes for poethepoet-0.27.0.tar.gz Hashes for poethepoet-0.27.0.tar.gz Algorithm Hash digest SHA256 907ab4dc1bc6326be5a3b10d2aa39d1acc0ca12024317d9506fbe9c0cdc912c9 Copy MD5 358ed9bdb976d4d2ce12c935dce9b4b1 Copy BLAKE2b-256 90d99bc436232310b5097df77662a6786c0337d52a4209ede516f9f59422286b Copy Close Close Hashes for poethepoet-0.27.0-py3-none-any.whl Hashes for poethepoet-0.27.0-py3-none-any.whl Algorithm Hash digest SHA256 0032d980a623b96e26dc7450ae200b0998be523f27d297d799b97510fe252a24 Copy MD5 2b99be2b3836281946828b2498dbca71 Copy BLAKE2b-256 879198f449a28499125b3cf60048d5c48addf19ae3937485f8cb6cc319026e2f Copy Close Help Installing packages Uploading packages User guide Project name retention FAQs About PyPI PyPI Blog Infrastructure dashboard Statistics Logos & trademarks Our sponsors Contributing to PyPI Bugs and feedback Contribute on GitHub Translate PyPI Sponsor PyPI Development credits Using PyPI Code of conduct Report security issue Privacy policy Terms of Use Acceptable Use Policy Status: all systems operational Developed and maintained by the Python community, for the Python community. Donate today! \"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation . ¬© 2024 Python Software Foundation Site map Switch to desktop version English espa√±ol fran√ßais Êó•Êú¨Ë™û portugu√™s (Brasil) —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞ ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ Deutsch ‰∏≠Êñá (ÁÆÄ‰Ωì) ‰∏≠Êñá (ÁπÅÈ´î) —Ä—É—Å—Å–∫–∏–π ◊¢◊ë◊®◊ô◊™ Esperanto Supported by AWS Cloud computing and Security Sponsor Datadog Monitoring Fastly CDN Google Download Analytics Microsoft PSF Sponsor Pingdom Monitoring Sentry Error logging StatusPage Status page"},
{"url": "https://www.python.org/downloads/", "text": "Download Python | Python.org Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Skip to content ‚ñº Close Python PSF Docs PyPI Jobs Community ‚ñ≤ The Python Network Donate ‚â° Menu Search This Site GO A A Smaller Larger Reset Socialize LinkedIn Mastodon Chat on IRC Twitter About Applications Quotes Getting Started Help Python Brochure Downloads All releases Source code Windows macOS Other Platforms License Alternative Implementations Documentation Docs Audio/Visual Talks Beginner's Guide Developer's Guide FAQ Non-English Docs PEP Index Python Books Python Essays Community Diversity Mailing Lists IRC Forums PSF Annual Impact Report Python Conferences Special Interest Groups Python Logo Python Wiki Code of Conduct Community Awards Get Involved Shared Stories Success Stories Arts Business Education Engineering Government Scientific Software Development News Python News PSF Newsletter PSF News PyCon US News News from the Community Events Python Events User Group Events Python Events Archive User Group Events Archive Submit an Event Download the latest source release Download Python 3.12.5 Download the latest version for Windows Download Python 3.12.5 Download the latest version for macOS Download Python 3.12.5 Download the latest version of Python Download Python 3.12.5 Looking for Python with a different OS? Python for Windows , Linux/UNIX , macOS , Other Want to help test development versions of Python 3.13? Prereleases , Docker images Active Python Releases For more information visit the Python Developer's Guide . Python version Maintenance status First released End of support Release schedule 3.13 prerelease 2024-10-01 (planned) 2029-10 PEP 719 3.12 bugfix 2023-10-02 2028-10 PEP 693 3.11 security 2022-10-24 2027-10 PEP 664 3.10 security 2021-10-04 2026-10 PEP 619 3.9 security 2020-10-05 2025-10 PEP 596 3.8 security 2019-10-14 2024-10 PEP 569 Looking for a specific release? Python releases by version number: Release version Release date Click for more Python 3.12.5 Aug. 6, 2024 Download Release Notes Python 3.12.4 June 6, 2024 Download Release Notes Python 3.12.3 April 9, 2024 Download Release Notes Python 3.11.9 April 2, 2024 Download Release Notes Python 3.10.14 March 19, 2024 Download Release Notes Python 3.9.19 March 19, 2024 Download Release Notes Python 3.8.19 March 19, 2024 Download Release Notes Python 3.11.8 Feb. 6, 2024 Download Release Notes Python 3.12.2 Feb. 6, 2024 Download Release Notes Python 3.12.1 Dec. 8, 2023 Download Release Notes Python 3.11.7 Dec. 4, 2023 Download Release Notes Python 3.12.0 Oct. 2, 2023 Download Release Notes Python 3.11.6 Oct. 2, 2023 Download Release Notes Python 3.11.5 Aug. 24, 2023 Download Release Notes Python 3.10.13 Aug. 24, 2023 Download Release Notes Python 3.9.18 Aug. 24, 2023 Download Release Notes Python 3.8.18 Aug. 24, 2023 Download Release Notes Python 3.10.12 June 6, 2023 Download Release Notes Python 3.11.4 June 6, 2023 Download Release Notes Python 3.7.17 June 6, 2023 Download Release Notes Python 3.8.17 June 6, 2023 Download Release Notes Python 3.9.17 June 6, 2023 Download Release Notes Python 3.10.11 April 5, 2023 Download Release Notes Python 3.11.3 April 5, 2023 Download Release Notes Python 3.10.10 Feb. 8, 2023 Download Release Notes Python 3.11.2 Feb. 8, 2023 Download Release Notes Python 3.11.1 Dec. 6, 2022 Download Release Notes Python 3.10.9 Dec. 6, 2022 Download Release Notes Python 3.9.16 Dec. 6, 2022 Download Release Notes Python 3.8.16 Dec. 6, 2022 Download Release Notes Python 3.7.16 Dec. 6, 2022 Download Release Notes Python 3.11.0 Oct. 24, 2022 Download Release Notes Python 3.9.15 Oct. 11, 2022 Download Release Notes Python 3.8.15 Oct. 11, 2022 Download Release Notes Python 3.10.8 Oct. 11, 2022 Download Release Notes Python 3.7.15 Oct. 11, 2022 Download Release Notes Python 3.7.14 Sept. 6, 2022 Download Release Notes Python 3.8.14 Sept. 6, 2022 Download Release Notes Python 3.9.14 Sept. 6, 2022 Download Release Notes Python 3.10.7 Sept. 6, 2022 Download Release Notes Python 3.10.6 Aug. 2, 2022 Download Release Notes Python 3.10.5 June 6, 2022 Download Release Notes Python 3.9.13 May 17, 2022 Download Release Notes Python 3.10.4 March 24, 2022 Download Release Notes Python 3.9.12 March 23, 2022 Download Release Notes Python 3.10.3 March 16, 2022 Download Release Notes Python 3.9.11 March 16, 2022 Download Release Notes Python 3.8.13 March 16, 2022 Download Release Notes Python 3.7.13 March 16, 2022 Download Release Notes Python 3.9.10 Jan. 14, 2022 Download Release Notes Python 3.10.2 Jan. 14, 2022 Download Release Notes Python 3.10.1 Dec. 6, 2021 Download Release Notes Python 3.9.9 Nov. 15, 2021 Download Release Notes Python 3.9.8 Nov. 5, 2021 Download Release Notes Python 3.10.0 Oct. 4, 2021 Download Release Notes Python 3.7.12 Sept. 4, 2021 Download Release Notes Python 3.6.15 Sept. 4, 2021 Download Release Notes Python 3.9.7 Aug. 30, 2021 Download Release Notes Python 3.8.12 Aug. 30, 2021 Download Release Notes Python 3.9.6 June 28, 2021 Download Release Notes Python 3.8.11 June 28, 2021 Download Release Notes Python 3.6.14 June 28, 2021 Download Release Notes Python 3.7.11 June 28, 2021 Download Release Notes Python 3.9.5 May 3, 2021 Download Release Notes Python 3.8.10 May 3, 2021 Download Release Notes Python 3.9.4 April 4, 2021 Download Release Notes Python 3.8.9 April 2, 2021 Download Release Notes Python 3.9.2 Feb. 19, 2021 Download Release Notes Python 3.8.8 Feb. 19, 2021 Download Release Notes Python 3.6.13 Feb. 15, 2021 Download Release Notes Python 3.7.10 Feb. 15, 2021 Download Release Notes Python 3.8.7 Dec. 21, 2020 Download Release Notes Python 3.9.1 Dec. 7, 2020 Download Release Notes Python 3.9.0 Oct. 5, 2020 Download Release Notes Python 3.8.6 Sept. 24, 2020 Download Release Notes Python 3.5.10 Sept. 5, 2020 Download Release Notes Python 3.7.9 Aug. 17, 2020 Download Release Notes Python 3.6.12 Aug. 17, 2020 Download Release Notes Python 3.8.5 July 20, 2020 Download Release Notes Python 3.8.4 July 13, 2020 Download Release Notes Python 3.7.8 June 27, 2020 Download Release Notes Python 3.6.11 June 27, 2020 Download Release Notes Python 3.8.3 May 13, 2020 Download Release Notes Python 2.7.18 April 20, 2020 Download Release Notes Python 3.7.7 March 10, 2020 Download Release Notes Python 3.8.2 Feb. 24, 2020 Download Release Notes Python 3.8.1 Dec. 18, 2019 Download Release Notes Python 3.7.6 Dec. 18, 2019 Download Release Notes Python 3.6.10 Dec. 18, 2019 Download Release Notes Python 3.5.9 Nov. 2, 2019 Download Release Notes Python 3.5.8 Oct. 29, 2019 Download Release Notes Python 2.7.17 Oct. 19, 2019 Download Release Notes Python 3.7.5 Oct. 15, 2019 Download Release Notes Python 3.8.0 Oct. 14, 2019 Download Release Notes Python 3.7.4 July 8, 2019 Download Release Notes Python 3.6.9 July 2, 2019 Download Release Notes Python 3.7.3 March 25, 2019 Download Release Notes Python 3.4.10 March 18, 2019 Download Release Notes Python 3.5.7 March 18, 2019 Download Release Notes Python 2.7.16 March 4, 2019 Download Release Notes Python 3.7.2 Dec. 24, 2018 Download Release Notes Python 3.6.8 Dec. 24, 2018 Download Release Notes Python 3.7.1 Oct. 20, 2018 Download Release Notes Python 3.6.7 Oct. 20, 2018 Download Release Notes Python 3.5.6 Aug. 2, 2018 Download Release Notes Python 3.4.9 Aug. 2, 2018 Download Release Notes Python 3.7.0 June 27, 2018 Download Release Notes Python 3.6.6 June 27, 2018 Download Release Notes Python 2.7.15 May 1, 2018 Download Release Notes Python 3.6.5 March 28, 2018 Download Release Notes Python 3.4.8 Feb. 5, 2018 Download Release Notes Python 3.5.5 Feb. 5, 2018 Download Release Notes Python 3.6.4 Dec. 19, 2017 Download Release Notes Python 3.6.3 Oct. 3, 2017 Download Release Notes Python 3.3.7 Sept. 19, 2017 Download Release Notes Python 2.7.14 Sept. 16, 2017 Download Release Notes Python 3.4.7 Aug. 9, 2017 Download Release Notes Python 3.5.4 Aug. 8, 2017 Download Release Notes Python 3.6.2 July 17, 2017 Download Release Notes Python 3.6.1 March 21, 2017 Download Release Notes Python 3.4.6 Jan. 17, 2017 Download Release Notes Python 3.5.3 Jan. 17, 2017 Download Release Notes Python 3.6.0 Dec. 23, 2016 Download Release Notes Python 2.7.13 Dec. 17, 2016 Download Release Notes Python 3.4.5 June 27, 2016 Download Release Notes Python 3.5.2 June 27, 2016 Download Release Notes Python 2.7.12 June 25, 2016 Download Release Notes Python 3.4.4 Dec. 21, 2015 Download Release Notes Python 3.5.1 Dec. 7, 2015 Download Release Notes Python 2.7.11 Dec. 5, 2015 Download Release Notes Python 3.5.0 Sept. 13, 2015 Download Release Notes Python 2.7.10 May 23, 2015 Download Release Notes Python 3.4.3 Feb. 25, 2015 Download Release Notes Python 2.7.9 Dec. 10, 2014 Download Release Notes Python 3.4.2 Oct. 13, 2014 Download Release Notes Python 3.3.6 Oct. 12, 2014 Download Release Notes Python 3.2.6 Oct. 12, 2014 Download Release Notes Python 2.7.8 July 2, 2014 Download Release Notes Python 2.7.7 June 1, 2014 Download Release Notes Python 3.4.1 May 19, 2014 Download Release Notes Python 3.4.0 March 17, 2014 Download Release Notes Python 3.3.5 March 9, 2014 Download Release Notes Python 3.3.4 Feb. 9, 2014 Download Release Notes Python 3.3.3 Nov. 17, 2013 Download Release Notes Python 2.7.6 Nov. 10, 2013 Download Release Notes Python 2.6.9 Oct. 29, 2013 Download Release Notes Python 3.3.2 May 15, 2013 Download Release Notes Python 3.2.5 May 15, 2013 Download Release Notes Python 2.7.5 May 12, 2013 Download Release Notes Python 2.7.4 April 6, 2013 Download Release Notes Python 3.2.4 April 6, 2013 Download Release Notes Python 3.3.1 April 6, 2013 Download Release Notes Python 3.3.0 Sept. 29, 2012 Download Release Notes Python 3.2.3 April 10, 2012 Download Release Notes Python 2.6.8 April 10, 2012 Download Release Notes Python 3.1.5 April 9, 2012 Download Release Notes Python 2.7.3 April 9, 2012 Download Release Notes Python 3.2.2 Sept. 3, 2011 Download Release Notes Python 3.2.1 July 9, 2011 Download Release Notes Python 2.7.2 June 11, 2011 Download Release Notes Python 3.1.4 June 11, 2011 Download Release Notes Python 2.6.7 June 3, 2011 Download Release Notes Python 2.5.6 May 26, 2011 Download Release Notes Python 3.2.0 Feb. 20, 2011 Download Release Notes Python 3.1.3 Nov. 27, 2010 Download Release Notes Python 2.7.1 Nov. 27, 2010 Download Release Notes Python 2.6.6 Aug. 24, 2010 Download Release Notes Python 2.7.0 July 3, 2010 Download Release Notes Python 3.1.2 March 20, 2010 Download Release Notes Python 2.6.5 March 18, 2010 Download Release Notes Python 2.5.5 Jan. 31, 2010 Download Release Notes Python 2.6.4 Oct. 26, 2009 Download Release Notes Python 2.6.3 Oct. 2, 2009 Download Release Notes Python 3.1.1 Aug. 17, 2009 Download Release Notes Python 3.1.0 June 26, 2009 Download Release Notes Python 2.6.2 April 14, 2009 Download Release Notes Python 3.0.1 Feb. 13, 2009 Download Release Notes Python 2.5.4 Dec. 23, 2008 Download Release Notes Python 2.4.6 Dec. 19, 2008 Download Release Notes Python 2.5.3 Dec. 19, 2008 Download Release Notes Python 2.6.1 Dec. 4, 2008 Download Release Notes Python 3.0.0 Dec. 3, 2008 Download Release Notes Python 2.6.0 Oct. 2, 2008 Download Release Notes Python 2.3.7 March 11, 2008 Download Release Notes Python 2.4.5 March 11, 2008 Download Release Notes Python 2.5.2 Feb. 21, 2008 Download Release Notes Python 2.5.1 April 19, 2007 Download Release Notes Python 2.3.6 Nov. 1, 2006 Download Release Notes Python 2.4.4 Oct. 18, 2006 Download Release Notes Python 2.5.0 Sept. 19, 2006 Download Release Notes Python 2.4.3 April 15, 2006 Download Release Notes Python 2.4.2 Sept. 27, 2005 Download Release Notes Python 2.4.1 March 30, 2005 Download Release Notes Python 2.3.5 Feb. 8, 2005 Download Release Notes Python 2.4.0 Nov. 30, 2004 Download Release Notes Python 2.3.4 May 27, 2004 Download Release Notes Python 2.3.3 Dec. 19, 2003 Download Release Notes Python 2.3.2 Oct. 3, 2003 Download Release Notes Python 2.3.1 Sept. 23, 2003 Download Release Notes Python 2.3.0 July 29, 2003 Download Release Notes Python 2.2.3 May 30, 2003 Download Release Notes Python 2.2.2 Oct. 14, 2002 Download Release Notes Python 2.2.1 April 10, 2002 Download Release Notes Python 2.1.3 April 9, 2002 Download Release Notes Python 2.2.0 Dec. 21, 2001 Download Release Notes Python 2.0.1 June 22, 2001 Download Release Notes View older releases Sponsors Visionary sponsors help to host Python downloads. Licenses All Python releases are Open Source . Historically, most, but not all, Python releases have also been GPL-compatible. The Licenses page details GPL-compatibility and Terms and Conditions. Read more Sources For most Unix systems, you must download and compile the source code. The same source code archive can also be used to build the Windows and Mac versions, and is the starting point for ports to all other platforms. Download the latest Python 3 source. Read more Alternative Implementations This site hosts the \"traditional\" implementation of Python (nicknamed CPython). A number of alternative implementations are available as well. Read more History Python was created in the early 1990s by Guido van Rossum at Stichting Mathematisch Centrum in the Netherlands as a successor of a language called ABC. Guido remains Python‚Äôs principal author, although it includes many contributions from others. Read more Release Schedules Python 3.13 Release Schedule Python 3.12 Release Schedule Python 3.11 Release Schedule Python 3.10 Release Schedule Python 3.9 Release Schedule Python 3.8 Release Schedule See Status of Python Versions for all an overview of all versions, including unsupported. Information about specific ports, and developer info Windows macOS Other platforms Source Python Developer's Guide Python Issue Tracker OpenPGP Public Keys Source and binary executables are signed by the release manager or binary builder using their\r\nOpenPGP key.  Release files for currently supported releases are signed by the following: Thomas Wouters (3.12.x and 3.13.x source files and tags) (key id: A821E680E5FA6305 ) Pablo Galindo Salgado (3.10.x and 3.11.x source files and tags) (key id: 64E628F8D684696D ) Steve Dower (Windows binaries) (key id: FC62 4643 4870 34E5 ) ≈Åukasz Langa (3.8.x and 3.9.x source files and tags) (key id: B269 95E3 1025 0568 ) Ned Deily (macOS binaries, 3.7.x / 3.6.x source files and tags) (key ids: 2D34 7EA6 AA65 421D and FB99 2128 6F5E 1540 Larry Hastings (3.5.x source files and tags) (key id: 3A5C A953 F73C 700D) Benjamin Peterson (2.7.z source files and tags) (key id: 04C3 67C2 18AD D4FF and A4135B38) Release files for older releases which have now reached end-of-life may have been signed by one of the following: Anthony Baxter (key id: 0EDD C5F2 6A45 C816) Georg Brandl (key id: 0A5B 1018 3658 0288) Martin v. L√∂wis (key id: 6AF0 53F0 7D9D C8D2 ) Ronald Oussoren (key id: C9BE 28DE E6DF 025C) Barry Warsaw (key ids: 126E B563 A74B 06BF , D986 6941 EA5B BD71, and ED9D77D5 ) You can import a person's public keys from a public keyserver network server\r\nyou trust by running a command like: gpg --recv-keys [key id] or, in many cases, public keys can also be found\r\nat keybase.io .\r\nOn the version-specific download pages, you should see a link to both the\r\ndownloadable file and a detached signature file.  To verify the authenticity\r\nof the download, grab both files and then run this command: gpg --verify Python-3.6.2.tgz.asc Note that you must use the name of the signature file, and you should use the\r\none that's appropriate to the download you're verifying. (These instructions are geared to GnuPG and Unix command-line users.) macOS Installer Packages Installer packages for Python on macOS downloadable from python.org are signed with\r\nwith an Apple Developer ID Installer certificate. As of Python 3.11.4 and 3.12.0b1 (2023-05-23), release installer packages are signed with certificates issued to the Python Software Foundation (Apple Developer ID BMM5U3QVKW) ). Installer packages for previous releases were signed with certificates issued to Ned Deily ( DJ3H93M7VJ ). Other Useful Items Looking for 3rd party Python modules ?  The Package Index has many of them. You can view the standard documentation\r\nonline, or you can download it\r\nin HTML, PostScript, PDF and other formats.  See the main Documentation page. Information on tools for unpacking archive files provided on python.org is available. Tip : even if you download a ready-made binary for your\r\nplatform, it makes sense to also download the source .\r\nThis lets you browse the standard library (the subdirectory Lib )\r\nand the standard collections of demos ( Demo ) and tools\r\n( Tools ) that come with it.  There's a lot you can learn from the\r\nsource! There is also a collection of Emacs packages that the Emacsing Pythoneer might find useful.  This includes major\r\nmodes for editing Python, C, C++, Java, etc., Python debugger\r\ninterfaces and more.  Most packages are compatible with Emacs and\r\nXEmacs. Want to contribute? Want to contribute?  See the Python Developer's Guide to learn about how Python development is managed. ‚ñ≤ Back to Top About Applications Quotes Getting Started Help Python Brochure Downloads All releases Source code Windows macOS Other Platforms License Alternative Implementations Documentation Docs Audio/Visual Talks Beginner's Guide Developer's Guide FAQ Non-English Docs PEP Index Python Books Python Essays Community Diversity Mailing Lists IRC Forums PSF Annual Impact Report Python Conferences Special Interest Groups Python Logo Python Wiki Code of Conduct Community Awards Get Involved Shared Stories Success Stories Arts Business Education Engineering Government Scientific Software Development News Python News PSF Newsletter PSF News PyCon US News News from the Community Events Python Events User Group Events Python Events Archive User Group Events Archive Submit an Event Contributing Developer's Guide Issue Tracker python-dev list Core Mentorship Report a Security Issue ‚ñ≤ Back to Top Help & General Contact Diversity Initiatives Submit Website Bug Status Copyright ¬©2001-2024. Python Software Foundation Legal Statements Privacy Policy"},
{"url": "https://microsoft.github.io/graphrag/data/operation_dulce/dataset.zip", "text": ""},
{"url": "https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azurite", "text": "Use Azurite emulator for local Azure Storage development | Microsoft Learn Skip to main content This browser is no longer supported. Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support. Download Microsoft Edge More info about Internet Explorer and Microsoft Edge Table of contents Exit focus mode Read in English Save Table of contents Read in English Save Add to Plan Edit Share via Facebook x.com LinkedIn Email Print Table of contents Use the Azurite emulator for local Azure Storage development Article 01/26/2024 19 contributors Feedback In this article The Azurite open-source emulator provides a free local environment for testing your Azure Blob, Queue Storage, and Table Storage applications. When you're satisfied with how your application is working locally, switch to using an Azure Storage account in the cloud. The emulator provides cross-platform support on Windows, Linux, and macOS. Azurite supersedes the Azure Storage Emulator , and continues to be updated to support the latest versions of Azure Storage APIs. This video shows you how to install and run the Azurite emulator. The steps in the video are also described in the following sections. Select any of these tabs. Install Azurite Visual Studio Visual Studio Code npm Docker Hub GitHub Azurite is automatically available with Visual Studio 2022 . The Azurite executable is updated as part of Visual Studio new version releases. If you're running an earlier version of Visual Studio, you can install Azurite by using either Node Package Manager (npm), DockerHub, or by cloning the Azurite GitHub repository. In Visual Studio Code, select the Extensions icon and search for Azurite . Select the Install button to install the Azurite extension. You can also navigate to Visual Studio Code extension market in your browser. Select the Install button to open Visual Studio Code and go directly to the Azurite extension page. Configure Azurite extension settings To configure Azurite settings within Visual Studio Code, select the Extensions icon. Select the Manage gear button for the Azurite entry. Select Extension Settings . The following settings are supported: azurite.blobHost - The Blob service listening endpoint. The default setting is 127.0.0.1. azurite.blobPort - The Blob service listening port. The default port is 10000. azurite.queueHost - The Queue service listening endpoint. The default setting is 127.0.0.1. azurite.queuePort - The Queue service listening port. The default port is 10001. azurite.tableHost - The Table service listening endpoint, by default setting is 127.0.0.1. azurite.tablePort - The Table service listening port, by default 10002. azurite.cert - Path to a locally trusted PEM or PFX certificate file path to enable HTTPS mode. azurite.debug - Output the debug log to the Azurite channel. The default value is false . azurite.key - Path to a locally trusted PEM key file, required when Azurite: Cert points to a PEM file. azurite.location - The workspace location path. The default is the Visual Studio Code working folder. azurite.loose - Enable loose mode, which ignores unsupported headers and parameters. azurite.oauth - Optional OAuth level. azurite.pwd - Password for PFX file. Required when Azurite: Cert points to a PFX file. azurite.silent - Silent mode disables the access log. The default value is false . azurite.skipApiVersionCheck - Skip the request API version check. The default value is false . azurite.disableProductStyleUrl Force the parsing of the storage account name from request Uri path, instead of from request Uri host. This installation method requires that you have Node.js version 8.0 or later installed. Node Package Manager (npm) is the package management tool included with every Node.js installation. After installing Node.js, execute the following npm command to install Azurite. npm install -g azurite Use DockerHub to pull the latest Azurite image by using the following command: docker pull mcr.microsoft.com/azure-storage/azurite This installation method requires that you have Git and Node.js version 8.0 or later installed. Clone the GitHub repository for the Azurite project by using the following console command. git clone https://github.com/Azure/Azurite.git After cloning the source code, execute following commands from the root of the cloned repo to build and install Azurite. npm install\nnpm run build\nnpm install -g Run Azurite Visual Studio Visual Studio Code npm Docker Hub GitHub To use Azurite with most project types in Visual Studio, you first need to run the Azurite executable. Once the executable is running, Azurite listens for connection requests from the application. To learn more, see Running Azurite from the command line . For Azure Functions projects and ASP.NET projects, you can choose to configure the project to start Azurite automatically. This configuration is done during the project setup. While this project configuration starts Azurite automatically, Visual Studio doesn't expose detailed Azurite configuration options. To customize detailed Azurite configuration options, run the Azurite executable before launching Visual Studio. To learn more about configuring Azure Functions projects and ASP.NET projects to start Azurite automatically, see the following guidance: Running Azurite from an Azure Functions project Running Azurite from an ASP.NET project Azurite executable file location The following table shows the location of the Azurite executable for different versions of Visual Studio running on a Windows machine: Visual Studio version Azurite executable location Visual Studio Community 2022 C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\Extensions\\Microsoft\\Azure Storage Emulator Visual Studio Professional 2022 C:\\Program Files\\Microsoft Visual Studio\\2022\\Professional\\Common7\\IDE\\Extensions\\Microsoft\\Azure Storage Emulator Visual Studio Enterprise 2022 C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\Common7\\IDE\\Extensions\\Microsoft\\Azure Storage Emulator Running Azurite from the command line You can find the Azurite executable file in the extensions folder of your Visual Studio installation, as detailed in the Azurite executable file location table . Navigate to the appropriate location and start azurite.exe . After you run the executable file, Azurite listens for connections. To learn more about available command line options to configure Azurite, see Command line options . Running Azurite from an Azure Functions project In Visual Studio 2022, create an Azure Functions project. While setting the project options, mark the box labeled Use Azurite for runtime storage account . After you create the project, Azurite starts automatically. The location of the Azurite executable file is detailed in the Azurite executable file location table . The output looks similar to the following screenshot: This configuration option can be changed later by modifying the project's Connected Services dependencies. Running Azurite from an ASP.NET project In Visual Studio 2022, create an ASP.NET Core Web App project. Then, open the Connected Services dialog box, select Add a service dependency , and then select Storage Azurite emulator . In the Configure Storage Azurite emulator dialog box, set the Connection string name field to StorageConnectionString , and then select Finish . When the configuration completes, select Close , and the Azurite emulator starts automatically. The location of the Azurite executable file is detailed in the Azurite executable file location table . The output looks similar to the following screenshot: This configuration option can be changed later by modifying the project's Connected Services dependencies. Note Azurite cannot be run from the command line if you only installed the Visual Studio Code extension. Instead, use the Visual Studio Code command palette to run commands. Configuration settings are detailed at Configure Azurite extension settings . The Azurite extension supports the following Visual Studio Code commands. To open the command palette, press F1 in Visual Studio Code. Azurite: Clean - Reset all Azurite services persistency data Azurite: Clean Blob Service - Clean blob service Azurite: Clean Queue Service - Clean queue service Azurite: Clean Table Service - Clean table service Azurite: Close - Close all Azurite services Azurite: Close Blob Service - Close blob service Azurite: Close Queue Service - Close queue service Azurite: Close Table Service - Close table service Azurite: Start - Start all Azurite services Azurite: Start Blob Service - Start blob service Azurite: Start Queue Service - Start queue service Azurite: Start Table Service - Start table service Launch Azurite by issuing the following command: azurite --silent --location c:\\azurite --debug c:\\azurite\\debug.log This command tells Azurite to store all data in a particular directory, c:\\azurite . If the --location option is omitted, it uses the current working directory. Run the Azurite Docker image : The following command runs the Azurite Docker image. The -p 10000:10000 parameter redirects requests from host machine's port 10000 to the Docker instance. docker run -p 10000:10000 -p 10001:10001 -p 10002:10002 \\\n    mcr.microsoft.com/azure-storage/azurite Specify the workspace location : In the following example, the -v c:/azurite:/data parameter specifies c:/azurite as the Azurite persisted data location. The directory, c:/azurite , must be created before running the Docker command. docker run -p 10000:10000 -p 10001:10001 -p 10002:10002 \\\n    -v c:/azurite:/data mcr.microsoft.com/azure-storage/azurite Run just the blob service docker run -p 10000:10000 mcr.microsoft.com/azure-storage/azurite \\\n    azurite-blob --blobHost 0.0.0.0 --blobPort 10000 For more information about configuring Azurite at start-up, see Command-line options . To get started immediately with the command line, create a directory called c:\\azurite , then launch Azurite by issuing the following command: azurite --silent --location c:\\azurite --debug c:\\azurite\\debug.log This command tells Azurite to store all data in a particular directory, c:\\azurite . If the --location option is omitted, it uses the current working directory. Command line options This section details the command line switches available when launching Azurite. Help Optional - Get command-line help by using the -h or --help switch. azurite -h\nazurite --help Listening host Blob Storage Queue Storage Table Storage Optional - By default, Azurite listens to 127.0.0.1 as the local server. Use the --blobHost switch to set the address to your requirements. Accept requests on the local machine only: azurite --blobHost 127.0.0.1 Allow remote requests: azurite --blobHost 0.0.0.0 Caution Allowing remote requests may make your system vulnerable to external attacks. Optional - By default, Azurite listens to 127.0.0.1 as the local server. Use the --queueHost switch to set the address to your requirements. Accept requests on the local machine only: azurite --queueHost 127.0.0.1 Allow remote requests: azurite --queueHost 0.0.0.0 Caution Allowing remote requests may make your system vulnerable to external attacks. Optional - By default, Azurite listens to 127.0.0.1 as the local server. Use the --tableHost switch to set the address to your requirements. Accept requests on the local machine only: azurite --tableHost 127.0.0.1 Allow remote requests: azurite --tableHost 0.0.0.0 Caution Allowing remote requests may make your system vulnerable to external attacks. Listening port configuration Blob Storage Queue Storage Table Storage Optional - By default, Azurite listens for the Blob service on port 10000. Use the --blobPort switch to specify the listening port that you require. Note After using a customized port, you need to update the connection string or corresponding configuration in your Azure Storage tools or SDKs. Customize the Blob service listening port: azurite --blobPort 8888 Let the system auto select an available port: azurite --blobPort 0 The port in use is displayed during Azurite startup. Optional - By default, Azurite listens for the Queue service on port 10001. Use the --queuePort switch to specify the listening port that you require. Note After using a customized port, you need to update the connection string or corresponding configuration in your Azure Storage tools or SDKs. Customize the Queue service listening port: azurite --queuePort 8888 Let the system auto select an available port: azurite --queuePort 0 The port in use is displayed during Azurite startup. Optional - By default, Azurite listens for the Table service on port 10002. Use the --tablePort switch to specify the listening port that you require. Note After using a customized port, you need to update the connection string or corresponding configuration in your Azure Storage tools or SDKs. Customize the Table service listening port: azurite --tablePort 11111 Let the system auto select an available port: azurite --tablePort 0 The port in use is displayed during Azurite startup. Workspace path Optional - Azurite stores data to the local disk during execution. Use the -l or --location switch to specify a path as the workspace location. By default, the current process working directory is used. Note the lowercase 'l'. azurite -l c:\\azurite\nazurite --location c:\\azurite Access log Optional - By default, the access log is displayed in the console window. Disable the display of the access log by using the -s or --silent switch. azurite -s\nazurite --silent Debug log Optional - The debug log includes detailed information on every request and exception stack trace. Enable the debug log by providing a valid local file path to the -d or --debug switch. azurite -d path/debug.log\nazurite --debug path/debug.log Loose mode Optional - By default, Azurite applies strict mode to block unsupported request headers and parameters. Disable strict mode by using the -L or --loose switch. Note the capital 'L'. azurite -L\nazurite --loose Version Optional - Display the installed Azurite version number by using the -v or --version switch. azurite -v\nazurite --version Certificate configuration (HTTPS) Optional - By default, Azurite uses the HTTP protocol. You can enable HTTPS mode by providing a path to a Privacy Enhanced Mail (.pem) or Personal Information Exchange (.pfx) certificate file to the --cert switch. HTTPS is required to connect to Azurite using OAuth authentication . When --cert is provided for a PEM file, you must provide a corresponding --key switch. azurite --cert path/server.pem --key path/key.pem When --cert is provided for a PFX file, you must provide a corresponding --pwd switch. azurite --cert path/server.pfx --pwd pfxpassword HTTPS setup For detailed information on generating PEM and PFX files, see HTTPS Setup . OAuth configuration Optional - Enable OAuth authentication for Azurite by using the --oauth switch. azurite --oauth basic --cert path/server.pem --key path/key.pem Note OAuth requires an HTTPS endpoint. Make sure HTTPS is enabled by providing --cert switch along with the --oauth switch. Azurite supports basic authentication by specifying the basic parameter to the --oauth switch. Azurite performs basic authentication, like validating the incoming bearer token, checking the issuer, audience, and expiry. Azurite doesn't check the token signature or permissions. To learn more about authorization, see Authorization for tools and SDKs . Skip API version check Optional - When starting up, Azurite checks that the requested API version is valid. The following command skips the API version check: azurite --skipApiVersionCheck Disable production-style URL Optional . When using the fully qualified domain name instead of the IP in request Uri host, by default Azurite parses the storage account name from request URI host. You can force the parsing of the storage account name from request URI path by using --disableProductStyleUrl : azurite --disableProductStyleUrl In-memory persistence Optional . By default, blob and queue metadata is persisted to disk and content is persisted to extent files. Table storage persists all data to disk. You can disable persisting any data to disk and only store data in-memory. In the in-memory persistence scenario, if the Azurite process is terminated, all data is lost. The default persistence behavior can be overridden using the following option: azurite --inMemoryPersistence This setting is rejected when the SQL-based metadata implementation is enabled (via AZURITE_DB ), or when the --location option is specified. Extent memory limit Optional . By default, the in-memory extent store (for blob and queue content) is limited to 50% of the total memory on the host machine. The total is evaluated using os.totalmem() . This limit can be overridden using the following option: azurite --extentMemoryLimit <megabytes> There's no restriction on the value specified for this option, but virtual memory might be used if the limit exceeds the amount of available physical memory as provided by the operating system. A high limit might eventually lead to out of memory errors or reduced performance. This option is rejected when --inMemoryPersistence isn't specified. To learn more, see Use in-memory storage . Connect to Azurite with SDKs and tools You can connect to Azurite from Azure Storage SDKs, or tools like Azure Storage Explorer . Authentication is required, and Azurite supports authorization with OAuth, Shared Key, and shared access signatures (SAS). Azurite also supports anonymous access to public containers. To learn more about using Azurite with the Azure SDKs, see Azure SDKs . Well-known storage account and key Azurite accepts the same well-known account and key used by the legacy Azure Storage Emulator. Account name: devstoreaccount1 Account key: Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw== Custom storage accounts and keys Azurite supports custom storage account names and keys by setting the AZURITE_ACCOUNTS environment variable in the following format: account1:key1[:key2];account2:key1[:key2];... . For example, use a custom storage account that has one key: set AZURITE_ACCOUNTS=\"account1:key1\" export AZURITE_ACCOUNTS=\"account1:key1\" Note The account keys must be a base64 encoded string. Or use multiple storage accounts with two keys each: set AZURITE_ACCOUNTS=\"account1:key1:key2;account2:key1:key2\" export AZURITE_ACCOUNTS=\"account1:key1:key2;account2:key1:key2\" Azurite refreshes custom account names and keys from the environment variable every minute by default. With this feature, you can dynamically rotate the account key, or add new storage accounts without restarting Azurite. Note The default devstoreaccount1 storage account is disabled when you set custom storage accounts. If you want to continue using devstoreaccount1 after enabling custom storage accounts, you need to add it to the list of custom accounts and keys in the AZURITE_ACCOUNTS environment variable. The account keys must be a base64 encoded string. Connection strings The easiest way to connect to Azurite from your application is to configure a connection string in your application's configuration file that references the shortcut UseDevelopmentStorage=true . Here's an example of a connection string in an app.config file: <appSettings>\n  <add key=\"StorageConnectionString\" value=\"UseDevelopmentStorage=true\" />\n</appSettings> HTTP connection strings You can pass the following connection strings to the Azure SDKs or tools, like Azure CLI 2.0 or Storage Explorer. The full connection string is: DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;QueueEndpoint=http://127.0.0.1:10001/devstoreaccount1;TableEndpoint=http://127.0.0.1:10002/devstoreaccount1; To connect to a specific service, you can use the following connection strings: Blob Storage Queue Storage Table Storage To connect to Blob Storage only, the connection string is: DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1; To connect to Queue Storage only, the connection string is: DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;QueueEndpoint=http://127.0.0.1:10001/devstoreaccount1; To connect to Table Storage only, the connection string is: DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;TableEndpoint=http://127.0.0.1:10002/devstoreaccount1; HTTPS connection strings The full HTTPS connection string is: DefaultEndpointsProtocol=https;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=https://127.0.0.1:10000/devstoreaccount1;QueueEndpoint=https://127.0.0.1:10001/devstoreaccount1;TableEndpoint=https://127.0.0.1:10002/devstoreaccount1; To connect to a specific service, you can use the following connection strings: Blob Storage Queue Storage Table Storage To use the blob service only, the HTTPS connection string is: DefaultEndpointsProtocol=https;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=https://127.0.0.1:10000/devstoreaccount1; To use the queue service only, the HTTPS connection string is: DefaultEndpointsProtocol=https;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;QueueEndpoint=https://127.0.0.1:10001/devstoreaccount1; To use the table service only, the HTTPS connection string is: DefaultEndpointsProtocol=https;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;TableEndpoint=https://127.0.0.1:10002/devstoreaccount1; If you used dotnet dev-certs to generate your self-signed certificate, use the following connection string. DefaultEndpointsProtocol=https;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=https://localhost:10000/devstoreaccount1;QueueEndpoint=https://localhost:10001/devstoreaccount1;TableEndpoint=https://localhost:10002/devstoreaccount1; Update the connection string when using custom storage accounts and keys . For more information, see Configure Azure Storage connection strings . Azure SDKs To connect to Azurite with the Azure SDKs , follow these steps: Enable OAuth authentication for Azurite via the --oauth switch. To learn more, see OAuth configuration . Enable HTTPS by using a self-signed certificate via the --cert and --key / --pwd options. To learn more about generating certificates, see Certificate configuration (HTTPS) and HTTPS setup . Once the certificates are in place, start Azurite with the following command line options: azurite --oauth basic --cert cert-name.pem --key cert-name-key.pem Replace cert-name.pem and certname-key.pem with the names of your certificate and key files. If you're using a PFX certificate, use the --pwd option instead of the --key option. Blob Storage Queue Storage Table Storage To interact with Blob Storage resources, you can instantiate a BlobContainerClient , BlobServiceClient , or BlobClient . The following examples show how to authorize a BlobContainerClient object using three different authorization mechanisms: DefaultAzureCredential , connection string, and shared key. DefaultAzureCredential provides a Bearer token-based authentication mechanism, and uses a chain of credential types used for authentication. Once authenticated, this credential provides the OAuth token as part of client instantiation. To learn more, see the DefaultAzureCredential class reference . // With container URL and DefaultAzureCredential\nvar client = new BlobContainerClient(\n    new Uri(\"https://127.0.0.1:10000/devstoreaccount1/container-name\"), new DefaultAzureCredential()\n  );\n\n// With connection string\nvar client = new BlobContainerClient(\n    \"DefaultEndpointsProtocol=https;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=https://127.0.0.1:10000/devstoreaccount1;\", \"container-name\"\n  );\n\n// With account name and key\nvar client = new BlobContainerClient(\n    new Uri(\"https://127.0.0.1:10000/devstoreaccount1/container-name\"),\n    new StorageSharedKeyCredential(\"devstoreaccount1\", \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\")\n  ); To interact with Queue Storage resources, you can instantiate a QueueClient or QueueServiceClient . The following examples show how to create and authorize a QueueClient object using three different authorization mechanisms: DefaultAzureCredential , connection string, and shared key. DefaultAzureCredential provides a Bearer token-based authentication mechanism, and uses a chain of credential types used for authentication. Once authenticated, this credential provides the OAuth token as part of client instantiation. To learn more, see the DefaultAzureCredential class reference . // With queue URL and DefaultAzureCredential\nvar client = new QueueClient(\n    new Uri(\"https://127.0.0.1:10001/devstoreaccount1/queue-name\"), new DefaultAzureCredential()\n  );\n\n// With connection string\nvar client = new QueueClient(\n    \"DefaultEndpointsProtocol=https;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;QueueEndpoint=https://127.0.0.1:10001/devstoreaccount1;\", \"queue-name\"\n  );\n\n// With account name and key\nvar client = new QueueClient(\n    new Uri(\"https://127.0.0.1:10001/devstoreaccount1/queue-name\"),\n    new StorageSharedKeyCredential(\"devstoreaccount1\", \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\")\n  ); To interact with Table Storage resources, you can instantiate a TableClient or TableServiceClient . The following examples show how to create and authorize a TableClient object using three different authorization mechanisms: DefaultAzureCredential , connection string, and shared key. DefaultAzureCredential provides a Bearer token-based authentication mechanism, and uses a chain of credential types used for authentication. Once authenticated, this credential provides the OAuth token as part of client instantiation. To learn more, see the DefaultAzureCredential class reference . // With table URL and DefaultAzureCredential\nvar client = new Client(\n    new Uri(\"https://127.0.0.1:10002/devstoreaccount1/table-name\"), new DefaultAzureCredential()\n  );\n\n// With connection string\nvar client = new TableClient(\n    \"DefaultEndpointsProtocol=https;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;TableEndpoint=https://127.0.0.1:10002/devstoreaccount1;\", \"table-name\"\n  );\n\n// With account name and key\nvar client = new TableClient(\n    new Uri(\"https://127.0.0.1:10002/devstoreaccount1/table-name\"),\n    new StorageSharedKeyCredential(\"devstoreaccount1\", \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\")\n  ); Microsoft Azure Storage Explorer You can use Storage Explorer to view the data stored in Azurite. Connect to Azurite using HTTP In Storage Explorer, connect to Azurite by following these steps: Select the Manage Accounts icon Select Add an account Select Attach to a local emulator Select Next Edit the Display name field to a name of your choice Select Next again Select Connect Connect to Azurite using HTTPS By default, Storage Explorer doesn't open an HTTPS endpoint that uses a self-signed certificate. If you're running Azurite with HTTPS, you're likely using a self-signed certificate. In Storage Explorer, import SSL certificates via the Edit -> SSL Certificates -> Import Certificates dialog. Import Certificate to Storage Explorer Find the certificate on your local machine. In Storage Explorer, go to Edit -> SSL Certificates -> Import Certificates and import your certificate. If you don't import a certificate, you get an error: unable to verify the first certificate or self signed certificate in chain Add Azurite via HTTPS connection string Follow these steps to add Azurite HTTPS to Storage Explorer: Select Toggle Explorer Select Local & Attached Right-click on Storage Accounts and select Connect to Azure Storage . Select Use a connection string Select Next . Enter a value in the Display name field. Enter the HTTPS connection string from the previous section of this document Select Next Select Connect Workspace structure The following files and folders might be created in the workspace location when initializing Azurite. __blobstorage__ - Directory containing Azurite blob service persisted binary data __queuestorage__ - Directory containing Azurite queue service persisted binary data __tablestorage__ - Directory containing Azurite table service persisted binary data __azurite_db_blob__.json - Azurite blob service metadata file __azurite_db_blob_extent__.json - Azurite blob service extent metadata file __azurite_db_queue__.json - Azurite queue service metadata file __azurite_db_queue_extent__.json - Azurite queue service extent metadata file __azurite_db_table__.json - Azurite table service metadata file __azurite_db_table_extent__.json - Azurite table service extent metadata file To clean up Azurite, delete above files and folders and restart the emulator. Differences between Azurite and Azure Storage There are functional differences between a local instance of Azurite and an Azure Storage account in the cloud. Endpoint and connection URL The service endpoints for Azurite are different from the endpoints of an Azure Storage account. The local computer doesn't do domain name resolution, requiring Azurite endpoints to be local addresses. When you address a resource in an Azure Storage account, the account name is part of the URI host name. The resource being addressed is part of the URI path: <http|https>://<account-name>.<service-name>.core.windows.net/<resource-path> The following URI is a valid address for a blob in an Azure Storage account: https://myaccount.blob.core.windows.net/mycontainer/myblob.txt IP-style URL Since the local computer doesn't resolve domain names, the account name is part of the URI path instead of the host name. Use the following URI format for a resource in Azurite: http://<local-machine-address>:<port>/<account-name>/<resource-path> The following address might be used for accessing a blob in Azurite: http://127.0.0.1:10000/myaccount/mycontainer/myblob.txt Production-style URL Optionally, you could modify your hosts file to access an account with production-style URL. First, add one or more lines to your hosts file. For example: 127.0.0.1 account1.blob.localhost\n127.0.0.1 account1.queue.localhost\n127.0.0.1 account1.table.localhost Next, set environment variables to enable customized storage accounts and keys: set AZURITE_ACCOUNTS=\"account1:key1:key2\" You could add more accounts. See the Custom storage accounts and keys section of this article. Start Azurite and use a customized connection string to access your account. In the following example, the connection string assumes that the default ports are used. DefaultEndpointsProtocol=http;AccountName=account1;AccountKey=key1;BlobEndpoint=http://account1.blob.localhost:10000;QueueEndpoint=http://account1.queue.localhost:10001;TableEndpoint=http://account1.table.localhost:10002; Don't access default account in this way with Azure Storage Explorer. There's a bug that Storage Explorer is always adding account name in URL path, causing failures. By default, when using Azurite with a production-style URL, the account name should be the host name in fully qualified domain name such as http://devstoreaccount1.blob.localhost:10000/container . To use production-style URL with account name in the URL path such as http://foo.bar.com:10000/devstoreaccount1/container , make sure to use the --disableProductStyleUrl parameter when you start Azurite. If you use host.docker.internal as request Uri host (For example: http://host.docker.internal:10000/devstoreaccount1/container ), Azurite gets the account name from the request Uri path. This behavior is true regardless of whether you use the --disableProductStyleUrl parameter when you start Azurite. Scaling and performance Azurite doesn't support large numbers of connected clients. There's no performance guarantee. Azurite is intended for development and testing purposes. Error handling Azurite is aligned with Azure Storage error handling logic, but there are differences. For example, error messages might be different, while error status codes align. RA-GRS Azurite supports read-access geo-redundant replication (RA-GRS). For storage resources, access the secondary location by appending -secondary to the account name. For example, the following address might be used for accessing a blob using the read-only secondary in Azurite: http://127.0.0.1:10000/devstoreaccount1-secondary/mycontainer/myblob.txt Table support Support for tables in Azurite is currently in preview. For more information, see the Azurite V3 Table project. Support for durable functions requires tables. Important Azurite support for Table Storage is currently in PREVIEW . See the Supplemental Terms of Use for Microsoft Azure Previews for legal terms that apply to Azure features that are in beta, preview, or otherwise not yet released into general availability. Azurite is open-source Contributions and suggestions for Azurite are welcome. Go to the Azurite GitHub project page or GitHub issues for milestones and work items we're tracking for upcoming features and bug fixes. Detailed work items are also tracked in GitHub. Next steps Use the Azure Storage Emulator for development and testing documents the legacy Azure Storage Emulator, which is superseded by Azurite. Configure Azure Storage connection strings explains how to assemble a valid Azure Storage connection string. Feedback Was this page helpful? Yes No Provide product feedback | Get help at Microsoft Q&A Feedback Coming soon: Throughout 2024 we will be phasing out GitHub Issues as the feedback mechanism for content and replacing it with a new feedback system. For more information see: https://aka.ms/ContentUserFeedback . Submit and view feedback for This product This page View all page feedback Additional resources California Consumer Privacy Act (CCPA) Opt-Out Icon Your Privacy Choices Theme Light Dark High contrast Previous Versions Blog Contribute Privacy Terms of Use Trademarks ¬© Microsoft 2024 Additional resources In this article California Consumer Privacy Act (CCPA) Opt-Out Icon Your Privacy Choices Theme Light Dark High contrast Previous Versions Blog Contribute Privacy Terms of Use Trademarks ¬© Microsoft 2024"},
{"url": "https://python-poetry.org/docs/", "text": "Introduction | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction System requirements Installation Enable tab completion for Bash, Fish, or Zsh Bash Fish Zsh Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Introduction Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Introduction System requirements Installation Enable tab completion for Bash, Fish, or Zsh Bash Fish Zsh Introduction # Poetry is a tool for dependency management and packaging in Python.\nIt allows you to declare the libraries your project depends on and it will manage (install/update) them for you.\nPoetry offers a lockfile to ensure repeatable installs, and can build your project for distribution. System requirements # Poetry requires Python 3.8+ . It is multi-platform and the goal is to make it work equally well\non Linux, macOS and Windows. Installation # Warning Poetry should always be installed in a dedicated virtual environment to isolate it from the rest of your system.\nIt should in no case be installed in the environment of the project that is to be managed by Poetry.\nThis ensures that Poetry‚Äôs own dependencies will not be accidentally upgraded or uninstalled.\n(Each of the following installation methods ensures that Poetry is installed into an isolated environment.)\nIn addition, the isolated virtual environment in which poetry is installed should not be activated for running poetry commands. Note If you are viewing documentation for the development branch, you may wish to install a preview or development version of Poetry.\nSee the advanced installation instructions to use a preview or alternate version of Poetry. With pipx With pipx With the official installer Manually (advanced) CI recommendations With pipx With the official installer Manually (advanced) CI recommendations pipx is used to install Python CLI applications globally while still isolating them in virtual environments. pipx will manage upgrades and uninstalls when used to install Poetry. Install pipx If pipx is not already installed, you can follow any of the options in the official pipx installation instructions .\nAny non-ancient version of pipx will do. Install Poetry pipx install poetry Install Poetry (advanced) Note You can skip this step, if you simply want the latest version and already installed Poetry as described in the\nprevious step. This step details advanced usages of this installation method. For example, installing Poetry from\nsource, having multiple versions installed at the same time etc. pipx can install different versions of Poetry, using the same syntax as pip: pipx install poetry == 1.2.0 pipx can also install versions of Poetry in parallel, which allows for easy testing of alternate or prerelease\nversions. Each version is given a unique, user-specified suffix, which will be used to create a unique binary name: pipx install --suffix = @1.2.0 poetry == 1.2.0\npoetry@1.2.0 --version pipx install --suffix = @preview --pip-args = --pre poetry\npoetry@preview --version Finally, pipx can install any valid pip requirement spec , which\nallows for installations of the development version from git , or even for local testing of pull requests: pipx install --suffix @main git+https://github.com/python-poetry/poetry.git@main\npipx install --suffix @pr1234 git+https://github.com/python-poetry/poetry.git@refs/pull/1234/head Update Poetry pipx upgrade poetry Uninstall Poetry pipx uninstall poetry We provide a custom installer that will install Poetry in a new virtual environment\nand allows Poetry to manage its own environment. Install Poetry The installer script is available directly at install.python-poetry.org ,\nand is developed in its own repository .\nThe script can be executed directly (i.e. ‚Äòcurl python‚Äô) or downloaded and then executed from disk\n(e.g. in a CI environment). Warning The install-poetry.py installer has been deprecated and removed from the Poetry repository.\nPlease migrate from the in-tree version to the standalone version described above. Linux, macOS, Windows (WSL) curl -sSL https://install.python-poetry.org | python3 - Note Note: On some systems, python may still refer to Python 2 instead of Python 3. We always suggest the python3 binary to avoid ambiguity. Windows (Powershell) ( Invoke-WebRequest -Uri https : // install . python-poetry . org -UseBasicParsing ). Content | py - Note If you have installed Python through the Microsoft Store, replace py with python in the command\nabove. Install Poetry (advanced) Note You can skip this step, if you simply want the latest version and already installed Poetry as described in the\nprevious step. This step details advanced usages of this installation method. For example, installing Poetry from\nsource, using a pre-release build, configuring a different installation location etc. By default, Poetry is installed into a platform and user-specific directory: ~/Library/Application Support/pypoetry on MacOS. ~/.local/share/pypoetry on Linux/Unix. %APPDATA%\\pypoetry on Windows. If you wish to change this, you may define the $POETRY_HOME environment variable: curl -sSL https://install.python-poetry.org | POETRY_HOME = /etc/poetry python3 - If you want to install prerelease versions, you can do so by passing the --preview option to the installation script\nor by using the $POETRY_PREVIEW environment variable: curl -sSL https://install.python-poetry.org | python3 - --preview\ncurl -sSL https://install.python-poetry.org | POETRY_PREVIEW = 1 python3 - Similarly, if you want to install a specific version, you can use --version option or the $POETRY_VERSION environment variable: curl -sSL https://install.python-poetry.org | python3 - --version 1.2.0\ncurl -sSL https://install.python-poetry.org | POETRY_VERSION = 1.2.0 python3 - You can also install Poetry from a git repository by using the --git option: curl -sSL https://install.python-poetry.org | python3 - --git https://github.com/python-poetry/poetry.git@main If you want to install different versions of Poetry in parallel, a good approach is the installation with pipx and suffix. Add Poetry to your PATH The installer creates a poetry wrapper in a well-known, platform-specific directory: $HOME/.local/bin on Unix. %APPDATA%\\Python\\Scripts on Windows. $POETRY_HOME/bin if $POETRY_HOME is set. If this directory is not present in your $PATH , you can add it in order to invoke Poetry\nas poetry . Alternatively, the full path to the poetry binary can always be used: ~/Library/Application Support/pypoetry/venv/bin/poetry on MacOS. ~/.local/share/pypoetry/venv/bin/poetry on Linux/Unix. %APPDATA%\\pypoetry\\venv\\Scripts\\poetry on Windows. $POETRY_HOME/venv/bin/poetry if $POETRY_HOME is set. Use Poetry Once Poetry is installed and in your $PATH , you can execute the following: poetry --version If you see something like Poetry (version 1.2.0) , your install is ready to use! Update Poetry Poetry is able to update itself when installed using the official installer. Warning Especially on Windows, self update may be problematic\nso that a re-install with the installer should be preferred. poetry self update If you want to install pre-release versions, you can use the --preview option. poetry self update --preview And finally, if you want to install a specific version, you can pass it as an argument\nto self update . poetry self update 1.2.0 Warning Poetry 1.1 series releases are not able to update in-place to 1.2 or newer series releases.\nTo migrate to newer releases, uninstall using your original install method, and then reinstall\nusing the methods above . Uninstall Poetry If you decide Poetry isn‚Äôt your thing, you can completely remove it from your system\nby running the installer again with the --uninstall option or by setting\nthe POETRY_UNINSTALL environment variable before executing the installer. curl -sSL https://install.python-poetry.org | python3 - --uninstall\ncurl -sSL https://install.python-poetry.org | POETRY_UNINSTALL = 1 python3 - Warning If you installed using the deprecated get-poetry.py script, you should remove the path it uses manually, e.g. rm -rf \" ${ POETRY_HOME :- ~/.poetry } \" Also remove ~/.poetry/bin from your $PATH in your shell configuration, if it is present. Poetry can be installed manually using pip and the venv module. By doing so you will essentially perform the steps carried\nout by the official installer. As this is an advanced installation method, these instructions are Unix-only and omit specific\nexamples such as installing from git . The variable $VENV_PATH will be used to indicate the path at which the virtual environment was created. python3 -m venv $VENV_PATH $VENV_PATH /bin/pip install -U pip setuptools $VENV_PATH /bin/pip install poetry Poetry will be available at $VENV_PATH/bin/poetry and can be invoked directly or symlinked elsewhere. To uninstall Poetry, simply delete the entire $VENV_PATH directory. Unlike development environments, where making use of the latest tools is desirable, in a CI environment reproducibility\nshould be made the priority. Here are some suggestions for installing Poetry in such an environment. Version pinning Whatever method you use, it is highly recommended to explicitly control the version of Poetry used, so that you are able\nto upgrade after performing your own validation. Each install method has a different syntax for setting the version that\nis used in the following examples. Using pipx Just as pipx is a powerful tool for development use, it is equally useful in a CI environment\nand should be one of your top choices for use of Poetry in CI. pipx install poetry == 1.2.0 Using install.python-poetry.org Note The official installer script ( install.python-poetry.org ) offers a streamlined and\nsimplified installation of Poetry, sufficient for developer use or for simple pipelines. However, in a CI environment\nthe other two supported installation methods (pipx and manual) should be seriously considered. Downloading a copy of the installer script to a place accessible by your CI pipelines (or maintaining a copy of the repository ) is strongly suggested, to ensure your\npipeline‚Äôs stability and to maintain control over what code is executed. By default, the installer will install to a user-specific directory. In more complex pipelines that may make accessing\nPoetry difficult (especially in cases like multi-stage container builds). It is highly suggested to make use of $POETRY_HOME when using the official installer in CI, as that way the exact paths can be controlled. export POETRY_HOME = /opt/poetry\npython3 install-poetry.py --version 1.2.0 $POETRY_HOME /bin/poetry --version Using pip (aka manually) For maximum control in your CI environment, installation with pip is fully supported and something you should\nconsider. While this requires more explicit commands and knowledge of Python packaging from you, it in return offers the\nbest debugging experience, and leaves you subject to the fewest external tools. export POETRY_HOME = /opt/poetry\npython3 -m venv $POETRY_HOME $POETRY_HOME /bin/pip install poetry == 1.2.0 $POETRY_HOME /bin/poetry --version Note If you install Poetry via pip , ensure you have Poetry installed into an isolated environment that is not the same as the target environment managed by Poetry. If Poetry and your project are installed into the same environment, Poetry\nis likely to upgrade or uninstall its own dependencies (causing hard-to-debug and understand errors). Enable tab completion for Bash, Fish, or Zsh # poetry supports generating completion scripts for Bash, Fish, and Zsh.\nSee poetry help completions for full details, but the gist is as simple as using one of the following: Bash # Auto-loaded (recommended) # poetry completions bash >> ~/.bash_completion Lazy-loaded # poetry completions bash > ${ XDG_DATA_HOME :- ~/.local/share } /bash-completion/completions/poetry Fish # poetry completions fish > ~/.config/fish/completions/poetry.fish Zsh # poetry completions zsh > ~/.zfunc/_poetry You must then add the following lines in your ~/.zshrc , if they do not already exist: fpath += ~/.zfunc\nautoload -Uz compinit && compinit Oh My Zsh # mkdir $ZSH_CUSTOM /plugins/poetry\npoetry completions zsh > $ZSH_CUSTOM /plugins/poetry/_poetry You must then add poetry to your plugins array in ~/.zshrc : plugins(\n\tpoetry\n\t...\n\t) prezto # poetry completions zsh > ~/.zprezto/modules/completion/external/src/_poetry Note You may need to restart your shell in order for these changes to take effect. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/", "text": "Poetry - Python dependency management and packaging made easy Home Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Python packaging and dependency management made easy P oetry Deterministic builds Develop Poetry comes with all the tools you might need to manage your projects in\na deterministic way. $ poetry add pendulum\n\nUsing version ^2.0.5 for pendulum Updating dependencies Resolving dependencies ... (1.5s) Package operations: 4 installs, 0 updates, 0 removals\n\n  - Installing six ( 1.13.0 ): Downloading... 25% - Updating pytzdata ( 2019.3 -> 2020.4 ): Installing... - Installing pendulum ( 2.0.5 ) Writing lock file Package with ease Build Easily build and package your projects\nwith a single command. $ poetry build\n\nBuilding poetry ( 1.0.0 )\n- Building sdist - Built poetry-1.0.0.tar.gz - Building wheel - Built poetry-1.0.0-py2.py3-none-any.whl Supports source distribution and wheels. Share your work Publish Make your work known by publishing it to PyPI. $ poetry publish\n\nPublishing poetry ( 1.0.0 ) to PyPI - Uploading poetry-1.0.0.tar.gz 100% - Uploading poetry-1.0.0-py2.py3-none-any.whl 58% You can also publish on private repositories. Check the state of your dependencies Track Having an insight of your project's dependencies is just one command away. $ poetry show --tree requests-toolbelt 0.8.0 A utility belt for advanced users...\n‚îî‚îÄ‚îÄ requests <3.0.0,>=2.0.1\n    ‚îú‚îÄ‚îÄ certifi >=2017.4.17\n    ‚îú‚îÄ‚îÄ chardet >=3.0.2,<3.1.0\n    ‚îú‚îÄ‚îÄ idna >=2.5,<2.7\n    ‚îî‚îÄ‚îÄ urllib3 <1.23,>=1.21.1 $ poetry show --latest pendulum 2.0.4 1.4.5 Python datetimes made easy. django 1.11.11 2.0.3 A high-level Python Web framework ... requests 2.18.4 2.18.4 Python HTTP for Humans. Dependency resolver Poetry comes with an exhaustive dependency resolver,\nwhich will always find a solution if it exists. And get a detailed explanation if no solution exists. Isolation Poetry either uses your configured virtualenvs or creates its own to\nalways be isolated from your system. The behavior is configurable . Intuitive CLI Poetry's commands are intuitive and easy to use, with\nsensible defaults while still being configurable. It's also extensible with plugin system. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/community/", "text": "Community | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community Badge FAQ pre-commit hooks Community Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Community Badge Community # Badge # For any projects using Poetry, you may add its official badge somewhere prominent like the README. Markdown [ ![Poetry ]( https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json )](https://python-poetry.org/) reStructuredText .. image :: https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json :alt: Poetry :target: https://python-poetry.org/ Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/plugins/", "text": "Plugins | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins Creating a plugin Plugin package Generic plugins Application plugins Event handler Using plugins With pipx inject With pip The self add command Maintaining a plugin The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Plugins Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Plugins Creating a plugin Plugin package Generic plugins Application plugins Event handler Using plugins With pipx inject With pip The self add command Maintaining a plugin Plugins # Poetry supports using and building plugins if you wish to\nalter or expand Poetry‚Äôs functionality with your own. For example if your environment poses special requirements\non the behaviour of Poetry which do not apply to the majority of its users\nor if you wish to accomplish something with Poetry in a way that is not desired by most users. In these cases you could consider creating a plugin to handle your specific logic. Creating a plugin # A plugin is a regular Python package which ships its code as part of the package\nand may also depend on further packages. Plugin package # The plugin package must depend on Poetry\nand declare a proper plugin in the pyproject.toml file. [ tool . poetry ] name = \"my-poetry-plugin\" version = \"1.0.0\" # ... [ tool . poetry . dependencies ] python = \"^3.7\" poetry = \"^1.2\" [ tool . poetry . plugins . \"poetry.plugin\" ] demo = \"poetry_demo_plugin.plugin:MyPlugin\" Generic plugins # Every plugin has to supply a class which implements the poetry.plugins.Plugin interface. The activate() method of the plugin is called after the plugin is loaded\nand receives an instance of Poetry as well as an instance of cleo.io.io.IO . Using these two objects all configuration can be read\nand all public internal objects and state can be manipulated as desired. Example: from cleo.io.io import IO from poetry.plugins.plugin import Plugin from poetry.poetry import Poetry class MyPlugin ( Plugin ): def activate ( self , poetry : Poetry , io : IO ): io . write_line ( \"Setting readme\" ) poetry . package . readme = \"README.md\" ... Application plugins # If you want to add commands or options to the poetry script you need\nto create an application plugin which implements the poetry.plugins.ApplicationPlugin interface. The activate() method of the application plugin is called after the plugin is loaded\nand receives an instance of poetry.console.Application . from cleo.commands.command import Command from poetry.plugins.application_plugin import ApplicationPlugin class CustomCommand ( Command ): name = \"my-command\" def handle ( self ) -> int : self . line ( \"My command\" ) return 0 def factory (): return CustomCommand () class MyApplicationPlugin ( ApplicationPlugin ): def activate ( self , application ): application . command_loader . register_factory ( \"my-command\" , factory ) Note It‚Äôs possible to do the following to register the command: application . add ( MyCommand ()) However, it is strongly recommended to register a new factory\nin the command loader to defer the loading of the command when it‚Äôs actually\ncalled. This will help keep the performances of Poetry good. The plugin also must be declared in the pyproject.toml file of the plugin package\nas a poetry.application.plugin plugin: [ tool . poetry . plugins . \"poetry.application.plugin\" ] foo - command = \"poetry_demo_plugin.plugin:MyApplicationPlugin\" Warning A plugin must not remove or modify in any way the core commands of Poetry. Event handler # Plugins can also listen to specific events and act on them if necessary. These events are fired by Cleo and are accessible from the cleo.events.console_events module. COMMAND : this event allows attaching listeners before any command is executed. SIGNAL : this event allows some actions to be performed after the command execution is interrupted. TERMINATE : this event allows listeners to be attached after the command. ERROR : this event occurs when an uncaught exception is raised. Let‚Äôs see how to implement an application event handler. For this example\nwe will see how to load environment variables from a .env file before executing\na command. from cleo.events.console_events import COMMAND from cleo.events.console_command_event import ConsoleCommandEvent from cleo.events.event_dispatcher import EventDispatcher from dotenv import load_dotenv from poetry.console.application import Application from poetry.console.commands.env_command import EnvCommand from poetry.plugins.application_plugin import ApplicationPlugin class MyApplicationPlugin ( ApplicationPlugin ): def activate ( self , application : Application ): application . event_dispatcher . add_listener ( COMMAND , self . load_dotenv ) def load_dotenv ( self , event : ConsoleCommandEvent , event_name : str , dispatcher : EventDispatcher ) -> None : command = event . command if not isinstance ( command , EnvCommand ): return io = event . io if io . is_debug (): io . write_line ( \"<debug>Loading environment variables.</debug>\" ) load_dotenv () Using plugins # Installed plugin packages are automatically loaded when Poetry starts up. You have multiple ways to install plugins for Poetry With pipx inject # If you used pipx to install Poetry you can add the plugin packages via the pipx inject command. pipx inject poetry poetry-plugin If you want to uninstall a plugin, you can run: pipx uninject poetry poetry-plugin # For pipx versions >= 1.2.0 pipx runpip poetry uninstall poetry-plugin # For pipx versions  < 1.2.0 With pip # The pip binary in Poetry‚Äôs virtual environment can also be used to install and remove plugins.\nThe environment variable $POETRY_HOME here is used to represent the path to the virtual environment.\nThe installation instructions can be referenced if you are not\nsure where Poetry has been installed. To add a plugin, you can use pip install : $POETRY_HOME /bin/pip install --user poetry-plugin If you want to uninstall a plugin, you can run: $POETRY_HOME /bin/pip uninstall poetry-plugin The self add command # Warning Especially on Windows, self add and self remove may be problematic\nso that other methods should be preferred. poetry self add poetry-plugin The self add command will ensure that the plugin is compatible with the current version of Poetry\nand install the needed packages for the plugin to work. The package specification formats supported by the self add command are the same as the ones supported\nby the add command . If you no longer need a plugin and want to uninstall it, you can use the self remove command. poetry self remove poetry-plugin You can also list all currently installed plugins by running: poetry self show plugins Maintaining a plugin # When writing a plugin, you will probably access internals of Poetry, since there is no\nstable public API. Although we try our best to deprecate methods first, before\nremoving them, sometimes the signature of an internal method has to be changed. As the author of a plugin, you are probably testing your plugin\nagainst the latest release of Poetry.\nAdditionally, you should consider testing against the latest release branch and the\nmain branch of Poetry and schedule a CI job that runs regularly even if you did not\nmake any changes to your plugin.\nThis way, you will notice internal changes that break your plugin immediately\nand can prepare for the next Poetry release. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/pre-commit-hooks/", "text": "pre-commit hooks | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks poetry-check Arguments poetry-lock Arguments poetry-export Arguments poetry-install Arguments Usage FAQ Why does pre-commit autoupdate not update to the latest version? pre-commit hooks Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks pre-commit hooks poetry-check Arguments poetry-lock Arguments poetry-export Arguments poetry-install Arguments Usage FAQ Why does pre-commit autoupdate not update to the latest version? pre-commit hooks # pre-commit is a framework for building and running git hooks .\nSee the official documentation for more information: pre-commit.com This document provides a list of available pre-commit hooks provided by Poetry. Note If you specify the args: for a hook in your .pre-commit-config.yaml ,\nthe defaults are overwritten. You must fully specify all arguments for\nyour hook if you make use of args: . Note If the pyproject.toml file is not in the root directory, you can specify args: [\"-C\", \"./subdirectory\"] . poetry-check # The poetry-check hook calls the poetry check command\nto make sure the poetry configuration does not get committed in a broken state. Arguments # The hook takes the same arguments as the poetry command.\nFor more information see the check command . poetry-lock # The poetry-lock hook calls the poetry lock command\nto make sure the lock file is up-to-date when committing changes. Arguments # The hook takes the same arguments as the poetry command.\nFor more information see the lock command . poetry-export # The poetry-export hook calls the poetry export command\nto sync your requirements.txt file with your current dependencies. Warning This hook is provided by the Export Poetry Plugin . Note It is recommended to run the poetry-lock hook or poetry-check with argument --lock prior to this one. Arguments # The hook takes the same arguments as the poetry command.\nFor more information see the export command . The default arguments are args: [\"-f\", \"requirements.txt\", \"-o\", \"requirements.txt\"] ,\nwhich will create/update the requirements.txt file in the current working directory. You may add verbose: true in your .pre-commit-config.yaml in order to output to the\nconsole: hooks : - id : poetry-export args : [ \"-f\" , \"requirements.txt\" ] verbose : true Also, --dev can be added to args to write dev-dependencies to requirements.txt : hooks : - id : poetry-export args : [ \"--dev\" , \"-f\" , \"requirements.txt\" , \"-o\" , \"requirements.txt\" ] poetry-install # The poetry-install hook calls the poetry install command to make sure all locked packages are installed.\nIn order to install this hook, you either need to specify default_install_hook_types , or you have\nto install it via pre-commit install --install-hooks -t post-checkout -t post-merge . Arguments # The hook takes the same arguments as the poetry command.\nFor more information see the install command . Usage # For more information on how to use pre-commit please see the official documentation . A minimalistic .pre-commit-config.yaml example: repos : - repo : https://github.com/python-poetry/poetry rev : '' # add version here hooks : - id : poetry-check - id : poetry-lock - id : poetry-export - id : poetry-install A .pre-commit-config.yaml example for a monorepo setup or if the pyproject.toml file is not in the root directory: repos : - repo : https://github.com/python-poetry/poetry rev : '' # add version here hooks : - id : poetry-check args : [ \"-C\" , \"./subdirectory\" ] - id : poetry-lock args : [ \"-C\" , \"./subdirectory\" ] - id : poetry-export args : [ \"-C\" , \"./subdirectory\" , \"-f\" , \"requirements.txt\" , \"-o\" , \"./subdirectory/requirements.txt\" ] - id : poetry-install args : [ \"-C\" , \"./subdirectory\" ] FAQ # Why does pre-commit autoupdate not update to the latest version? # pre-commit autoupdate updates the rev for each repository defined in your .pre-commit-config.yaml to the latest available tag in the default branch. Poetry follows a branching strategy where the default branch is the active development branch,\nand fixes get backported to stable branches. New tags are assigned in these stable branches. pre-commit does not support such a branching strategy and has decided to not implement\nan option, either on the user‚Äôs side or the hook author‚Äôs side , to define a branch for looking\nup the latest available tag. Thus, pre-commit autoupdate is not usable for the hooks described here. You can avoid changing the rev to an unexpected value by using the --repo parameter (may be specified multiple\ntimes), to explicitly list repositories that should be updated. An option to explicitly exclude\nrepositories will not be implemented into pre-commit . Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/pyproject/", "text": "The pyproject.toml file | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file package-mode name version description license authors maintainers readme homepage repository documentation keywords classifiers packages include and exclude dependencies and dependency groups scripts extras plugins urls Poetry and PEP-517 Contributing to Poetry Community FAQ pre-commit hooks The pyproject.toml file Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks The pyproject.toml file package-mode name version description license authors maintainers readme homepage repository documentation keywords classifiers packages include and exclude dependencies and dependency groups scripts extras plugins urls Poetry and PEP-517 The pyproject.toml file # The tool.poetry section of the pyproject.toml file is composed of multiple sections. package-mode # Whether Poetry operates in package mode (default) or not. Optional See basic usage for more information. package - mode = false name # The name of the package. Required in package mode This should be a valid name as defined by PEP 508 . name = \"my-package\" version # The version of the package. Required in package mode This should be a valid PEP 440 string. version = \"0.1.0\" Note If you would like to use semantic versioning for your project, please see here . description # A short description of the package. Required in package mode description = \"A short description of the package.\" license # The license of the package. The recommended notation for the most common licenses is (alphabetical): Apache-2.0 BSD-2-Clause BSD-3-Clause BSD-4-Clause GPL-2.0-only GPL-2.0-or-later GPL-3.0-only GPL-3.0-or-later LGPL-2.1-only LGPL-2.1-or-later LGPL-3.0-only LGPL-3.0-or-later MIT Optional, but it is highly recommended to supply this.\nMore identifiers are listed at the SPDX Open Source License Registry . license = \"MIT\" Note If your project is proprietary and does not use a specific licence, you can set this value as Proprietary . authors # The authors of the package. Required in package mode This is a list of authors and should contain at least one author. Authors must be in the form name <email> . authors = [ \"S√©bastien Eustace <sebastien@eustace.io>\" , ] maintainers # The maintainers of the package. Optional This is a list of maintainers and should be distinct from authors. Maintainers may contain an email and be in the form name <email> . maintainers = [ \"John Smith <johnsmith@example.org>\" , \"Jane Smith <janesmith@example.org>\" , ] readme # A path, or list of paths corresponding to the README file(s) of the package. Optional The file(s) can be of any format, but if you intend to publish to PyPI keep the recommendations for a PyPI-friendly README in\nmind. README paths are implicitly relative to pyproject.toml . Note Whether paths are case-sensitive follows platform defaults, but it is recommended to keep cases. To be specific, you can set readme = \"rEaDmE.mD\" for README.md on macOS and Windows, but Linux users can‚Äôt poetry install after cloning your repo. This is because macOS and Windows are case-insensitive and case-preserving. The contents of the README file(s) are used to populate the Description\nfield of your distribution‚Äôs metadata (similar to long_description in setuptools).\nWhen multiple files are specified they are concatenated with newlines. [ tool . poetry ] # ... readme = \"README.md\" [ tool . poetry ] # ... readme = [ \"docs/README1.md\" , \"docs/README2.md\" ] homepage # An URL to the website of the project. Optional homepage = \"https://python-poetry.org/\" repository # An URL to the repository of the project. Optional repository = \"https://github.com/python-poetry/poetry\" documentation # An URL to the documentation of the project. Optional documentation = \"https://python-poetry.org/docs/\" keywords # A list of keywords that the package is related to. Optional keywords = [ \"packaging\" , \"poetry\" ] classifiers # A list of PyPI trove classifiers that describe the project. Optional [ tool . poetry ] # ... classifiers = [ \"Topic :: Software Development :: Build Tools\" , \"Topic :: Software Development :: Libraries :: Python Modules\" ] Note Note that Python classifiers are still automatically added for you and are determined by your python requirement. The license property will also set the License classifier automatically. packages # A list of packages and modules to include in the final distribution. If your project structure differs from the standard one supported by poetry ,\nyou can specify the packages you want to include in the final distribution. [ tool . poetry ] # ... packages = [ { include = \"my_package\" }, { include = \"extra_package/**/*.py\" }, ] If your package is stored inside a ‚Äúlib‚Äù directory, you must specify it: [ tool . poetry ] # ... packages = [ { include = \"my_package\" , from = \"lib\" }, ] The to parameter is designed to specify the relative destination path\nwhere the package will be located upon installation. This allows for\ngreater control over the organization of packages within your project‚Äôs structure. [ tool . poetry ] # ... packages = [ { include = \"my_package\" , from = \"lib\" , to = \"target_package\" }, ] If you want to restrict a package to a specific build format you can specify\nit by using format : [ tool . poetry ] # ... packages = [ { include = \"my_package\" }, { include = \"my_other_package\" , format = \"sdist\" }, ] From now on, only the sdist build archive will include the my_other_package package. Note Using packages disables the package auto-detection feature meaning you have to explicitly specify the ‚Äúdefault‚Äù package. For instance, if you have a package named my_package and you want to also include\nanother package named extra_package , you will need to specify my_package explicitly: packages = [ { include = \"my_package\" }, { include = \"extra_package\" }, ] Note Poetry is clever enough to detect Python subpackages. Thus, you only have to specify the directory where your root package resides. include and exclude # A list of patterns that will be included in the final package. You can explicitly specify to Poetry that a set of globs should be ignored or included for the purposes of packaging.\nThe globs specified in the exclude field identify a set of files that are not included when a package is built. If a VCS is being used for a package, the exclude field will be seeded with the VCS‚Äô ignore settings ( .gitignore for git for example). Note Explicitly declaring entries in include will negate VCS' ignore settings. [ tool . poetry ] # ... include = [ \"CHANGELOG.md\" ] You can also specify the formats for which these patterns have to be included, as shown here: [ tool . poetry ] # ... include = [ { path = \"tests\" , format = \"sdist\" }, { path = \"for_wheel.txt\" , format = [ \"sdist\" , \"wheel\" ] } ] If no format is specified, include defaults to only sdist . In contrast, exclude defaults to both sdist and wheel . exclude = [ \"my_package/excluded.py\" ] dependencies and dependency groups # Poetry is configured to look for dependencies on PyPI by default.\nOnly the name and a version string are required in this case. [ tool . poetry . dependencies ] requests = \"^2.13.0\" If you want to use a private repository ,\nyou can add it to your pyproject.toml file, like so: [[ tool . poetry . source ]] name = \"private\" url = \"http://example.com/simple\" If you have multiple repositories configured, you can explicitly tell poetry where to look for a specific package: [ tool . poetry . dependencies ] requests = { version = \"^2.13.0\" , source = \"private\" } Note Be aware that declaring the python version for which your package\nis compatible is mandatory: [ tool . poetry . dependencies ] python = \"^3.7\" You can organize your dependencies in groups to manage them in a more granular way. [ tool . poetry . group . test . dependencies ] pytest = \"*\" [ tool . poetry . group . docs . dependencies ] mkdocs = \"*\" See Dependency groups for a more in-depth look\nat how to manage dependency groups and Dependency specification for more information on other keys and specifying version ranges. scripts # This section describes the scripts or executables that will be installed when installing the package [ tool . poetry . scripts ] my_package_cli = 'my_package.console:run' Here, we will have the my_package_cli script installed which will execute the run function in the console module in the my_package package. Note When a script is added or updated, run poetry install to make them available in the project‚Äôs virtualenv. extras # Poetry supports extras to allow expression of: optional dependencies, which enhance a package, but are not required; and clusters of optional dependencies. [ tool . poetry ] name = \"awesome\" [ tool . poetry . dependencies ] # These packages are mandatory and form the core of this package‚Äôs distribution. mandatory = \"^1.0\" # A list of all of the optional dependencies, some of which are included in the # below `extras`. They can be opted into by apps. psycopg2 = { version = \"^2.9\" , optional = true } mysqlclient = { version = \"^1.3\" , optional = true } [ tool . poetry . extras ] mysql = [ \"mysqlclient\" ] pgsql = [ \"psycopg2\" ] databases = [ \"mysqlclient\" , \"psycopg2\" ] When installing packages with Poetry, you can specify extras by using the -E|--extras option: poetry install --extras \"mysql pgsql\" poetry install -E mysql -E pgsql Any extras you don‚Äôt specify will be removed. Note this behavior is different from optional dependency groups not\nselected for install, e.g. those not specified via install --with . You can install all extras with the --all-extras option: poetry install --all-extras Note Note that install --extras and the variations mentioned above ( --all-extras , --extras foo , etc.) only work on dependencies defined in the current project. If you want to install extras defined by dependencies, you‚Äôll have to express that in the dependency itself: [ tool . poetry . dependencies ] pandas = { version = \"^2.2.1\" , extras =[ \"computation\" , \"performance\" ]} [ tool . poetry . group . dev . dependencies ] fastapi = { version = \"^0.92.0\" , extras =[ \"all\" ]} When installing or specifying Poetry-built packages, the extras defined in this section can be activated\nas described in PEP 508 . For example, when installing the package using pip , the dependencies required by\nthe databases extra can be installed as shown below. pip install awesome [ databases ] Note The dependencies specified for each extra must already be defined as project dependencies. Dependencies listed in dependency groups cannot be specified as extras. plugins # Poetry supports arbitrary plugins, which are exposed as the ecosystem-standard entry points and discoverable using importlib.metadata . This is similar to (and compatible with) the entry points feature of setuptools .\nThe syntax for registering a plugin is: [ tool . poetry . plugins ] # Optional super table [ tool . poetry . plugins . \"A\" ] B = \"C:D\" Which are: A - type of the plugin, for example poetry.plugin or flake8.extension B - name of the plugin C - python module import path D - the entry point of the plugin (a function or class) Example (from poetry-plugin-export ): [ tool . poetry . plugins . \"poetry.application.plugin\" ] export = \"poetry_plugin_export.plugins:ExportApplicationPlugin\" urls # In addition to the basic urls ( homepage , repository and documentation ), you can specify\nany custom url in the urls section. [ tool . poetry . urls ] \"Bug Tracker\" = \"https://github.com/python-poetry/poetry/issues\" If you publish your package on PyPI, they will appear in the Project Links section. Poetry and PEP-517 # PEP-517 introduces a standard way\nto define alternative build systems to build a Python project. Poetry is compliant with PEP-517, by providing a lightweight core library,\nso if you use Poetry to manage your Python project you should reference\nit in the build-system section of the pyproject.toml file like so: [ build - system ] requires = [ \"poetry-core>=1.0.0\" ] build - backend = \"poetry.core.masonry.api\" Note When using the new or init command this section will be automatically added. Note If your pyproject.toml file still references poetry directly as a build backend,\nyou should update it to reference poetry-core instead. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/managing-environments/", "text": "Managing environments | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Switching between environments Displaying the environment information Listing the environments associated with the project Deleting the environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Managing environments Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Managing environments Switching between environments Displaying the environment information Listing the environments associated with the project Deleting the environments Managing environments # Poetry makes project environment isolation one of its core features. What this means is that it will always work isolated from your global Python installation.\nTo achieve this, it will first check if it‚Äôs currently running inside a virtual environment.\nIf it is, it will use it directly without creating a new one. But if it‚Äôs not, it will use\none that it has already created or create a brand new one for you. By default, Poetry will try to use the Python version used during Poetry‚Äôs installation\nto create the virtual environment for the current project. However, for various reasons, this Python version might not be compatible\nwith the python range supported by the project. In this case, Poetry will try\nto find one that is and use it. If it‚Äôs unable to do so then you will be prompted\nto activate one explicitly, see Switching environments . Note If you use a tool like pyenv to manage different Python versions,\nyou can set the experimental virtualenvs.prefer-active-python option to true . Poetry\nwill then try to find the current python of your shell. For instance, if your project requires a newer Python than is available with\nyour system, a standard workflow would be: pyenv install 3.9.8\npyenv local 3.9.8 # Activate Python 3.9 for the current project poetry install Note Since version 1.2, Poetry no longer supports managing environments for Python 2.7. Switching between environments # Sometimes this might not be feasible for your system, especially Windows where pyenv is not available, or you simply prefer to have a more explicit control over your environment.\nFor this specific purpose, you can use the env use command to tell Poetry\nwhich Python version to use for the current project. poetry env use /full/path/to/python If you have the python executable in your PATH you can use it: poetry env use python3.7 You can even just use the minor Python version in this case: poetry env use 3.7 If you want to disable the explicitly activated virtual environment, you can use the\nspecial system Python version to retrieve the default behavior: poetry env use system Displaying the environment information # If you want to get basic information about the currently activated virtual environment,\nyou can use the env info command: poetry env info will output something similar to this: Virtualenv\nPython:         3.7.1\nImplementation: CPython\nPath:           /path/to/poetry/cache/virtualenvs/test-O3eWbxRl-py3.7\nValid:          True\n\nBase\nPlatform: darwin\nOS:       posix\nPython:   /path/to/main/python If you only want to know the path to the virtual environment, you can pass the --path option\nto env info : poetry env info --path If you only want to know the path to the python executable (useful for running mypy from a global environment without installing it in the virtual environment), you can pass the --executable option\nto env info : poetry env info --executable Listing the environments associated with the project # You can also list all the virtual environments associated with the current project\nwith the env list command: poetry env list will output something like the following: test-O3eWbxRl-py3.6\ntest-O3eWbxRl-py3.7 (Activated) You can pass the option --full-path to display the full path to the environments: poetry env list --full-path Deleting the environments # Finally, you can delete existing virtual environments by using env remove : poetry env remove /full/path/to/python\npoetry env remove python3.7\npoetry env remove 3.7\npoetry env remove test-O3eWbxRl-py3.7 You can delete more than one environment at a time. poetry env remove python3.6 python3.7 python3.8 Use the --all option to delete all virtual environments at once. poetry env remove --all If you remove the currently activated virtual environment, it will be automatically deactivated. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/dependency-specification/", "text": "Dependency specification | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Version constraints Caret requirements Tilde requirements Wildcard requirements Inequality requirements Exact requirements Using the @ operator git dependencies path dependencies url dependencies Dependency extras source dependencies Python restricted dependencies Using environment markers Multiple constraints dependencies Combining git / url / path dependencies with source repositories Expanded dependency specification syntax Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Dependency specification Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Dependency specification Version constraints Caret requirements Tilde requirements Wildcard requirements Inequality requirements Exact requirements Using the @ operator git dependencies path dependencies url dependencies Dependency extras source dependencies Python restricted dependencies Using environment markers Multiple constraints dependencies Combining git / url / path dependencies with source repositories Expanded dependency specification syntax Dependency specification # Dependencies for a project can be specified in various forms, which depend on the type\nof the dependency and on the optional constraints that might be needed for it to be installed. Version constraints # Caret requirements # Caret requirements allow SemVer compatible updates to a specified version. An update is allowed if the new version number does not modify the left-most non-zero digit in the major, minor, patch grouping. For instance, if we previously ran poetry add requests@^2.13.0 and wanted to update the library and ran poetry update requests , poetry would update us to version 2.14.0 if it was available, but would not update us to 3.0.0 . If instead we had specified the version string as ^0.1.13 , poetry would update to 0.1.14 but not 0.2.0 . 0.0.x is not considered compatible with any other version. Here are some more examples of caret requirements and the versions that would be allowed with them: Requirement Versions allowed ^1.2.3 >=1.2.3 <2.0.0 ^1.2 >=1.2.0 <2.0.0 ^1 >=1.0.0 <2.0.0 ^0.2.3 >=0.2.3 <0.3.0 ^0.0.3 >=0.0.3 <0.0.4 ^0.0 >=0.0.0 <0.1.0 ^0 >=0.0.0 <1.0.0 Tilde requirements # Tilde requirements specify a minimal version with some ability to update.\nIf you specify a major, minor, and patch version or only a major and minor version, only patch-level changes are allowed.\nIf you only specify a major version, then minor- and patch-level changes are allowed. ~1.2.3 is an example of a tilde requirement. Requirement Versions allowed ~1.2.3 >=1.2.3 <1.3.0 ~1.2 >=1.2.0 <1.3.0 ~1 >=1.0.0 <2.0.0 Wildcard requirements # Wildcard requirements allow for the latest (dependency dependent) version where the wildcard is positioned. * , 1.* and 1.2.* are examples of wildcard requirements. Requirement Versions allowed * >=0.0.0 1.* >=1.0.0 <2.0.0 1.2.* >=1.2.0 <1.3.0 Inequality requirements # Inequality requirements allow manually specifying a version range or an exact version to depend on. Here are some examples of inequality requirements: >= 1.2.0\n> 1\n< 2\n!= 1.2.3 Multiple requirements # Multiple version requirements can also be separated with a comma, e.g. >= 1.2, < 1.5 . Exact requirements # You can specify the exact version of a package. 1.2.3 is an example of an exact version specification. This will tell Poetry to install this version and this version only.\nIf other dependencies require a different version, the solver will ultimately fail and abort any install or update procedures. Exact versions can also be specified with == according to PEP 440 . ==1.2.3 is an example of this. Using the @ operator # When adding dependencies via poetry add , you can use the @ operator.\nThis is understood similarly to the == syntax, but also allows prefixing any\nspecifiers that are valid in pyproject.toml . For example: poetry add django@^4.0.0 The above would translate to the following entry in pyproject.toml : Django = \"^4.0.0\" The special keyword latest is also understood by the @ operator: poetry add django@latest The above would translate to the following entry in pyproject.toml , assuming the latest release of django is 4.0.5 : Django = \"^4.0.5\" Extras # Extras and @ can be combined as one might expect ( package[extra]@version ): poetry add django [ bcrypt ] @^4.0.0 git dependencies # To depend on a library located in a git repository,\nthe minimum information you need to specify is the location of the repository with the git key: [ tool . poetry . dependencies ] requests = { git = \"https://github.com/requests/requests.git\" } Since we haven‚Äôt specified any other information,\nPoetry assumes that we intend to use the latest commit on the main branch\nto build our project. You can combine the git key with the branch key to use another branch.\nAlternatively, use rev or tag to pin a dependency to a specific commit hash\nor tagged ref, respectively. For example: [ tool . poetry . dependencies ] # Get the latest revision on the branch named \"next\" requests = { git = \"https://github.com/kennethreitz/requests.git\" , branch = \"next\" } # Get a revision by its commit hash flask = { git = \"https://github.com/pallets/flask.git\" , rev = \"38eb5d3b\" } # Get a revision by its tag numpy = { git = \"https://github.com/numpy/numpy.git\" , tag = \"v0.13.2\" } In cases where the package you want to install is located in a subdirectory of the VCS repository, you can use the subdirectory option, similarly to what pip provides: [ tool . poetry . dependencies ] # Install a package named `subdir_package` from a folder called `subdir` within the repository subdir_package = { git = \"https://github.com/myorg/mypackage_with_subdirs.git\" , subdirectory = \"subdir\" } with the corresponding add call: poetry add \"git+https://github.com/myorg/mypackage_with_subdirs.git#subdirectory=subdir\" To use an SSH connection, for example in the case of private repositories, use the following example syntax: [ tool . poetry . dependencies ] requests = { git = \"git@github.com:requests/requests.git\" } To use HTTP basic authentication with your git repositories, you can configure credentials similar to\nhow repository credentials are configured. poetry config repositories.git-org-project https://github.com/org/project.git\npoetry config http-basic.git-org-project username token\npoetry add git+https://github.com/org/project.git Note With Poetry 1.2 releases, the default git client used is Dulwich . We fall back to legacy system git client implementation in cases where gitcredentials is used. This fallback will be removed in\na future release where gitcredentials helpers can be better supported natively. In cases where you encounter issues with the default implementation that used to work prior to\nPoetry 1.2, you may wish to explicitly configure the use of the system git client via a shell\nsubprocess call. poetry config experimental.system-git-client true Keep in mind however, that doing so will surface bugs that existed in versions prior to 1.2 which\nwere caused due to the use of the system git client. path dependencies # To depend on a library located in a local directory or file,\nyou can use the path property: [ tool . poetry . dependencies ] # directory my - package = { path = \"../my-package/\" , develop = false } # file my - package = { path = \"../my-package/dist/my-package-0.1.0.tar.gz\" } Note Before poetry 1.1 directory path dependencies were installed in editable mode by default. You should set the develop attribute explicitly,\nto make sure the behavior is the same for all poetry versions. url dependencies # To depend on a library located on a remote archive,\nyou can use the url property: [ tool . poetry . dependencies ] # directory my - package = { url = \"https://example.com/my-package-0.1.0.tar.gz\" } with the corresponding add call: poetry add https://example.com/my-package-0.1.0.tar.gz Dependency extras # You can specify PEP-508 Extras for a dependency as shown here. [ tool . poetry . dependencies ] gunicorn = { version = \"^20.1\" , extras = [ \"gevent\" ] } Note These activate extra defined for the dependency, to configure an optional dependency\nfor extras in your project refer to extras . source dependencies # To depend on a package from an alternate repository ,\nyou can use the source property: [[ tool . poetry . source ]] name = \"foo\" url = \"https://foo.bar/simple/\" priority = \"supplemental\" [ tool . poetry . dependencies ] my - cool - package = { version = \"*\" , source = \"foo\" } with the corresponding add call: poetry add my-cool-package --source foo Note In this example, we expect foo to be configured correctly. See using a private repository for further information. Python restricted dependencies # You can also specify that a dependency should be installed only for specific Python versions: [ tool . poetry . dependencies ] tomli = { version = \"^2.0.1\" , python = \"<3.11\" } [ tool . poetry . dependencies ] pathlib2 = { version = \"^2.2\" , python = \"^3.2\" } Using environment markers # If you need more complex install conditions for your dependencies,\nPoetry supports environment markers via the markers property: [ tool . poetry . dependencies ] pathlib2 = { version = \"^2.2\" , markers = \"python_version <= '3.4' or sys_platform == 'win32'\" } Multiple constraints dependencies # Sometimes, one of your dependency may have different version ranges depending\non the target Python versions. Let‚Äôs say you have a dependency on the package foo which is only compatible\nwith Python 3.6-3.7 up to version 1.9, and compatible with Python 3.8+ from version 2.0:\nyou would declare it like so: [ tool . poetry . dependencies ] foo = [ { version = \"<=1.9\" , python = \">=3.6,<3.8\" }, { version = \"^2.0\" , python = \">=3.8\" } ] Note The constraints must have different requirements (like python )\notherwise it will cause an error when resolving dependencies. Combining git / url / path dependencies with source repositories # Direct origin ( git / url / path ) dependencies can satisfy the requirement of a dependency that\ndoesn‚Äôt explicitly specify a source, even when mutually exclusive markers are used. For instance\nin the following example the url package will also be a valid solution for the second requirement: foo = [ { platform = \"darwin\" , url = \"https://example.com/example-1.0-py3-none-any.whl\" }, { platform = \"linux\" , version = \"^1.0\" }, ] Sometimes you may instead want to use a direct origin dependency for specific conditions\n(i.e. a compiled package that is not available on PyPI for a certain platform/architecture) while\nfalling back on source repositories in other cases. In this case you should explicitly ask for your\ndependency to be satisfied by another source . For example: foo = [ { platform = \"darwin\" , url = \"https://example.com/foo-1.0.0-py3-none-macosx_11_0_arm64.whl\" }, { platform = \"linux\" , version = \"^1.0\" , source = \"pypi\" }, ] Expanded dependency specification syntax # In the case of more complex dependency specifications, you may find that you\nend up with lines which are very long and difficult to read. In these cases,\nyou can shift from using ‚Äúinline table‚Äù syntax, to the ‚Äústandard table‚Äù syntax. An example where this might be useful is the following: [ tool . poetry . group . dev . dependencies ] black = { version = \"19.10b0\" , allow - prereleases = true , python = \"^3.7\" , markers = \"platform_python_implementation == 'CPython'\" } As a single line, this is a lot to digest. To make this a bit easier to\nwork with, you can do the following: [ tool . poetry . group . dev . dependencies . black ] version = \"19.10b0\" allow - prereleases = true python = \"^3.7\" markers = \"platform_python_implementation == 'CPython'\" The same information is still present, and ends up providing the exact\nsame specification. It‚Äôs simply split into multiple, slightly more readable,\nlines. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/contributing/", "text": "Contributing to Poetry | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry How to contribute Reporting bugs Suggesting enhancements Documentation contributions Code contributions Issue triage Community FAQ pre-commit hooks Contributing to Poetry Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Contributing to Poetry How to contribute Reporting bugs Suggesting enhancements Documentation contributions Code contributions Issue triage Contributing to Poetry # First off, thanks for taking the time to contribute! The following is a set of guidelines for contributing to Poetry on GitHub. These are mostly guidelines, not rules. Use\nyour best judgement, and feel free to propose changes to this document in a pull request. How to contribute # Reporting bugs # This section guides you through submitting a bug report for Poetry.\nFollowing these guidelines helps maintainers and the community understand your report, reproduce the behavior, and find\nrelated reports. Before submitting a bug report # Check the FAQ for a list of common questions and problems. Check the blog for release notes from recent releases, including steps for upgrading and known issues. Check that your issue does not already exist in the issue tracker . Make sure your issue is really a bug, and is not a support request or question better suited for Discussions or Discord . Note If you find a Closed issue that seems like it is the same thing that you‚Äôre experiencing, open a new issue and\ninclude a link to the original issue in the body of your new one. How do I submit a bug report? # Bugs concerning Poetry and poetry-core should be submitted to the main issue tracker , using the correct issue template . Explain the problem and make it easy for others to search for and understand: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. Describe the behavior you observed after following the steps and point out how this is a bug. Explain which behavior you expected to see instead and why. If the problem involves an unexpected error being raised , execute the problematic command in debug mode\n(with -vvv flag). Provide detailed steps for reproduction of your issue: Provide your pyproject.toml file in a Gist , pastebin, or example repository after\nremoving potential private information like private package repositories or names. Provide specific examples to demonstrate the steps to reproduce the issue . This could be an example repository, a\nsequence of steps run in a container, or just a pyproject.toml for very simple cases. Are you unable to reliably reproduce the issue? If so, provide details about how often the problem happens\nand under which conditions it normally happens. Provide more context by answering these questions: Did the problem start happening recently (e.g. after updating to a new version of Poetry) or was this always a\nproblem? If the problem started happening recently, can you reproduce the problem in an older version of Poetry? What‚Äôs the\nmost recent version in which the problem does not happen? Is there anything exotic or unusual about your environment? This could include use of special container images,\nnewer CPU architectures like Apple Silicon, or corporate proxies that intercept or modify your network traffic. Include details about your configuration and environment: Which version of Poetry are you using? You can get the exact version by running poetry --version . What version of Python is being used to run Poetry? Execute the poetry debug info to get this information. What‚Äôs the name and version of the OS you‚Äôre using? Examples include Ubuntu 22.04 or macOS 12.6. To give others the best chance to understand and reproduce your issue, please be sure to put extra effort into your\nreproduction steps. You can both rule out local configuration issues on your end, and ensure others can cleanly\nreproduce your issue if attempt all reproductions in a pristine container (or VM), and provide the steps you performed\ninside that container/VM in your issue report. Suggesting enhancements # This section guides you through submitting an enhancement suggestion for Poetry, including completely new features as\nwell as improvements to existing functionality. Following these guidelines helps maintainers and the community\nunderstand your suggestion and find related suggestions. Before submitting a suggested enhancement # Check the FAQ for a list of common questions and problems. Check that your issue does not already exist in the issue tracker . How do I submit a suggested enhancement? # Suggested enhancements concerning Poetry and poetry-core should be submitted to the main issue tracker , using the\ncorrect issue template . Use a clear and descriptive title for the issue to identify the suggestion. Provide a detailed description of the proposed enhancement , with specific steps or examples when possible. Describe the current behavior and explain which behavior you would like to see instead , and why. Documentation contributions # One of the simplest ways to get started contributing to a project is through improving documentation. Poetry is\nconstantly evolving, and this means that sometimes our documentation has gaps. You can help by adding missing sections,\nediting the existing content to be more accessible, or creating new content such as tutorials, FAQs, etc. Note Github Discussions and the kind/question label are excellent sources for FAQ\ncandidates. Issues pertaining to the documentation are usually marked with the area/docs label , which will also trigger a preview\nof the changes as rendered by this website. Code contributions # Picking an issue # Note If you are a first time contributor, and are looking for an issue to take on, you might want to look for\nat the contributing page for candidates. We do our best to curate\ngood issues for first-time contributors there, but do fall behind ‚Äì so if you don‚Äôt see anything good, feel free to\nask. If you would like to take on an issue, feel free to comment on the issue tagging @python-poetry/triage .\nWe are more than happy to discuss solutions on the issue. If you would like help with navigating the code base, are\nlooking for something to work on, or want feedback on a design or change, join us on our Discord server or\nstart a Discussion . Local development # Poetry is developed using Poetry. Refer to the documentation to install Poetry in your local environment. Note Poetry‚Äôs development toolchain requires Python 3.8 or newer. You should first fork the Poetry repository and then clone it locally, so that you can make pull requests against the\nproject. If you are new to Git and pull request based development, GitHub provides a guide you will find helpful. Next, you should install Poetry‚Äôs dependencies, and run the test suite to make sure everything is working as expected: poetry install\npoetry run pytest When you contribute to Poetry, automated tools will be run to make sure your code is suitable to be merged. Besides\npytest, you will need to make sure your code typechecks properly using mypy : poetry run mypy Finally, a great deal of linting tools are run on your code, to try and ensure consistent code style and root out common\nmistakes. The pre-commit tool is used to install and run these tools, and requires one-time\nsetup: poetry run pre-commit install pre-commit will now run and check your code every time you make a commit. By default, it will only run on changed files,\nbut you can run it on all files manually (this may be useful if you altered the pre-commit config): poetry run pre-commit run --all-files Pull requests # Fill out the pull request body completely and describe your changes as accurately as possible. The pull request body\nshould be kept up to date as it will usually form the base for the final merge commit and the changelog entry. Be sure that your pull request contains tests that cover the changed or added code. Tests are generally required for\ncode be to be considered mergeable, and code without passing tests will not be merged. Ensure your pull request passes the mypy and pre-commit checks. Remember that you can run these tools locally\ninstead of relying on remote CI. If your changes warrant a documentation change, the pull request must also update the documentation. Make sure to\nreview the documentation preview generated by CI for any rendering issues. Note Make sure your branch is rebased against the latest\nbase branch. A maintainer might ask you to ensure the branch is up-to-date prior to merging your pull request\n(especially if there have been CI changes on the base branch), and will also ask you to fix any conflicts. All pull requests, unless otherwise instructed, need to be first accepted into the main branch. Maintainers will\ngenerally decide if any backports to other branches are required, and carry them out as needed. Issue triage # Note If you have an issue that hasn‚Äôt had any attention, you can ping us @python-poetry/triage on the issue. Please give us\nreasonable time to get to your issue first, and avoid pinging any individuals directly, especially if they are not part\nof the Poetry team. If you are helping with the triage of reported issues, this section provides some useful information to assist you in\nyour contribution. Triage steps # Determine what area and versions of Poetry the issue is related to, and set the appropriate labels (e.g. version/x.x.x , area/docs , area/venv ), and remove the status/triage label. If requested information (such as debug logs, pyproject.toml, etc.) is not provided and is relevant, request it from\nthe author. Set the status/waiting-on-response label while waiting to hear back from the author. Attempt to reproduce the issue with the reported Poetry version or request further clarification from the author. Ensure the issue is not already resolved. Try reproducing it on the latest stable release, the latest prerelease (if\npresent), and the development branch. If the issue cannot be reproduced, request more reproduction steps and clarification from the issue‚Äôs author, set the status/needs-reproduction label, close the issue if there is no reproduction forthcoming. If the issue can be reproduced, comment on the issue confirming so, set the status/confirmed label, if possible, identify the root cause of the issue, if interested, attempt to fix it via a pull request. Multiple versions # When trying to reproduce issues, you often want to use multiple versions of Poetry at the same time. pipx makes this easy to do: pipx install --suffix @1.2.1 'poetry==1.2.1' pipx install --suffix @1.3.0rc1 'poetry==1.3.0rc1' pipx install --suffix @main 'poetry @ git+https://github.com/python-poetry/poetry' pipx install --suffix @local '/path/to/local/clone/of/poetry' # now you can use any of the chosen versions of Poetry with their configured suffix, e.g. poetry@main --version Note Do not forget to pipx upgrade poetry@main before using it, to make sure you have the latest changes. Note This mechanism can also be used to test pull requests by using GitHub‚Äôs pull request remote refs: pipx install --suffix @pr1234 git+https://github.com/python-poetry/poetry.git@refs/pull/1234/head Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/blog/", "text": "Blog | Poetry - Python dependency management and packaging made easy Home Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Announcing Poetry 1.8.0 Published on February 25, 2024 in Releases with tags 1.X 1.8 The Poetry team is pleased to announce the immediate availability of Poetry 1.8.0 . Announcing Poetry 1.7.0 Published on November 3, 2023 in Releases with tags 1.X 1.7 The Poetry team is pleased to announce the immediate availability of Poetry 1.7.0 . Announcing Poetry 1.6.0 Published on August 20, 2023 in Releases with tags 1.X 1.6 The Poetry team is pleased to announce the immediate availability of Poetry 1.6.0 . Announcing Poetry 1.5.0 Published on May 19, 2023 in Releases with tags 1.X 1.5 The Poetry team is pleased to announce the immediate availability of Poetry 1.5.0 . Announcing Poetry 1.4.0 Published on February 27, 2023 in Releases with tags 1.X 1.4 The Poetry team is pleased to announce the immediate availability of Poetry 1.4.0 . Announcing Poetry 1.3.0 Published on December 9, 2022 in Releases with tags 1.X 1.3 The Poetry team is pleased to announce the immediate availability of Poetry 1.3.0 . Announcing Poetry 1.2.2 Published on October 10, 2022 in Releases with tags 1.X 1.2 The Poetry team is pleased to announce the immediate availability of Poetry 1.2.2 . Announcing Poetry 1.2.1 Published on September 16, 2022 in Releases with tags 1.X 1.2 The Poetry team is pleased to announce the immediate availability of Poetry 1.2.1 . Announcing Poetry 1.2.0 Published on August 31, 2022 in Releases with tags 1.X 1.2 The Poetry team is pleased to announce the immediate availability of Poetry 1.2.0 . Announcing Poetry 1.2.0rc2 Published on August 26, 2022 in Releases with tags 1.X 1.2 The Poetry team is pleased to announce the immediate availability of Poetry 1.2.0rc2 . ¬´¬´ ¬´ 1 2 3 &mldr; 9 ¬ª ¬ª¬ª Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/faq/", "text": "FAQ | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ Why is the dependency resolution process slow? What kind of versioning scheme does Poetry use for itself? Why does Poetry not adhere to semantic versioning? Are unbound version constraints a bad idea? Is tox supported? Is Nox supported? I don‚Äôt want Poetry to manage my virtual environments. Can I disable it? Why is Poetry telling me that the current project‚Äôs supported Python range is not compatible with one or more packages' Python requirements? Why does Poetry enforce PEP 440 versions? Poetry busts my Docker cache because it requires me to COPY my source files in before installing 3rd party dependencies My requests are timing out! pre-commit hooks FAQ Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks FAQ Why is the dependency resolution process slow? What kind of versioning scheme does Poetry use for itself? Why does Poetry not adhere to semantic versioning? Are unbound version constraints a bad idea? Is tox supported? Is Nox supported? I don‚Äôt want Poetry to manage my virtual environments. Can I disable it? Why is Poetry telling me that the current project‚Äôs supported Python range is not compatible with one or more packages' Python requirements? Why does Poetry enforce PEP 440 versions? Poetry busts my Docker cache because it requires me to COPY my source files in before installing 3rd party dependencies My requests are timing out! FAQ # Why is the dependency resolution process slow? # While the dependency resolver at the heart of Poetry is highly optimized and\nshould be fast enough for most cases, with certain sets of dependencies\nit can take time to find a valid solution. This is due to the fact that not all libraries on PyPI have properly declared their metadata\nand, as such, they are not available via the PyPI JSON API. At this point, Poetry has no choice\nbut to download the packages and inspect them to get the necessary information. This is an expensive\noperation, both in bandwidth and time, which is why it seems this is a long process. At the moment there is no way around it. However, if you notice that Poetry\nis downloading many versions of a single package, you can lessen the workload\nby constraining that one package in your pyproject.toml more narrowly. That way\nPoetry does not have to sift through so many versions of it, which may speed up\nthe locking process considerably in some cases. Note Once Poetry has cached the releases' information on your machine, the dependency resolution process\nwill be much faster. What kind of versioning scheme does Poetry use for itself? # Poetry uses ‚Äúmajor.minor.micro‚Äù version identifiers as mentioned in PEP 440 . Version bumps are done similar to Python‚Äôs versioning: A major version bump (incrementing the first number) is only done for breaking changes\nif a deprecation cycle is not possible and many users have to perform some manual steps\nto migrate from one version to the next one. A minor version bump (incrementing the second number) may include new features as well\nas new deprecations and drop features deprecated in an earlier minor release. A micro version bump (incrementing the third number) usually only includes bug fixes.\nDeprecated features will not be dropped in a micro release. Why does Poetry not adhere to semantic versioning? # Because of its large user base, even small changes not considered relevant by most users\ncan turn out to be a breaking change for some users in hindsight.\nSticking to strict semantic versioning and (almost) always bumping\nthe major version instead of the minor version does not seem desirable\nsince the minor version will not carry any meaning anymore. Are unbound version constraints a bad idea? # A version constraint without an upper bound such as * or >=3.4 will allow updates to any future version of the dependency.\nThis includes major versions breaking backward compatibility. Once a release of your package is published, you cannot tweak its dependencies anymore in case a dependency breaks BC\n‚Äì you have to do a new release but the previous one stays broken.\n(Users can still work around the broken dependency by restricting it by themselves.) To avoid such issues you can define an upper bound on your constraints,\nwhich you can increase in a new release after testing that your package is compatible\nwith the new major version of your dependency. For example instead of using >=3.4 you can use ^3.4 which allows all versions <4.0 .\nThe ^ operator works very well with libraries following semantic versioning . However, when defining an upper bound, users of your package are not able to update\na dependency beyond the upper bound even if it does not break anything\nand is fully compatible with your package.\nYou have to release a new version of your package with an increased upper bound first. If your package will be used as a library in other packages, it might be better to avoid\nupper bounds and thus unnecessary dependency conflicts (unless you already know for sure\nthat the next release of the dependency will break your package).\nIf your package will be used as an application, it might be worth to define an upper bound. Is tox supported? # Yes . Provided that you are using tox >= 4, you can use it in combination with\nthe PEP 517 compliant build system provided by Poetry. (With tox 3, you have to set the isolated build option.) So, in your pyproject.toml file, add this section if it does not already exist: [ build - system ] requires = [ \"poetry-core>=1.0.0\" ] build - backend = \"poetry.core.masonry.api\" tox can be configured in multiple ways. It depends on what should be the code under test and which dependencies\nshould be installed. Use case #1 # [tox] [testenv] deps = pytest commands = pytest tests/ --import-mode importlib tox will create an sdist package of the project and uses pip to install it in a fresh environment.\nThus, dependencies are resolved by pip . Use case #2 # [tox] [testenv] allowlist_externals = poetry commands_pre = poetry install --no-root --sync commands = poetry run pytest tests/ --import-mode importlib tox will create an sdist package of the project and uses pip to install it in a fresh environment.\nThus, dependencies are resolved by pip in the first place. But afterward we run Poetry,\nwhich will install the locked dependencies into the environment. Use case #3 # [tox] [testenv] skip_install = true allowlist_externals = poetry commands_pre = poetry install commands = poetry run pytest tests/ --import-mode importlib tox will not do any install. Poetry installs all the dependencies and the current package in editable mode.\nThus, tests are running against the local files and not the built and installed package. Note about credentials # Note that tox does not forward the environment variables of your current shell session by default.\nThis may cause Poetry to not be able to install dependencies in the tox environments if you have configured\ncredentials using the system keyring on Linux systems or using environment variables in general.\nYou can use the passenv configuration option to forward the\nrequired variables explicitly or passenv = \"*\" to forward all of them.\nLinux systems may require forwarding the DBUS_SESSION_BUS_ADDRESS variable to allow access to the system keyring,\nthough this may vary between desktop environments. Alternatively, you can disable the keyring completely: poetry config keyring.enabled false Be aware that this will cause Poetry to write passwords to plaintext config files.\nYou will need to set the credentials again after changing this setting. Is Nox supported? # Use the nox-poetry package to install locked versions of\ndependencies specified in poetry.lock into Nox sessions. I don‚Äôt want Poetry to manage my virtual environments. Can I disable it? # While Poetry automatically creates virtual environments to always work isolated\nfrom the global Python installation, there are rare scenarios where the use a Poetry managed\nvirtual environment is not possible or preferred. In this case, you can disable this feature by setting the virtualenvs.create setting to false : poetry config virtualenvs.create false Warning The recommended best practice, including when installing an application within a container, is to make\nuse of a virtual environment. This can also be managed by another tool. The Poetry team strongly encourages the use of a virtual environment. Why is Poetry telling me that the current project‚Äôs supported Python range is not compatible with one or more packages' Python requirements? # Unlike pip , Poetry doesn‚Äôt resolve for just the Python in the current environment. Instead it makes sure that a dependency\nis resolvable within the given Python version range in pyproject.toml . Assume you have the following pyproject.toml : [ tool . poetry . dependencies ] python = \"^3.7\" This means your project aims to be compatible with any Python version >=3.7,<4.0. Whenever you try to add a dependency\nwhose Python requirement doesn‚Äôt match the whole range Poetry will tell you this, e.g.: The current project's supported Python range (>=3.7.0,<4.0.0) is not compatible with some of the required packages Python requirement:\n    - scipy requires Python >=3.7,<3.11, so it will not be satisfied for Python >=3.11,<4.0.0 Usually you will want to match the supported Python range of your project with the upper bound of the failing dependency.\nAlternatively you can tell Poetry to install this dependency only for a specific range of Python versions ,\nif you know that it‚Äôs not needed in all versions. Why does Poetry enforce PEP 440 versions? # This is done to be compliant with the broader Python ecosystem. For example, if Poetry builds a distribution for a project that uses a version that is not valid according to PEP 440 , third party tools will be unable to parse the version correctly. Poetry busts my Docker cache because it requires me to COPY my source files in before installing 3rd party dependencies # By default, running poetry install ... requires you to have your source files present (both the ‚Äúroot‚Äù package and any directory path dependencies you might have).\nThis interacts poorly with Docker‚Äôs caching mechanisms because any change to a source file will make any layers (subsequent commands in your Dockerfile) re-run.\nFor example, you might have a Dockerfile that looks something like this: FROM python\nCOPY pyproject.toml poetry.lock .\nCOPY src/ ./src\nRUN pip install poetry && poetry install --only main As soon as any source file changes, the cache for the RUN layer will be invalidated, which forces all 3rd party dependencies (likely the slowest step out of these) to be installed again if you changed any files in src/ . To avoid this cache busting you can split this into two steps: Install 3rd party dependencies. Copy over your source code and install just the source code. This might look something like this: FROM python\nCOPY pyproject.toml poetry.lock .\nRUN pip install poetry && poetry install --only main --no-root --no-directory\nCOPY src/ ./src\nRUN poetry install --only main The two key options we are using here are --no-root (skips installing the project source) and --no-directory (skips installing any local directory path dependencies, you can omit this if you don‚Äôt have any). More information on the options available for poetry install . My requests are timing out! # Poetry‚Äôs default HTTP request timeout is 15 seconds, the same as pip .\nSimilar to PIP_REQUESTS_TIMEOUT , the experimental environment variable POETRY_REQUESTS_TIMEOUT can be set to alter this value. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/history/", "text": "History | Poetry - Python dependency management and packaging made easy Home Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History History 1.8.3 - 2024-05-08 # Added # Add support for untagged CPython builds with versions ending with a + ( #9207 ). Changed # Require pkginfo>=1.10 to ensure support for packages with metadata version 2.3 ( #9130 ). Improve locking on FIPS systems ( #9152 ). Fixed # Fix an issue where unrecognized package metadata versions silently resulted in empty dependencies ( #9203 , #9226 ). Fix an issue where trailing slashes in git URLs where not handled correctly ( #9205 ). Fix an issue where poetry self commands printed a warning that the current project cannot be installed ( #9302 ). Fix an issue where poetry install sporadically failed with a KeyError due to a race condition ( #9335 ). Docs # Fix incorrect information about poetry shell ( #9060 ). Add a git subdirectory example to poetry add ( #9080 ). Mention interactive credential configuration ( #9074 ). Add notes for optional advanced installation steps ( #9098 ). Add reference to configuration credentials in documentation of poetry publish ( #9110 ). Improve documentation for configuring credentials via environment variables ( #9121 ). Remove misleading wording around virtual environments ( #9213 ). Remove outdated advice regarding seeding keyring backends ( #9164 ). Add a pyproject.toml example for a dependency with multiple extras ( #9138 ). Clarify help of poetry add ( #9230 ). Add a note how to configure credentials for TestPyPI for poetry publish ( #9255 ). Fix information about the --readme option in poetry new ( #9260 ). Clarify what is special about the Python constraint in dependencies ( #9256 ). Update how to uninstall plugins via pipx ( #9320 ). 1.8.2 - 2024-03-02 # Fixed # Harden lazy-wheel error handling if the index server is behaving badly in an unexpected way ( #9051 ). Improve lazy-wheel error handling if the index server does not handle HTTP range requests correctly ( #9082 ). Improve lazy-wheel error handling if the index server pretends to support HTTP range requests but does not respect them ( #9084 ). Improve lazy-wheel to allow redirects for HEAD requests ( #9087 ). Improve debug logging for lazy-wheel errors ( #9059 ). Fix an issue where the hash of a metadata file could not be calculated correctly due to an encoding issue ( #9049 ). Fix an issue where poetry add failed in non-package mode if no project name was set ( #9046 ). Fix an issue where a hint to non-package mode was not compliant with the final name of the setting ( #9073 ). 1.8.1 - 2024-02-26 # Fixed # Update the minimum required version of packaging ( #9031 ). Handle unexpected responses from servers that do not support HTTP range requests with negative offsets more robust ( #9030 ). Docs # Rename master branch to main ( #9022 ). 1.8.0 - 2024-02-25 # Added # Add a non-package mode for use cases where Poetry is only used for dependency management ( #8650 ). Add support for PEP 658 to fetch metadata without having to download wheels ( #5509 ). Add a lazy-wheel config option (default: true ) to reduce wheel downloads during dependency resolution ( #8815 , #8941 ). Improve performance of dependency resolution by using shallow copies instead of deep copies ( #8671 ). poetry check validates that no unknown sources are referenced in dependencies ( #8709 ). Add archive validation during installation for further hash algorithms ( #8851 ). Add a to key in tool.poetry.packages to allow custom subpackage names ( #8791 ). Add a config option to disable keyring ( #8910 ). Add a --sync option to poetry update ( #8931 ). Add an --output option to poetry build ( #8828 ). Add a --dist-dir option to poetry publish ( #8828 ). Changed # The implicit PyPI source is disabled if at least one primary source is configured ( #8771 ). Deprecate source priority default ( #8771 ). Upgrade the warning about an inconsistent lockfile to an error ( #8737 ). Deprecate setting installer.modern-installation to false ( #8988 ). Drop support for pip<19 ( #8894 ). Require requests-toolbelt>=1 ( #8680 ). Allow platformdirs 4.x ( #8668 ). Allow and require xattr 1.x on macOS ( #8801 ). Improve venv shell activation in fish ( #8804 ). Rename system to base in output of poetry env info ( #8832 ). Use pretty name in output of poetry version ( #8849 ). Improve error handling for invalid entries in tool.poetry.scripts ( #8898 ). Improve verbose output for dependencies with extras during dependency resolution ( #8834 ). Improve message about an outdated lockfile ( #8962 ). Fixed # Fix an issue where poetry shell failed when Python has been installed with MSYS2 ( #8644 ). Fix an issue where Poetry commands failed in a terminal with a non-UTF-8 encoding ( #8608 ). Fix an issue where a missing project name caused an incomprehensible error message ( #8691 ). Fix an issue where Poetry failed to install an sdist path dependency ( #8682 ). Fix an issue where poetry install failed because an unused extra was not available ( #8548 ). Fix an issue where poetry install --sync did not remove an unrequested extra ( #8621 ). Fix an issue where poetry init did not allow specific characters in the author field ( #8779 ). Fix an issue where Poetry could not download sdists from misconfigured servers ( #8701 ). Fix an issue where metadata of sdists that call CLI tools of their build requirements could not be determined ( #8827 ). Fix an issue where Poetry failed to use the currently activated environment ( #8831 ). Fix an issue where poetry shell failed in zsh if a space was in the venv path ( #7245 ). Fix an issue where scripts with extras could not be installed ( #8900 ). Fix an issue where explicit sources where not propagated correctly ( #8835 ). Fix an issue where debug prints where swallowed when using a build script ( #8760 ). Fix an issue where explicit sources of locked dependencies where not propagated correctly ( #8948 ). Fix an issue where Poetry‚Äôs own environment was falsely identified as system environment ( #8970 ). Fix an issue where dependencies from a setup.py were ignored silently ( #9000 ). Fix an issue where environment variables for virtualenv.options were ignored ( #9015 ). Fix an issue where virtualenvs.options.no-pip and virtualenvs.options.no-setuptools were not normalized ( #9015 ). Docs # Replace deprecated --no-dev with --without dev in the FAQ ( #8659 ). Recommend poetry-check instead of the deprecated poetry-lock pre-commit hook ( #8675 ). Clarify the names of the environment variables to provide credentials for repositories ( #8782 ). Add note how to install several version of Poetry in parallel ( #8814 ). Improve description of poetry show --why ( #8817 ). Improve documentation of poetry update ( #8706 ). Add a warning about passing variables that may start with a hyphen via command line ( #8850 ). Mention that the virtual environment in which Poetry itself is installed should not be activated ( #8833 ). Add note about poetry run and externally managed environments ( #8748 ). Update FAQ entry about tox for tox 4.x ( #8658 ). Fix documentation for default format option for include and exclude value ( #8852 ). Add note about tox and configured credentials ( #8888 ). Add note and link how to install pipx ( #8878 ). Fix examples for poetry add with git dependencies over ssh ( #8911 ). Remove reference to deprecated scripts extras feature ( #8903 ). Change examples to prefer --only main instead of --without dev ( #8921 ). Mention that the develop attribute is a Poetry-specific feature and not propagated to other tools ( #8971 ). Fix examples for adding supplemental and secondary sources ( #8953 ). Add PyTorch example for explicit sources ( #9006 ). poetry-core ( 1.9.0 ) # Deprecate scripts that depend on extras ( #690 ). Add support for path dependencies that do not define a build system ( #675 ). Update list of supported licenses ( #659 , #669 , #678 , #694 ). Rework list of files included in build artifacts ( #666 ). Fix an issue where insignificant errors were printed if the working directory is not inside a git repository ( #684 ). Fix an issue where the project‚Äôs directory was not recognized as git repository on Windows due to an encoding issue ( #685 ). 1.7.1 - 2023-11-16 # Fixed # Fix an issue where sdists that call CLI tools of their build requirements could not be installed ( #8630 ). Fix an issue where sdists with symlinks could not be installed due to a broken tarfile datafilter ( #8649 ). Fix an issue where poetry init failed when trying to add dependencies ( #8655 ). Fix an issue where poetry install failed if virtualenvs.create was set to false ( #8672 ). 1.7.0 - 2023-11-03 # Added # Add official support for Python 3.12 ( #7803 , #8544 ). Print a future warning that poetry-plugin-export will not be installed by default anymore ( #8562 ). Add poetry-install pre-commit hook ( #8327 ). Add --next-phase option to poetry version ( #8089 ). Print a warning when overwriting files from another package at installation ( #8386 ). Print a warning if the current project cannot be installed ( #8369 ). Report more details on build backend exceptions ( #8464 ). Changed # Set Poetry as user-agent for all HTTP requests ( #8394 ). Do not install setuptools per default in Python 3.12 ( #7803 ). Do not install wheel per default ( #7803 ). Remove setuptools and wheel when running poetry install --sync if they are not required by the project ( #8600 ). Improve error message about PEP-517 support ( #8463 ). Improve keyring handling ( #8227 ). Read the description field when extracting metadata from setup.py files ( #8545 ). Fixed # Fix an issue where dependencies of inactive extras were locked and installed ( #8399 ). Fix an issue where build requirements were not installed due to a race condition in the artifact cache ( #8517 ). Fix an issue where packages included in the system site packages were installed even though virtualenvs.options.system-site-packages was set ( #8359 ). Fix an issue where git dependencies' submodules with relative URLs were handled incorrectly ( #8020 ). Fix an issue where a failed installation of build dependencies was not noticed directly ( #8479 ). Fix an issue where poetry shell did not work completely with nushell ( #8478 ). Fix an issue where a confusing error messages was displayed when running poetry config pypi-token.pypi without a value ( #8502 ). Fix an issue where a cryptic error message is printed if there is no metadata entry in the lockfile ( #8523 ). Fix an issue with the encoding with special characters in the virtualenv‚Äôs path ( #8565 ). Fix an issue where the connection pool size was not adjusted to the number of workers ( #8559 ). Docs # Improve the wording regarding a project‚Äôs supported Python range ( #8423 ). Make pipx the preferred (first mentioned) installation method ( #8090 ). Add a warning about poetry self on Windows ( #8090 ). Fix example for poetry add with a git dependency ( #8438 ). Add information about auto-included files in wheels and sdist ( #8555 ). Fix documentation of the POETRY_REPOSITORIES_ variables docs ( #8492 ). Add CITATION.cff file ( #8510 ). poetry-core ( 1.8.1 ) # Add support for creating packages dynamically in the build script ( #629 ). Improve marker logic for extra markers ( #636 ). Update list of supported licenses ( #635 , #646 ). Fix an issue where projects with extension modules were not installed in editable mode ( #633 ). Fix an issue where the wrong or no lib folder was added to the wheel ( #634 ). poetry-plugin-export ( ^1.6.0 ) # Add an --all-extras option ( #241 ). Fix an issue where git dependencies are exported with the branch name instead of the resolved commit hash ( #213 ). 1.6.1 - 2023-08-21 # Fixed # Update the minimum required version of requests ( #8336 ). 1.6.0 - 2023-08-20 # Added # Add support for repositories that do not provide a supported hash algorithm ( #8118 ). Add full support for duplicate dependencies with overlapping markers ( #7257 ). Improve performance of poetry lock for certain edge cases ( #8256 ). Improve performance of poetry install ( #8031 ). poetry check validates that specified readme files do exist ( #7444 ). Add a downgrading note when updating to an older version ( #8176 ). Add support for vox in the xonsh shell ( #8203 ). Add support for pre-commit hooks for projects where the pyproject.toml file is located in a subfolder ( #8204 ). Add support for the git+http:// scheme ( #6619 ). Changed # Drop support for Python 3.7 ( #7674 ). Move poetry lock --check to poetry check --lock and deprecate the former ( #8015 ). Change future warning that PyPI will only be disabled automatically if there are no primary sources ( #8151 ). Fixed # Fix an issue where build-system.requires were not respected for projects with build scripts ( #7975 ). Fix an issue where the encoding was not handled correctly when calling a subprocess ( #8060 ). Fix an issue where poetry show --top-level did not show top level dependencies with extras ( #8076 ). Fix an issue where poetry init handled projects with src layout incorrectly ( #8218 ). Fix an issue where Poetry wrote .pth files with the wrong encoding ( #8041 ). Fix an issue where poetry install did not respect the source if the same version of a package has been locked from different sources ( #8304 ). Docs # Document official Poetry badge ( #8066 ). Update configuration folder path for macOS ( #8062 ). Add a warning about pip ignoring lock files ( #8117 ). Clarify the use of the virtualenvs.in-project setting. ( #8126 ). Change pre-commit YAML style to be consistent with pre-commit‚Äôs own examples ( #8146 ). Fix command for listing installed plugins ( #8200 ). Mention the nox-poetry package ( #8173 ). Add an example with a PyPI source in the pyproject.toml file ( #8171 ). Use reference instead of deprecated callable in the scripts example ( #8211 ). poetry-core ( 1.7.0 ) # Improve performance of marker handling ( #609 ). Allow | as a value separator in markers with the operators in and not in ( #608 ). Put pretty name (instead of normalized name) in metadata ( #620 ). Update list of supported licenses ( #623 ). Fix an issue where PEP 508 dependency specifications with names starting with a digit could not be parsed ( #607 ). Fix an issue where Poetry considered an unrelated .gitignore file resulting in an empty wheel ( #611 ). poetry-plugin-export ( ^1.5.0 ) # Fix an issue where markers for dependencies required by an extra were not generated correctly ( #209 ). 1.5.1 - 2023-05-29 # Added # Improve dependency resolution performance in cases with a lot of backtracking ( #7950 ). Changed # Disable wheel content validation during installation ( #7987 ). Fixed # Fix an issue where partially downloaded wheels were cached ( #7968 ). Fix an issue where poetry run did no longer execute relative-path scripts ( #7963 ). Fix an issue where dependencies were not installed in in-project environments ( #7977 ). Fix an issue where no solution was found for a transitive dependency on a pre-release of a package ( #7978 ). Fix an issue where cached repository packages were incorrectly parsed, leading to its dependencies being ignored ( #7995 ). Fix an issue where an explicit source was ignored so that a direct origin dependency was used instead ( #7973 ). Fix an issue where the installation of big wheels consumed a lot of memory ( #7987 ). Docs # Add information about multiple constraints dependencies with direct origin and version dependencies ( #7973 ). poetry-core ( 1.6.1 ) # Fix an endless recursion in marker handling ( #593 ). Fix an issue where the wheel tag was not built correctly under certain circumstances ( #591 ). poetry-plugin-export ( ^1.4.0 ) # Fix an issue where --extra-index-url and --trusted-host was not generated for sources with priority explicit ( #205 ). 1.5.0 - 2023-05-19 # Added # Introduce the new source priorities explicit and supplemental ( #7658 , #6879 ). Introduce the option to configure the priority of the implicit PyPI source ( #7801 ). Add handling for corrupt cache files ( #7453 ). Improve caching of URL and git dependencies ( #7693 , #7473 ). Add option to skip installing directory dependencies ( #6845 , #7923 ). Add --executable option to poetry env info ( #7547 ). Add --top-level option to poetry show ( #7415 ). Add --lock option to poetry remove ( #7917 ). Add experimental POETRY_REQUESTS_TIMEOUT option ( #7081 ). Improve performance of wheel inspection by avoiding unnecessary file copy operations ( #7916 ). Changed # Remove the old deprecated installer and the corresponding setting experimental.new-installer ( #7356 ). Introduce priority key for sources and deprecate flags default and secondary ( #7658 ). Deprecate poetry run <entry point> if the entry point was not previously installed via poetry install ( #7606 ). Only write the lock file if the installation succeeds ( #7498 ). Do not write the unused package category into the lock file ( #7637 ). Fixed # Fix an issue where Poetry‚Äôs internal pyproject.toml continually grows larger with empty lines ( #7705 ). Fix an issue where Poetry crashes due to corrupt cache files ( #7453 ). Fix an issue where the Retry-After in HTTP responses was not respected and retries were handled inconsistently ( #7072 ). Fix an issue where Poetry silently ignored invalid groups ( #7529 ). Fix an issue where Poetry does not find a compatible Python version if not given explicitly ( #7771 ). Fix an issue where the direct_url.json of an editable install from a git dependency was invalid ( #7473 ). Fix an issue where error messages from build backends were not decoded correctly ( #7781 ). Fix an infinite loop when adding certain dependencies ( #7405 ). Fix an issue where pre-commit hooks skip pyproject.toml files in subdirectories ( #7239 ). Fix an issue where pre-commit hooks do not use the expected Python version ( #6989 ). Fix an issue where an unclear error message is printed if the project name is the same as one of its dependencies ( #7757 ). Fix an issue where poetry install returns a zero exit status even though the build script failed ( #7812 ). Fix an issue where an existing .venv was not used if in-project was not set ( #7792 ). Fix an issue where multiple extras passed to poetry add were not parsed correctly ( #7836 ). Fix an issue where poetry shell did not send a newline to fish ( #7884 ). Fix an issue where poetry update --lock printed operations that were not executed ( #7915 ). Fix an issue where poetry add --lock did perform a full update of all dependencies ( #7920 ). Fix an issue where poetry shell did not work with nushell ( #7919 ). Fix an issue where subprocess calls failed on Python 3.7 ( #7932 ). Fix an issue where keyring was called even though the password was stored in an environment variable ( #7928 ). Docs # Add information about what to use instead of --dev ( #7647 ). Promote semantic versioning less aggressively ( #7517 ). Explain Poetry‚Äôs own versioning scheme in the FAQ ( #7517 ). Update documentation for configuration with environment variables ( #6711 ). Add details how to disable the virtualenv prompt ( #7874 ). Improve documentation on whether to commit poetry.lock ( #7506 ). Improve documentation of virtualenv.create ( #7608 ). poetry-core ( 1.6.0 ) # Improve error message for invalid markers ( #569 ). Increase robustness when deleting temporary directories on Windows ( #460 ). Replace tomlkit with tomli , which changes the interface of some internal classes ( #483 ). Deprecate Package.category ( #561 ). Fix a performance regression in marker handling ( #568 ). Fix an issue where wildcard version constraints were not handled correctly ( #402 ). Fix an issue where poetry build created duplicate Python classifiers if they were specified manually ( #578 ). Fix an issue where local versions where not handled correctly ( #579 ). 1.4.2 - 2023-04-02 # Changed # When trying to install wheels with invalid RECORD files, Poetry does not fail anymore but only prints a warning.\nThis mitigates an unintended change introduced in Poetry 1.4.1 ( #7694 ). Fixed # Fix an issue where relative git submodule urls were not parsed correctly ( #7017 ). Fix an issue where Poetry could freeze when building a project with a build script if it generated enough output to fill the OS pipe buffer ( #7699 ). 1.4.1 - 2023-03-19 # Fixed # Fix an issue where poetry install did not respect the requirements for building editable dependencies ( #7579 ). Fix an issue where poetry init crashed due to bad input when adding packages interactively ( #7569 ). Fix an issue where poetry install ignored the subdirectory argument of git dependencies ( #7580 ). Fix an issue where installing packages with no-binary could result in a false hash mismatch ( #7594 ). Fix an issue where the hash of sdists was neither validated nor written to the direct_url.json during installation ( #7594 ). Fix an issue where poetry install --sync attempted to remove itself ( #7626 ). Fix an issue where wheels with non-normalized dist-info directory names could not be installed ( #7671 ). Fix an issue where poetry install --compile compiled with optimization level 1 ( #7666 ). Docs # Clarify the behavior of the --extras option ( #7563 ). Expand the FAQ on reasons for slow dependency resolution ( #7620 ). poetry-core ( 1.5.2 ) # Fix an issue where wheels built on Windows could contain duplicate entries in the RECORD file ( #555 ). 1.4.0 - 2023-02-27 # Added # Add a modern installer ( installer.modern-installation ) for faster installation of packages and independence from pip ( #6205 ). Add support for Private :: trove classifiers ( #7271 ). Add the version of poetry in the @generated comment at the beginning of the lock file ( #7339 ). Add support for virtualenvs.prefer-active-python when running poetry new and poetry init ( #7100 ). Changed # Deprecate the old installer, i.e. setting experimental.new-installer to false ( #7358 ). Remove unused platform field from cached package info and bump the cache version ( #7304 ). Extra dependencies of the root project are now sorted in the lock file ( #7375 ). Remove upper boundary for importlib-metadata dependency ( #7434 ). Validate path dependencies during use instead of during construction ( #6844 ). Remove the deprecated repository modules ( #7468 ). Fixed # Fix an issue where an unconditional dependency of an extra was not installed in specific environments ( #7175 ). Fix an issue where a pre-release of a dependency was chosen even if a stable release fulfilled the constraint ( #7225 , #7236 ). Fix an issue where HTTP redirects were not handled correctly during publishing ( #7160 ). Fix an issue where poetry check did not handle the -C, --directory option correctly ( #7241 ). Fix an issue where the subdirectory information of a git dependency was not written to the lock file ( #7367 ). Fix an issue where the wrong Python version was selected when creating an virtual environment ( #7221 ). Fix an issue where packages that should be kept were uninstalled when calling poetry install --sync ( #7389 ). Fix an issue where an incorrect value was set for sys.argv[0] when running installed scripts ( #6737 ). Fix an issue where hashes in direct_url.json files were not written according to the specification ( #7475 ). Fix an issue where poetry commands failed due to special characters in the path of the project or virtual environment ( #7471 ). Fix an issue where poetry crashed with a JSONDecodeError when running a Python script that produced certain warnings ( #6665 ). Docs # Add advice on how to maintain a poetry plugin ( #6977 ). Update tox examples to comply with the latest tox release ( #7341 ). Mention that the poetry export can export constraints.txt files ( #7383 ). Add clarifications for moving configuration files ( #6864 ). Mention the different types of exact version specifications ( #7503 ). poetry-core ( 1.5.1 ) # Improve marker handling ( #528 , #534 , #530 , #546 , #547 ). Validate whether dependencies referenced in extras are defined in the main dependency group ( #542 ). Poetry no longer generates a setup.py file in sdists by default ( #318 ). Fix an issue where trailing newlines were allowed in tool.poetry.description ( #505 ). Fix an issue where the name of the data folder in wheels was not normalized ( #532 ). Fix an issue where the order of entries in the RECORD file was not deterministic ( #545 ). Fix an issue where zero padding was not correctly handled in version comparisons ( #540 ). Fix an issue where sdist builds did not support multiple READMEs ( #486 ). poetry-plugin-export ( ^1.3.0 ) # Fix an issue where the export failed if there was a circular dependency on the root package ( #118 ). 1.3.2 - 2023-01-10 # Fixed # Fix a performance regression when locking dependencies from PyPI ( #7232 ). Fix an issue where passing a relative path via -C, --directory fails ( #7266 ). Docs # Update docs to reflect the removal of the deprecated get-poetry.py installer from the repository ( #7288 ). Add clarifications for virtualenvs.path settings ( #7286 ). 1.3.1 - 2022-12-12 # Fixed # Fix an issue where an explicit dependency on lockfile was missing, resulting in a broken Poetry in rare circumstances ( 7169 ). 1.3.0 - 2022-12-09 # Added # Mark the lock file with an @generated comment as used by common tooling ( #2773 ). poetry check validates trove classifiers and warns for deprecations ( #2881 ). Introduce a top level -C, --directory option to set the working path ( #6810 ). Changed # New lock file format (version 2.0) ( #6393 ). Path dependency metadata is unconditionally re-locked ( #6843 ). URL dependency hashes are locked ( #7121 ). poetry update and poetry lock should now resolve dependencies more similarly ( #6477 ). poetry publish will report more useful errors when a file does not exist ( #4417 ). poetry add will check for duplicate entries using canonical names ( #6832 ). Wheels are preferred to source distributions when gathering metadata ( #6547 ). Git dependencies of extras are only fetched if the extra is requested ( #6615 ). Invoke pip with --no-input to prevent hanging without feedback ( #6724 , #6966 ). Invoke pip with --isolated to prevent the influence of user configuration ( #6531 ). Interrogate environments with Python in isolated ( -I ) mode ( #6628 ). Raise an informative error when multiple version constraints overlap and are incompatible ( #7098 ). Fixed # Fix an issue where concurrent instances of Poetry would corrupt the artifact cache ( #6186 ). Fix an issue where Poetry can hang after being interrupted due to stale locking in cache ( #6471 ). Fix an issue where the output of commands executed with --dry-run contained duplicate entries ( #4660 ). Fix an issue where requests ‚Äôs pool size did not match the number of installer workers ( #6805 ). Fix an issue where poetry show --outdated failed with a runtime error related to direct origin dependencies ( #6016 ). Fix an issue where only the last command of an ApplicationPlugin is registered ( #6304 ). Fix an issue where git dependencies were fetched unnecessarily when running poetry lock --no-update ( #6131 ). Fix an issue where stdout was polluted with messages that should go to stderr ( #6429 ). Fix an issue with poetry shell activation and zsh ( #5795 ). Fix an issue where a url dependencies were shown as outdated ( #6396 ). Fix an issue where the source field of a dependency with extras was ignored ( #6472 ). Fix an issue where a package from the wrong source was installed for a multiple-constraints dependency with different sources ( #6747 ). Fix an issue where dependencies from different sources where merged during dependency resolution ( #6679 ). Fix an issue where experimental.system-git-client could not be used via environment variable ( #6783 ). Fix an issue where Poetry fails with an AssertionError due to distribution.files being None ( #6788 ). Fix an issue where poetry env info did not respect virtualenvs.prefer-active-python ( #6986 ). Fix an issue where poetry env list does not list the in-project environment ( #6979 ). Fix an issue where poetry env remove removed the wrong environment ( #6195 ). Fix an issue where the return code of a script was not relayed as exit code ( #6824 ). Fix an issue where the solver could silently swallow ValueError ( #6790 ). Docs # Improve documentation of package sources ( #5605 ). Correct the default cache path on Windows ( #7012 ). poetry-core ( 1.4.0 ) # The PEP 517 metadata_directory is now respected as an input to the build_wheel hook ( #487 ). ParseConstraintError is now raised on version and constraint parsing errors, and includes information on the package that caused the error ( #514 ). Fix an issue where invalid PEP 508 requirements were generated due to a missing space before semicolons ( #510 ). Fix an issue where relative paths were encoded into package requirements, instead of a file:// URL as required by PEP 508 ( #512 ). poetry-plugin-export ( ^1.2.0 ) # Ensure compatibility with Poetry 1.3.0. No functional changes. cleo ( ^2.0.0 ) # Fix an issue where shell completions had syntax errors ( #247 ). Fix an issue where not reading all the output of a command resulted in a ‚ÄúBroken pipe‚Äù error ( #165 ). Fix an issue where errors were not shown in non-verbose mode ( #166 ). 1.2.2 - 2022-10-10 # Added # Add forward compatibility for lock file format 2.0, which will be used by Poetry 1.3 ( #6608 ). Changed # Allow poetry lock to re-generate the lock file when invalid or incompatible ( #6753 ). Fixed # Fix an issue where the deprecated JSON API was used to query PyPI for available versions of a package ( #6081 ). Fix an issue where versions were escaped wrongly when building the wheel name ( #6476 ). Fix an issue where the installation of dependencies failed if pip is a dependency and is updated in parallel to other dependencies ( #6582 ). Fix an issue where the names of extras were not normalized according to PEP 685 ( #6541 ). Fix an issue where sdist names were not normalized ( #6621 ). Fix an issue where invalid constraints, which are ignored, were only reported in a debug message instead of a warning ( #6730 ). Fix an issue where poetry shell was broken in git bash on Windows ( #6560 ). Docs # Rework the README and contribution docs ( #6552 ). Fix for inconsistent docs for multiple-constraint dependencies ( #6604 ). Rephrase plugin configuration ( #6557 ). Add a note about publishable repositories to publish ( #6641 ). Fix the path for lazy-loaded bash completion ( #6656 ). Fix a reference to the invalid option --require ( #6672 ). Add a PowerShell one-liner to the basic usage section ( #6683 ). Fix the minimum poetry version in the example for plugins ( #6739 ). poetry-core ( 1.3.2 ) # Add 3.11 to the list of available Python versions ( #477 ). Fix an issue where caret constraints of pre-releases with a major version of 0 resulted in an empty version range ( #475 ). poetry-plugin-export ( ^1.1.2 ) # Add support for exporting constraints.txt files ( #128 ). Fix an issue where a relative path passed via -o was not interpreted relative to the current working directory ( #130 ). 1.2.1 - 2022-09-16 # Changed # Bump poetry-core to 1.2.0 . Bump poetry-plugin-export to ^1.0.7 . Fixed # Fix an issue where poetry cache clear did not respect the -n/--no-interaction flag ( #6338 ). Fix an issue where poetry lock --no-update updated dependencies from non-PyPI package sources ( #6335 ). Fix a poetry install performance regression by falling back to internal pip ( #6062 ). Fix an issue where a virtual environment was created unnecessarily when running poetry export ( #6282 ). Fix an issue where poetry lock --no-update added duplicate hashes to the lock file ( #6389 ). Fix an issue where poetry install fails because of missing hashes for url dependencies ( #6389 ). Fix an issue where Poetry was not able to update pip in Windows virtual environments ( #6430 ). Fix an issue where Poetry was not able to install releases that contained less common link types ( #5767 ). Fix a poetry lock performance regression when checking non-PyPI sources for yanked versions ( #6442 ). Fix an issue where --no-cache was not respected when running poetry install ( #6479 ). Fix an issue where deprecation warnings for --dev were missing ( #6475 ). Fix an issue where Git dependencies failed to clone when insteadOf was used in .gitconfig using the Dulwich Git client ( #6506 ). Fix an issue where no cache entry is found when calling poetry cache clear with a non-normalized package name ( #6537 ). Fix an invalid virtualenv constraint on Poetry ( #6402 ). Fix outdated build system requirements for Poetry ( #6509 ). Docs # Add missing path segment to paths used by install.python-poetry.org ( #6311 ). Add recommendations about how to install Poetry in a CI environment ( #6345 ). Fix examples for --with and --without ( #6318 ). Update configuration folder path for macOS ( #6395 ). Improve the description of the virtualenv.create option ( #6460 ). Clarify that poetry install removes dependencies of non-installed extras ( #6229 ). Add a note about pre-commit autoupdate and Poetry‚Äôs hooks ( #6497 ). 1.2.0 - 2022-08-31 # For a detailed breakdown of user-facing changes, refer to the 1.2.0 release announcement . Docs # Added note about how to add a git dependency with a subdirectory ( #6218 ) Fixed several style issues in the docs ( #6254 ) Fixed outdated info about --only parameter ( #6263 ) 1.2.0rc2 - 2022-08-26 # Fixed # Fixed an issue where virtual environments were created unnecessarily when running poetry self commands ( #6225 ) Ensure that packages' pretty_name are written to the lock file ( #6237 ) Improvements # Improved the consistency of Pool().remove_repository() to make it easier to write poetry plugins ( #6214 ) Docs # Removed mentions of Python 2.7 from docs ( #6234 ) Added note about the difference between groups and extras ( #6230 ) 1.2.0rc1 - 2022-08-22 # Added # Added support for subdirectories in git dependencies ( #5172 ) Added support for yanked releases and files (PEP-592) ( #5841 ) Virtual environments can now be created even with empty project names ( #5856 ) Added support for nushell in poetry shell ( #6063 ) Changed # Poetry now falls back to gather metadata for dependencies via pep517 if parsing pyproject.toml fails ( #5834 ) Replaced Poetry‚Äôs helper method canonicalize_name() with packaging.utils.canonicalize_name() ( #6022 ) Removed code for the export command, which is now provided via plugin ( #6128 ) Extras and extras dependencies are now sorted in the lock file ( #6169 ) Removed deprecated (1.2-only) CLI options ( #6210 ) Fixed # Fixed an issue where symlinks in the lock file were not resolved ( #5850 ) Fixed a tomlkit regression resulting in inconsistent line endings ( #5870 ) Fixed an issue where the POETRY_PYPI_TOKEN_PYPI environment variable wasn‚Äôt respected ( #5911 ) Fixed an issue where neither Python nor a managed venv can be found, when using Python from MS Store ( #5931 ) Improved error message of poetry publish in the event of an upload error ( #6043 ) Fixed an issue where poetry lock fails without output ( #6058 ) Fixed an issue where Windows drive mappings break virtual environment names ( #6110 ) tomlkit versions with memory leak are now avoided ( #6160 ) Fixed an infinite loop in the solver ( #6178 ) Fixed an issue where latest version was used instead of locked one for vcs dependencies with extras ( #6185 ) Docs # Document use of the subdirectory parameter ( #5949 ) Document suggested tox config for different use cases ( #6026 ) 1.1.15 - 2022-08-22 # Changed # Poetry now fallback to gather metadata for dependencies via pep517 if parsing pyproject.toml fail ( #6206 ) Extras and extras dependencies are now sorted in lock file ( #6207 ) 1.2.0b3 - 2022-07-13 # Important : This release fixes a critical issue that prevented hashes from being retrieved when locking dependencies,\ndue to a breaking change on PyPI JSON API (see #5972 and the upstream change for more details). After upgrading, you have to clear Poetry cache manually to get that feature working correctly again: $ poetry cache clear pypi --all Added # Added --only-root to poetry install to install a project without its\ndependencies ( #5783 ) Changed # Improved user experience of poetry init ( #5838 ) Added default timeout for all HTTP requests, to avoid hanging\nrequests ( #5881 ) Updated poetry init to better specify how to skip adding\ndependencies ( #5946 ) Updated Poetry repository names to avoid clashes with user-defined\nrepositories ( #5910 ) Fixed # Fixed an issue where extras where not handled if they did not match the case-sensitive name of the\npackages ( #4122 ) Fixed configuration of experimental.system-git-client option\nthrough poetry config ( #5818 ) Fixed uninstallation of git dependencies on Windows ( #5836 ) Fixed an issue where ~ was not correctly expanded\nin virtualenvs.path ( #5848 ) Fixed an issue where installing/locking dependencies would hang when setting an incorrect git\nrepository ( #5880 ) Fixed an issue in poetry publish when keyring was not properly\nconfigured ( #5889 ) Fixed duplicated line output in console ( #5890 ) Fixed an issue where the same wheels where downloaded multiple times during\ninstallation ( #5871 ) Fixed an issue where dependencies hashes could not be retrieved when locking due to a breaking change on PyPI JSON\nAPI ( #5973 ) Fixed an issue where a dependency with non-requested extras could not be installed if it is requested with extras by\nanother dependency ( #5770 ) Updated git backend to correctly read local/global git config when using dulwich as a git\nbackend ( #5935 ) Fixed an issue where optional dependencies where not correctly exported when defining\ngroups ( #5819 ) Docs # Fixed configuration instructions for repositories\nspecification ( #5809 ) Added a link to dependency specification\nfrom pyproject.toml ( #5815 ) Improved zsh autocompletion instructions ( #5859 ) Improved installation and update documentations ( #5857 ) Improved exact requirements documentation ( #5874 ) Added documentation for @ operator ( #5822 ) Improved autocompletion documentation ( #5879 ) Improved scripts definition documentation ( #5884 ) 1.1.14 - 2022-07-08 # Fixed # Fixed an issue where dependencies hashes could not be retrieved when locking due to a breaking change on PyPI JSON API ( #5973 ) 1.2.0b2 - 2022-06-07 # Added # Added support for multiple-constraint direct origin dependencies with the same\nversion ( #5715 ) Added support disabling TLS verification for custom package sources via poetry config certificates.<repository>.cert false ( #5719 Added new configuration ( virtualenvs.prompt ) to customize the prompt of the Poetry-managed virtual environment ( #5606 ) Added progress indicator to download_file (used when downloading dists) ( #5451 ) Added --dry-run to poetry version command ( #5603 ) Added --why to poetry show ( #5444 ) Added support for single page (html) repositories ( #5517 ) Added support for PEP 508 strings when adding\ndependencies via poetry add command ( #5554 ) Added --no-cache as a global option ( #5519 ) Added cert retrieval for HTTP requests made by Poetry ( #5320 ) Added --skip-existing to poetry publish ( #2812 ) Added --all-extras to poetry install ( #5452 ) Added new poetry self sub-commands to manage plugins and/or system environment packages, eg: keyring backends ( #5450 ) Added new configuration ( installer.no-binary ) to allow selection of non-binary distributions when installing a dependency ( #5609 ) Changed # poetry plugin commands are now deprecated in favor of the more generic poetry self commands ( #5450 ) When creating new projects, Poetry no longer restricts README extensions to md and rst ( #5357 ) Changed the provider to allow fallback to installed packages ( #5704 ) Solver now correctly handles and prefers direct reference constraints (vcs, file etc.) over public version identifiers ( #5654 ) Changed the build script behavior to create an ephemeral build environment when a build script is\nspecified ( #5401 ) Improved performance when determining PEP 517 metadata from sources ( #5601 ) Project package sources no longer need to be redefined as global repositories when configuring credentials ( #5563 ) Replaced external git command use with dulwich, in order to force the legacy behaviour set experimental.system-git-client configuration to true ( #5428 ) Improved http request handling for sources and multiple paths on same netloc ( #5518 ) Made no-pip and no-setuptools configuration explicit ( #5455 ) Improved application logging, use of -vv now provides more debug information ( #5503 ) Renamed implicit group default to main ( #5465 ) Replaced in-tree implementation of poetry export with poetry-plugin-export ( #5413 ) Changed the password manager behavior to use a \"null\" keyring when\ndisabled ( #5251 ) Incremental improvement of Solver performance ( #5335 ) Newly created virtual environments on macOS now are excluded from Time Machine backups ( #4599 ) Poetry no longer raises an exception when a package is not found on PyPI ( #5698 ) Update packaging dependency to use major version 21, this change forces Poetry to drop support for managing Python 2.7 environments ( #4749 ) Fixed # Fixed poetry update --dry-run to not modify poetry.lock ( #5718 , #3666 , #3766 ) Fixed #5537 where export fails to resolve dependencies with more than one\npath ( #5688 ) Fixed an issue where the environment variables POETRY_CONFIG_DIR and POETRY_CACHE_DIR were not being respected ( #5672 ) Fixed #3628 and #4702 by handling invalid distributions\ngracefully ( #5645 ) Fixed an issue where the provider ignored subdirectory when merging and improve subdirectory support for vcs\ndeps ( #5648 ) Fixed an issue where users could not select an empty choice when selecting\ndependencies ( #4606 ) Fixed an issue where poetry init -n crashes in a root directory ( #5612 ) Fixed an issue where Solver errors arise due to wheels having different Python\nconstraints ( #5616 ) Fixed an issue where editable path dependencies using setuptools could not be correctly installed ( #5590 ) Fixed flicker when displaying executor operations ( #5556 ) Fixed an issue where the poetry lock --no-update only sorted by name and not by name and\nversion ( #5446 ) Fixed an issue where the Solver fails when a dependency has multiple constrained dependency definitions for the same\npackage ( #5403 ) Fixed an issue where dependency resolution takes a while because Poetry checks all possible combinations\neven markers are mutually exclusive ( #4695 ) Fixed incorrect version selector constraint ( #5500 ) Fixed an issue where poetry lock --no-update dropped\npackages ( #5435 ) Fixed an issue where packages were incorrectly grouped when\nexporting ( #5156 ) Fixed an issue where lockfile always updates when using private\nsources ( #5362 ) Fixed an issue where the solver did not account for selected package features ( #5305 ) Fixed an issue with console script execution of editable dependencies on Windows ( #3339 ) Fixed an issue where editable builder did not write PEP-610 metadata ( #5703 ) Fixed an issue where Poetry 1.1 lock files were incorrectly identified as not fresh ( #5458 ) Docs # Updated plugin management commands ( #5450 ) Added the --readme flag to documentation ( #5357 ) Added example for multiple maintainers ( #5661 ) Updated documentation for issues #4800 , #3709 , #3573 , #2211 and #2414 ( #5656 ) Added poetry.toml note in configuration ( #5492 ) Add documentation for poetry about , poetry help , poetrylist , and the --full-path and --all options\ndocumentation ( #5664 ) Added more clarification to the --why flag ( #5653 ) Updated documentation to refer to PowerShell for Windows, including\ninstructions ( #3978 , #5618 ) Added PEP 508 name requirement ( #5642 ) Added example for each section of pyproject.toml ( #5585 ) Added documentation for --local to fix issue #5623 ( #5629 ) Added troubleshooting documentation for using proper quotation with\nZSH ( #4847 ) Added information on git and basic http auth ( #5578 ) Removed ambiguity about PEP 440 and semver ( #5576 ) Removed Pipenv comparison ( #5561 ) Improved dependency group related documentation ( #5338 ) Added documentation for default directories used by Poetry ( #5391 ) Added warning about credentials preserved in shell history ( #5726 ) Improved documentation of the readme option, including multiple files and additional formats ( #5158 ) Improved contributing documentation ( #5708 ) Remove all references to --dev-only option ( #5771 ) 1.2.0b1 - 2022-03-17 # Fixed # Fixed an issue where the system environment couldn‚Äôt be detected ( #4406 ). Fixed another issue where the system environment couldn‚Äôt be detected ( #4433 ). Replace deprecated requests parameter in uploader ( #4580 ). Fix an issue where venv are detected as broken when using MSys2 on windows ( #4482 ). Fixed an issue where the cache breaks on windows ( #4531 ). Fixed an issue where a whitespace before a semicolon was missing on poetry export ( #4575 ). Fixed an issue where markers were not correctly assigned to nested dependencies ( #3511 ). Recognize one digit version in wheel filenames ( #3338 ). Fixed an issue when locale is unset ( #4038 ). Fixed an issue where the fallback to another interpreter didn‚Äôt work ( #3475 ). Merge any marker constraints into constraints with specific markers ( #4590 ). Normalize path before hashing so that the generated venv name is independent of case on Windows ( #4813 ). Fixed an issue where a dependency wasn‚Äôt upgrade by using @latest on poetry update ( #4945 ). Fixed an issue where conda envs in windows are always reported as broken( #5007 ). Fixed an issue where Poetry doesn‚Äôt find its own venv on poetry self update ( #5049 ). Fix misuse of pretty_constraint ( #4932 ). Fixed an issue where the reported python version used for venv creation wasn‚Äôt correct ( #5086 ). Fixed an issue where the searched package wasn‚Äôt display in the interactive dialog of poetry init ( #5076 ). Fixed an issue where Poetry raises an exception on poetry show when no lock files exists ( #5242 ). Fixed an issue where Poetry crashes when optional vcs_info.requested_version in direct_url.json wasn‚Äôt included ( #5274 ). Fixed an issue where dependencies with extras were updated despite using --no-update ( #4618 ). Fixed various places where poetry writes messages to stdout instead of stderr ( #4110 , #5179 ). Ensured that when complete packages are created dependency inherits source and resolved refs from package ( #4604 ). Ensured that when complete packages are created dependency inherits subdirectory from package if supported ( #4604 ). Fixed an issue where POETRY_EXPERIMENTAL_NEW_INSTALLER needs to be set to an empty string to disable it ( #3811 ). Added # poetry show <package> now also shows which packages depend on it ( #2351 ). The info dialog by poetry about now contains version information about installed poetry and poetry-core ( #5288 ). Print error message when poetry publish fails ( #3549 ). Added in info output to poetry lock --check ( #5081 ). Added new argument --all for poetry env remove to delete all venv of a project at once ( #3212 ). Added new argument --without-urls for poetry export to exclude source repository urls from the exported file ( #4763 ). Added a new installer.max-workers property to the configuration ( #3516 ). Added experimental option virtualenvs.prefer-active-python to detect current activated python ( #4852 ). Added better windows shell support ( #5053 ). Changed # Drop python3.6 support ( #5055 ). Exit with callable return code in generated script ( #4456 ). Internal use of the pep517 high level interfaces for package metadata inspections have been replaced with the build package. ( #5155 ). Poetry now raises an error if the python version in the project environment is no longer compatible with the project ( #4520 ). 1.1.13 - 2022-02-10 # Fixed # Fixed an issue where envs in MSYS2 always reported as broken ( #4942 ) Fixed an issue where conda envs in windows are always reported as broken( #5008 ) Fixed an issue where Poetry doesn‚Äôt find its own venv on poetry self update ( #5048 ) 1.1.12 - 2021-11-27 # Fixed # Fixed broken caches on Windows due to Path starting with a slash ( #4549 ) Fixed JSONDecodeError when installing packages by updating cachecontrol version ( #4831 ) Fixed dropped markers in dependency walk ( #4686 ) 1.1.11 - 2021-10-04 # Fixed # Fixed errors when installing packages on Python 3.10. ( #4592 ) Fixed an issue where the wrong git executable could be used on Windows. ( python-poetry/poetry-core#213 ) Fixed an issue where the Python 3.10 classifier was not automatically added. ( python-poetry/poetry-core#215 ) 1.1.10 - 2021-09-21 # Fixed # Fixed an issue where non-sha256 hashes were not checked. ( #4529 ) 1.1.9 - 2021-09-18 # Fixed # Fixed a security issue where file hashes were not checked prior to installation. ( #4420 , #4444 , python-poetry/poetry-core#193 ) Fixed the detection of the system environment when the setting virtualenvs.create is deactivated. ( #4507 ) Fixed an issue where unsafe parameters could be passed to git commands. ( python-poetry/poetry-core#203 ) Fixed an issue where the wrong git executable could be used on Windows. ( python-poetry/poetry-core#205 ) 1.1.8 - 2021-08-19 # Fixed # Fixed an error with repository prioritization when specifying secondary repositories. ( #4241 ) Fixed the detection of the system environment when the setting virtualenvs.create is deactivated. ( #4330 , #4407 ) Fixed the evaluation of relative path dependencies. ( #4246 ) Fixed environment detection for Python 3.10 environments. ( #4387 ) Fixed an error in the evaluation of in/not in markers ( python-poetry/poetry-core#189 ) 1.2.0a2 - 2021-08-01 # Added # Poetry now supports dependency groups. ( #4260 ) The install command now supports a --sync option to synchronize the environment with the lock file. ( #4336 ) Changed # Improved the way credentials are retrieved to better support keyring backends. ( #4086 ) The --remove-untracked option of the install command is now deprecated in favor of the new --sync option. ( #4336 ) The user experience when installing dependency groups has been improved. ( #4336 ) Fixed # Fixed performance issues when resolving dependencies. ( #3839 ) Fixed an issue where transitive dependencies of directory or VCS dependencies were not installed or otherwise removed. ( #4202 ) Fixed the behavior of the init command in non-interactive mode. ( #2899 ) Fixed the detection of the system environment when the setting virtualenvs.create is deactivated. ( #4329 ) Fixed the display of possible solutions for some common errors. ( #4332 ) 1.1.7 - 2021-06-25 # Note : Lock files might need to be regenerated for the first fix below to take effect.\nYou can use poetry lock to do so without the --no-update option. Changed # This release is compatible with the install-poetry.py installation script to ease the migration path from 1.1 releases to 1.2 releases. ( #4192 ) Fixed # Fixed an issue where transitive dependencies of directory or VCS dependencies were not installed or otherwise removed. ( #4203 ) Fixed an issue where the combination of the --tree and --no-dev options for the show command was still displaying development dependencies. ( #3992 ) 1.2.0a1 - 2021-05-21 # This release is the first testing release of the upcoming 1.2.0 version. It drops support for Python 2.7 and 3.5. Added # Poetry now supports a plugin system to alter or expand Poetry‚Äôs functionality. ( #3733 ) Poetry now supports PEP 610 . ( #3876 ) Several configuration options to better control the way virtual environments are created are now available. ( #3157 , #3711 ). The new command now supports namespace packages. ( #2768 ) The add command now supports the --editable option to add packages in editable mode. ( #3940 ) Changed # Python 2.7 and 3.5 are no longer supported. ( #3405 ) The usage of the get-poetry.py script is now deprecated and is replaced by the install-poetry.py script. ( #3706 ) Directory dependencies are now in non-develop mode by default. ( poetry-core#98 ) Improved support for PEP 440 specific versions that do not abide by semantic versioning. ( poetry-core#140 ) Improved the CLI experience and performance by migrating to the latest version of Cleo. ( #3618 ) Packages previously considered as unsafe ( pip , setuptools , wheels and distribute ) can now be managed as any other package. ( #2826 ) The new command now defaults to the Markdown format for README files. ( #2768 ) Fixed # Fixed an error where command line options were not taken into account when using the run command. ( #3618 ) Fixed an error in the way custom repositories were resolved. ( #3406 ) 1.1.6 - 2021-04-14 # Fixed # Fixed export format for path dependencies. ( #3121 ) Fixed errors caused by environment modification when executing some commands. ( #3253 ) Fixed handling of wheel files with single-digit versions. ( #3338 ) Fixed an error when handling single-digit Python markers. ( poetry-core#156 ) Fixed dependency markers not being properly copied when changing the constraint leading to resolution errors. ( poetry-core#163 ) Fixed an error where VCS dependencies were always updated. ( #3947 ) Fixed an error where the incorrect version of a package was locked when using environment markers. ( #3945 ) 1.1.5 - 2021-03-04 # Fixed # Fixed an error in the export command when no lock file existed and a verbose flag was passed to the command. ( #3310 ) Fixed an error where the pyproject.toml was not reverted when using the add command. ( #3622 ) Fixed errors when using non-HTTPS indices. ( #3622 ) Fixed errors when handling simple indices redirection. ( #3622 ) Fixed errors when trying to handle newer wheels by using the latest version of poetry-core and packaging . ( #3677 ) Fixed an error when using some versions of poetry-core due to an incorrect import . ( #3696 ) 1.1.4 - 2020-10-23 # Added # Added installer.parallel boolean flag (defaults to true ) configuration to enable/disable parallel execution of operations when using the new installer. ( #3088 ) Changed # When using system environments as an unprivileged user, user site and bin directories are created if they do not already exist. ( #3107 ) Fixed # Fixed editable installation of poetry projects when using system environments. ( #3107 ) Fixed locking of nested extra activations. If you were affected by this issue, you will need to regenerate the lock file using poetry lock --no-update . ( #3229 ) Fixed prioritisation of non-default custom package sources. ( #3251 ) Fixed detection of installed editable packages when non-poetry managed .pth file exists. ( #3210 ) Fixed scripts generated by editable builder to use valid import statements. ( #3214 ) Fixed recursion error when locked dependencies contain cyclic dependencies. ( #3237 ) Fixed propagation of editable flag for VCS dependencies. ( #3264 ) 1.1.3 - 2020-10-14 # Changed # Python version support deprecation warning is now written to stderr . ( #3131 ) Fixed # Fixed KeyError when PATH is not defined in environment variables. ( #3159 ) Fixed error when using config command in a directory with an existing pyproject.toml without any Poetry configuration. ( #3172 ) Fixed incorrect inspection of package requirements when same dependency is specified multiple times with unique markers. ( #3147 ) Fixed show command to use already resolved package metadata. ( #3117 ) Fixed multiple issues with export command output when using requirements.txt format. ( #3119 ) 1.1.2 - 2020-10-06 # Changed # Dependency installation of editable packages and all uninstall operations are now performed serially within their corresponding priority groups. ( #3099 ) Improved package metadata inspection of nested poetry projects within project path dependencies. ( #3105 ) Fixed # Fixed export of requirements.txt when project dependency contains git dependencies. ( #3100 ) 1.1.1 - 2020-10-05 # Added # Added --no-update option to lock command. ( #3034 ) Fixed # Fixed resolution of packages with missing required extras. ( #3035 ) Fixed export of requirements.txt dependencies to include development dependencies. ( #3024 ) Fixed incorrect selection of unsupported binary distribution formats when selecting a package artifact to install. ( #3058 ) Fixed incorrect use of system executable when building package distributions via build command. ( #3056 ) Fixed errors in init command when specifying --dependency in non-interactive mode when a pyproject.toml file already exists. ( #3076 ) Fixed incorrect selection of configured source url when a publish repository url configuration with the same name already exists. ( #3047 ) Fixed dependency resolution issues when the same package is specified in multiple dependency extras. ( #3046 ) 1.1.0 - 2020-10-01 # Changed # The init command will now use existing pyproject.toml if possible ( #2448 ). Error messages when metadata information retrieval fails have been improved ( #2997 ). Fixed # Fixed parsing of version constraint for rc prereleases ( #2978 ). Fixed how some metadata information are extracted from setup.cfg files ( #2957 ). Fixed return codes returned by the executor ( #2981 ). Fixed whitespaces not being accepted for the list of extras when adding packages ( #2985 ). Fixed repositories specified in the pyproject.toml file not being taken into account for authentication when downloading packages ( #2990 ). Fixed permission errors when installing the root project if the site-packages directory is not writeable ( #3002 ). Fixed environment marker propagation when exporting to the requirements.txt format ( #3002 ). Fixed errors when paths in run command contained spaces ( #3015 ). 1.1.0rc1 - 2020-09-25 # Changed # The virtualenvs.in-project setting will now always be honored, if set explicitly, regardless of the presence of a .venv directory ( #2771 ). Adding packages already present in the pyproject.toml file will no longer raise an error ( #2886 ). Errors when authenticating against custom repositories will now be logged ( #2577 ). Fixed # Fixed an error on Python 3.5 when resolving URL dependencies ( #2954 ). Fixed the dependency option of the init command being ignored ( #2587 ). Fixed the show command displaying erroneous information following the changes in the lock file format ( #2967 ). Fixed dependency resolution errors due to invalid python constraints propagation ( #2968 ). 1.1.0b4 - 2020-09-23 # Changed # When running under Python 2.7 on Windows, install command will be limited to one worker to mitigate threading issue ( #2941 ). 1.1.0b3 - 2020-09-18 # Changed # Improved the error reporting when HTTP error are encountered for legacy repositories ( #2459 ). When displaying the name of packages retrieved from remote repositories, the original name will now be used ( #2305 ). Failed package downloads will now be retried on connection errors ( #2813 ). Path dependencies will now be installed as editable only when develop option is set to true ( #2887 ). Fixed # Fixed the detection of the type of installed packages ( #2722 ). Fixed deadlocks when installing packages on systems not supporting non-ascii characters ( #2721 ). Fixed handling of wildcard constraints for packages with prereleases only ( #2821 ). Fixed dependencies of some packages not being discovered by ensuring we use the PEP-516 backend if specified ( #2810 ). Fixed recursion errors when retrieving extras ( #2787 ). Fixed PyPI always being displayed when publishing even for custom repositories ( #2905 ). Fixed handling of packages extras when resolving dependencies ( #2887 ). 1.1.0b2 - 2020-07-24 # Changed # Added support for build scripts without the setup.py file generation in the editable builder ( #2718 ). Fixed # Fixed an error occurring when using older lock files ( #2717 ). 1.1.0b1 - 2020-07-24 # Changed # Virtual environments will now exclusively be built with virtualenv ( #2666 ). Support for Python 2.7 and 3.5 is now officially deprecated and a warning message will be displayed ( #2683 ). Improved metadata inspection of packages by using the PEP-517 build system ( #2632 ). Fixed # Fixed parallel tasks not being cancelled when the installation is interrupted or has failed ( #2656 ). Fixed an error where the editable builder would not expose all packages ( #2664 ). Fixed an error for Python 2.7 when a file could not be downloaded in the installer ( #2709 ). Fixed the lock file content-hash value not being updated when using the add and remove commands ( #2710 ). Fixed incorrect resolution errors being raised for packages with python requirements ( #2712 ). Fixed an error causing the build log messages to no longer be displayed ( #2715 ). 1.0.10 - 2020-07-21 # Changed # The lock files are now versioned to ease transitions for lock file format changes, with warnings being displayed on incompatibility detection ( #2695 ). The init and new commands will now provide hints on invalid given licenses ( #1634 ). Fixed # Fixed error messages when the authors specified in the pyproject.toml file are invalid ( #2525 ). Fixed empty .venv directories being deleted ( #2064 ). Fixed the shell command for tcsh shells ( #2583 ). Fixed errors when installing directory or file dependencies in some cases ( #2582 ). 1.1.0a3 - 2020-07-10 # Added # New installer which provides a faster and better experience ( #2595 ). Fixed # Fixed resolution error when handling duplicate dependencies with environment markers ( #2622 ). Fixed erroneous resolution errors when resolving packages to install ( #2625 ). Fixed errors when detecting installed editable packages ( #2602 ). 1.1.0a2 - 2020-06-26 # Note that lock files generated with this release are not compatible with previous releases of Poetry. Added # The install command now supports a --remove-untracked option to ensure only packages from the lock file are present in the environment ( #2172 ). Some errors will now be provided with possible solutions and links to the documentation ( #2396 ). Changed # Editable installations of Poetry projects have been improved and are now faster ( #2360 ). Improved the accuracy of the dependency resolver in case of dependencies with environment markers ( #2361 ) Environment markers of dependencies are no longer stored in the lock file ( #2361 ). Improved the way connection errors are handled when publishing ( #2285 ). Fixed # Fixed errors when handling duplicate dependencies with environment markers ( #2342 ). Fixed the detection of installed packages ( #2360 ). 1.1.0a1 - 2020-03-27 # This release must be downloaded via the get-poetry.py script and not via the self update command. Added # Added a new --dry-run option to the publish command ( #2199 ). Changed # The core features of Poetry have been extracted in to a separate library: poetry-core ( #2212 ). The build backend is no longer poetry.masonry.api but poetry.core.masonry.api which requires poetry-core>=1.0.0a5 ( #2212 ). The exceptions are now beautifully displayed in the terminal with various level of details depending on the verbosity ( 2230 ). 1.0.9 - 2020-06-09 # Fixed # Fixed an issue where packages from custom indices where continuously updated ( #2525 ). Fixed errors in the way Python environment markers were parsed and generated ( #2526 ). 1.0.8 - 2020-06-05 # Fixed # Fixed a possible error when installing the root package ( #2505 ). Fixed an error where directory and VCS dependencies were not installed ( #2505 ). 1.0.7 - 2020-06-05 # Fixed # Fixed an error when trying to execute some packages setup.py file ( #2349 ). 1.0.6 - 2020-06-05 # Changed # The self update command has been updated in order to handle future releases of Poetry ( #2429 ). Fixed # Fixed an error were a new line was not written when displaying the virtual environment‚Äôs path with env info ( #2196 ). Fixed a misleading error message when the packages property was empty ( #2265 ). Fixed shell detection by using environment variables ( #2147 ). Fixed the removal of VCS dependencies ( #2239 ). Fixed generated wheel ABI tags for Python 3.8 ( #2121 ). Fixed a regression when building stub-only packages ( #2000 ). Fixed errors when parsing PEP-440 constraints with whitespace ( #2347 ). Fixed PEP 508 representation of VCS dependencies ( #2349 ). Fixed errors when source distributions were read-only ( #1140 ). Fixed dependency resolution errors and inconsistencies with directory, file and VCS dependencies ( #2398 ). Fixed custom repositories information not being properly locked ( #2484 ). 1.0.5 - 2020-02-29 # Fixed # Fixed an error when building distributions if the git executable was not found ( #2105 ). Fixed various errors when reading Poetry‚Äôs TOML files by upgrading tomlkit . 1.0.4 - 2020-02-28 # Fixed # Fixed the PyPI URL used when installing packages ( #2099 ). Fixed errors when the author‚Äôs name contains special characters ( #2006 ). Fixed VCS excluded files detection when building wheels ( #1947 ). Fixed packages detection when building sdists ( #1626 ). Fixed the local .venv virtual environment not being displayed in env list ( #1762 ). Fixed incompatibilities with the most recent versions of virtualenv ( #2096 ). Fixed Poetry‚Äôs own vendor dependencies being retrieved when updating dependencies ( #1981 ). Fixed encoding of credentials in URLs ( #1911 ). Fixed url constraints not being accepted in multi-constraints dependencies ( #2035 ). Fixed an error where credentials specified via environment variables were not retrieved ( #2061 ). Fixed an error where git dependencies referencing tags were not locked to the corresponding commit ( #1948 ). Fixed an error when parsing packages setup.py files ( #2041 ). Fixed an error when parsing some git URLs ( #2018 ). 1.0.3 - 2020-01-31 # Fixed # Fixed an error which caused the configuration environment variables (like POETRY_HTTP_BASIC_XXX_PASSWORD ) to not be used ( #1909 ). Fixed an error where the --help option was not working ( #1910 ). Fixed an error where packages from private indices were not decompressed properly ( #1851 ). Fixed an error where the version of some PEP-508-formatted wheel dependencies was not properly retrieved ( #1932 ). Fixed internal regexps to avoid potential catastrophic backtracking errors ( #1913 ). Fixed performance issues when custom indices were defined in the pyproject.toml file ( #1892 ). Fixed the get_requires_for_build_wheel() function of masonry.api which wasn‚Äôt returning the proper result ( #1875 ). 1.0.2 - 2020-01-10 # Fixed # Reverted a previous fix ( #1796 ) which was causing errors for projects with file and/or directory dependencies ( #1865 ). 1.0.1 - 2020-01-10 # Fixed # Fixed an error in env use where the wrong Python executable was being used to check compatibility ( #1736 ). Fixed an error where VCS dependencies were not properly categorized as development dependencies ( #1725 ). Fixed an error where some shells would no longer be usable after using the shell command ( #1673 ). Fixed an error where explicitly included files where not included in wheel distributions ( #1750 ). Fixed an error where some Git dependencies url were not properly parsed ( #1756 ). Fixed an error in the env commands on Windows if the path to the executable contained a space ( #1774 ). Fixed several errors and UX issues caused by keyring on some systems ( #1788 ). Fixed errors when trying to detect installed packages ( #1786 ). Fixed an error when packaging projects where Python packages were not properly detected ( #1592 ). Fixed an error where local file dependencies were exported as editable when using the export command ( #1840 ). Fixed the way environment markers are propagated and evaluated when resolving dependencies ( #1829 , #1789 ). Fixed an error in the PEP-508 compliant representation of directory and file dependencies ( #1796 ). Fixed an error where invalid virtual environments would be silently used. They will not be recreated and a warning will be displayed ( #1797 ). Fixed an error where dependencies were not properly detected when reading the setup.py file in some cases ( #1764 ). 1.0.0 - 2019-12-12 # Added # Added an export command to export the lock file to other formats (only requirements.txt is currently supported). Added a env info command to get basic information about the current environment. Added a env use command to control the Python version used by the project. Added a env list command to list the virtualenvs associated with the current project. Added a env remove command to delete virtualenvs associated with the current project. Added support for POETRY_HOME declaration within get-poetry.py . Added support for declaring a specific source for dependencies. Added support for disabling PyPI and making another repository the default one. Added support for declaring private repositories as secondary. Added the ability to specify packages on a per-format basis. Added support for custom urls in metadata. Full environment markers are now supported for dependencies via the markers property. Added the ability to specify git dependencies directly in add , it no longer requires the --git option. Added the ability to specify path dependencies directly in add , it no longer requires the --path option. Added support for url dependencies ( #1260 ). Publishing to PyPI using API tokens is now supported ( #1275 ). Licenses can now be identified by their full name. Added support for custom certificate authority and client certificates for private repositories. Poetry can now detect and use Conda environments. Changed # Slightly changed the lock file, making it potentially incompatible with previous Poetry versions. The cache:clear command has been renamed to cache clear . The debug:info command has been renamed to debug info . The debug:resolve command has been renamed to debug resolve . The self:update command has been renamed to self update . Changed the way virtualenvs are stored (names now depend on the project‚Äôs path). The --git option of the add command has been removed. The --path option of the add command has been removed. The add command will now automatically select the latest prerelease if only prereleases are available. The add command can now update a dependencies if an explicit constraint is given ( #1221 ). Removed the --develop option from the install command. Improved UX when searching for packages in the init command. The shell command has been improved. The poetry run command now uses os.execvp() rather than spawning a new subprocess. Specifying dependencies with allows-prereleases in the pyproject.toml file is deprecated for consistency with the add command. Use allow-prereleases instead. Improved the error message when the lock file is invalid. Whenever Poetry needs to use the ‚Äúsystem‚Äù Python, it will now call sys.executable instead of the python command. Improved the error message displayed on conflicting Python requirements ( #1681 ). Improved the site-packages directory detection ( #1683 ). Fixed # Fixed transitive extra dependencies being removed when updating a specific dependency. The pyproject.toml configuration is now properly validated. Fixed installing Poetry-based packages breaking with pip . Fixed packages with empty markers being added to the lock file. Fixed invalid lock file generation in some cases. Fixed local version identifier handling in wheel file names. Fixed packages with invalid metadata triggering an error instead of being skipped. Fixed the generation of invalid lock files in some cases. Git dependencies are now properly locked to a specific revision when specifying a branch or a tag. Fixed the behavior of the ~= operator. Fixed dependency resolution for conditional development dependencies. Fixed generated dependency constraints when they contain inequality operators. The run command now properly handles the -- separator. Fixed some issues with path dependencies being seen as git dependencies. Fixed various issues with the way extra markers in dependencies were handled. Fixed the option conflicts in the run command. Fixed wrong latest version being displayed when executing show -l . Fixed TooManyRedirects errors being raised when resolving dependencies. Fixed custom indices dependencies being constantly updated. Fixed the behavior of the --install option of the debug resolve command. Fixed an error in show when using the -o/--outdated option. Fixed PEP 508 url dependency handling. Fixed excluded files via the exclude being included in distributions. Fixed an error in env use if the virtualenvs.in-project setting is activated ( #1682 ) Fixed handling of empty and any markers in unions of markers ( #1650 ). 0.12.17 - 2019-07-03 # Fixed # Fixed dependency resolution with circular dependencies. Fixed encoding errors when reading files on Windows. (Thanks to @vlcinsky ) Fixed unclear errors when executing commands in virtual environments. (Thanks to @Imaclean74 ) Fixed handling of .venv when it‚Äôs not a directory. (Thanks to @mpanarin ) 0.12.16 - 2019-05-17 # Fixed # Fixed packages with no hashes retrieval for legacy repositories. Fixed multiple constraints for dev dependencies. Fixed dependency resolution failing on badly formed package versions instead of skipping. Fixed permissions of built wheels. 0.12.15 - 2019-05-03 # Fixed # Fixed an AttributeError in the editable builder. Fixed resolution of packages with only Python 3 wheels and sdist when resolving for legacy repositories. Fixed non-sha256 hashes retrieval for legacy repositories. 0.12.14 - 2019-04-26 # Fixed # Fixed root package installation for pure Python packages. 0.12.13 - 2019-04-26 # Fixed # Fixed root package installation with pip>=19.0 . Fixed packages not being removed after using the remove command. 0.12.12 - 2019-04-11 # Fixed # Fix lock idempotency. Fix markers evaluation for python_version with precision < 3. Fix permissions of the dist-info files. Fix prepare_metadata_for_build_wheel() missing in the build backend. Fix metadata inconsistency between wheels and sdists. Fix parsing of platform_release markers. Fix metadata information when the project has git dependencies. Fix error reporting when publishing fails. Fix retrieval of extras_require in some setup.py files. (Thanks to @asodeur ) Fix wheel compression when building. (Thanks to @ccosby ) Improve retrieval of information for packages with two python specific wheels. Fix request authentication when credentials are included in URLs. (Thanks to @connorbrinton ) 0.12.11 - 2019-01-13 # Fixed # Fixed the way packages information are retrieved for legacy repositories. Fixed an error when adding packages with invalid versions. Fixed an error when resolving directory dependencies with no sub dependencies. Fixed an error when locking packages with no description. Fixed path resolution for transitive file dependencies. Fixed multiple constraints handling for the root package. Fixed exclude functionality on case sensitive systems. 0.12.10 - 2018-11-22 # Fixed # Fixed run not executing scripts. Fixed environment detection. Fixed handling of authentication for legacy repositories. 0.12.9 - 2018-11-19 # Fixed # Fixed executables from outside the virtualenv not being accessible. Fixed a possible error when building distributions with the exclude option. Fixed the run command for namespaced packages. Fixed errors for virtualenvs with spaces in their path. Fixed prerelease versions being selected with the add command. 0.12.8 - 2018-11-13 # Fixed # Fixed permission errors when adding/removing git dependencies on Windows. Fixed Pool not raising an exception when no package could be found. Fixed reading bz2 source distribution. Fixed handling of arbitrary equals in InstalledRepository . 0.12.7 - 2018-11-08 # Fixed # Fixed reading of some setup.py files. Fixed a KeyError when getting information for packages which require reading setup files. Fixed the building of wheels with C extensions and an src layout. Fixed extras being selected when resolving dependencies even when not required. Fixed performance issues when packaging projects if a lot of files were excluded. Fixed installation of files. Fixed extras not being retrieved for legacy repositories. Fixed invalid transitive constraints raising an error for legacy repositories. 0.12.6 - 2018-11-05 # Changed # Poetry will now try to read, without executing, setup files ( setup.py and/or setup.cfg ) if the egg_info command fails when resolving dependencies. Fixed # Fixed installation of directory dependencies. Fixed handling of dependencies with a not in marker operator. Fixed support for VCS dependencies. Fixed the exclude property not being respected if no VCS was available. 0.12.5 - 2018-10-26 # Fixed # Fixed installation of Poetry git dependencies with a build system. Fixed possible errors when resolving dependencies for specific packages. Fixed handling of Python versions compatibility. Fixed the dependency resolver picking up unnecessary dependencies due to not using the python_full_version marker. Fixed the Python-Requires metadata being invalid for single Python versions. 0.12.4 - 2018-10-21 # Fixed # Fixed possible error on some combinations of markers. Fixed venv detection so that it only uses VIRTUAL_ENV to detect activated virtualenvs. 0.12.3 - 2018-10-18 # Fixed # Fixed the --no-dev option in install not working properly. Fixed prereleases being selected even if another constraint conflicted with them. Fixed an error when installing current package in development mode if the generated setup.py had special characters. Fixed an error in install for applications not following a known structure. Fixed an error when trying to retrieve the current environment. Fixed debug:info not showing the current project‚Äôs virtualenv. 0.12.2 - 2018-10-17 # Fixed # Fixed an error when installing from private repositories. Fixed an error when trying to move the lock file on Python 2.7. 0.12.1 - 2018-10-17 # Fixed # Fixed an error when license is unspecified. 0.12.0 - 2018-10-17 # Added # Added a brand new installer. Added support for multi-constraints dependencies. Added a cache version system. Added a --lock option to update to only update the lock file without executing operations. (Thanks to @greysteil ) Added support for the Project-URL metadata. Added support for optional scripts. Added a --no-dev option to show . (Thanks to @rodcloutier ) Changed # Improved virtualenv detection and management. Wildcard python dependencies are now equivalent to ~2.7 || ^3.4 . Changed behavior of the resolver for conditional dependencies. The install command will now install the current project in editable mode. The develop command is now deprecated in favor of install . Improved the check command. Empty passwords are now supported when publishing. Fixed # Fixed a memory leak in the resolver. Fixed a recursion error on duplicate dependencies with only different extras. Fixed handling of extras. Fixed duplicate entries in both sdist and wheel. Fixed excluded files appearing in the package_data of the generated setup.py . Fixed transitive directory dependencies installation. Fixed file permissions for configuration and authentication files. Fixed an error in cache:clear for Python 2.7. Fixed publishing for the first time with a prerelease. 0.11.5 - 2018-09-04 # Fixed # Fixed a recursion error with circular dependencies. Fixed the config command setting incorrect values for paths. Fixed an OSError on Python >= 3.5 for git dependencies with recursive symlinks. Fixed the possible deletion of system paths by cache:clear . Fixed a performance issue when parsing the lock file by upgrading tomlkit . 0.11.4 - 2018-07-30 # Fixed # Fixed wrong wheel being selected when resolving dependencies. Fixed an error when publishing. Fixed an error when building wheels with the packages property set. Fixed single value display in config command. 0.11.3 - 2018-07-26 # Changed # Poetry now only uses TOML Kit for TOML files manipulation. Improved dependency resolution debug information. Fixed # Fixed missing dependency information for some packages. Fixed handling of single versions when packaging. Fixed dependency information retrieval from .zip and .bz2 archives. Fixed searching for and installing packages from private repositories with authentication. (Thanks to @MarcDufresne ) Fixed a potential error when checking the pyproject.toml validity. (Thanks to @ojii ) Fixed the lock file not tracking the extras information from pyproject.toml . (Thanks to @cauebs ) Fixed missing trailing slash in the Simple API urls for private repositories. (Thanks to @bradsbrown ) 0.11.2 - 2018-07-03 # Fixed # Fixed missing dependencies when resolving in some cases. Fixed path dependencies not working in dev-dependencies . Fixed license validation in init . (Thanks to @cauebs ) 0.11.1 - 2018-06-29 # Fixed # Fixed an error when locking dependencies on Python 2.7. 0.11.0 - 2018-06-28 # Added # Added support for packages , include and exclude properties. Added a new shell command. (Thanks to @cauebs ) Added license validation in init command. Changed # Changed the dependency installation order, deepest dependencies are now installed first. Improved solver error messages. poetry now always reads/writes the pyproject.toml file with the utf-8 encoding. config --list now lists all available settings. init no longer adds pytest to development dependencies. Fixed # Fixed handling of duplicate dependencies with different constraints. Fixed system requirements in lock file for sub dependencies. Fixed detection of new prereleases. Fixed unsafe packages being locked. Fixed versions detection in custom repositories. Fixed package finding with multiple custom repositories. Fixed handling of root incompatibilities. Fixed an error where packages from custom repositories would not be found. Fixed wildcard Python requirement being wrongly set in distributions metadata. Fixed installation of packages from a custom repository. Fixed remove command‚Äôs case sensitivity. (Thanks to @cauebs ) Fixed detection of .egg-info directory for non-poetry projects. (Thanks to @gtors ) Fixed only-wheel builds. (Thanks to @gtors ) Fixed key and array order in lock file to avoid having differences when relocking. Fixed errors when git could not be found. 0.10.3 - 2018-06-04 # Fixed # Fixed self:update command on Windows. Fixed self:update not picking up new versions. Fixed a RuntimeError on Python 3.7. Fixed bad version number being picked with private repositories. Fixed handling of duplicate dependencies with same constraint. Fixed installation from custom repositories. Fixed setting an explicit version in version command. Fixed parsing of wildcards version constraints. 0.10.2 - 2018-05-31 # Fixed # Fixed handling of in environment markers with commas. Fixed a UnicodeDecodeError when an error occurs in venv. Fixed Python requirements not properly set when resolving dependencies. Fixed terminal coloring being activated even if not supported. Fixed wrong executable being picked up on Windows in poetry run . Fixed error when listing distribution links for private repositories. Fixed handling of PEP 440 ~= version constraint. 0.10.1 - 2018-05-28 # Fixed # Fixed packages not found for prerelease version constraints when resolving dependencies. Fixed init and add commands. 0.10.0 - 2018-05-28 # Added # Added a new, more efficient dependency resolver. Added a new init command to generate a pyproject.toml file in existing projects. Added a new setting settings.virtualenvs.in-project to make poetry create the project‚Äôs virtualenv inside the project‚Äôs directory. Added the --extras and --python options to debug:resolve to help debug dependency resolution. Added a --src option to new command to create an src layout. Added support for specifying the platform for dependencies. Added the --python option to the add command. Added the --platform option to the add command. Added a --develop option to the install command to install path dependencies in development/editable mode. Added a develop command to install the current project in development mode. Changed # Improved the show command to make it easier to check if packages are properly installed. The script command has been deprecated, use run instead. The publish command no longer build packages by default. Use --build to retrieve the previous behavior. Improved support for private repositories. Expanded version constraints now keep the original version‚Äôs precision. The lock file hash no longer uses the project‚Äôs name and version. The LICENSE file, or similar, is now automatically added to the built packages. Fixed # Fixed the dependency resolver selecting incompatible packages. Fixed override of dependency with dependency with extras in dev-dependencies . 0.9.1 - 2018-05-18 # Fixed # Fixed handling of package names with dots. (Thanks to bertjwregeer ) Fixed path dependencies being resolved from the current path instead of the pyproject.toml file. (Thanks to radix ) 0.9.0 - 2018-05-07 # Added # Added the cache:clear command. Added support for git dependencies in the add command. Added support for path dependencies in the add command. Added support for extras in the add command. Added support for directory dependencies. Added support for src/ layout for packages. Added automatic detection of .venv virtualenvs. Changed # Drastically improved dependency resolution speed. Dependency resolution caches now use sha256 hashes. Changed CLI error style. Improved debugging of dependency resolution. Poetry now attempts to find pyproject.toml not only in the directory it was\ninvoked in, but in all its parents up to the root. This allows to run Poetry\ncommands in project subdirectories. Made the email address for authors optional. Fixed # Fixed handling of extras when resolving dependencies. Fixed self:update command for some installation. Fixed handling of extras when building projects. Fixed handling of wildcard dependencies wen packaging/publishing. Fixed an error when adding a new packages with prereleases in lock file. Fixed packages name normalization. 0.8.6 - 2018-04-30 # Fixed # Fixed config files not being created. 0.8.5 - 2018-04-19 # Fixed # Fixed a bug in dependency resolution which led to installation errors. Fixed a bug where malformed sdists would lead to dependency resolution failing. 0.8.4 - 2018-04-18 # Fixed # Fixed a bug where dependencies constraints in lock were too strict. Fixed unicode error in search command for Python 2.7. Fixed error with git dependencies. 0.8.3 - 2018-04-16 # Fixed # Fixed platform verification which led to missing packages. Fixed duplicates in pyproject.lock . 0.8.2 - 2018-04-14 # Fixed # Fixed add command picking up prereleases by default. Fixed dependendency resolution on Windows when unpacking distributions. Fixed dependency resolution with post releases. Fixed dependencies being installed even if not necessary for current system. 0.8.1 - 2018-04-13 # Fixed # Fixed resolution with bad (empty) releases. Fixed version for prereleases. Fixed search not working outside of a project. Fixed self:update not working outside of a project. 0.8.0 - 2018-04-13 # Added # Added support for Python 2.7. Added a fallback mechanism for missing dependencies. Added the search command. Added support for local files as dependencies. Added the self:update command. Changes # Improved dependency resolution time by using cache control. Fixed # Fixed install_requires and extras in generated sdist. Fixed dependency resolution crash with malformed dependencies. Fixed errors when license metadata is not set. Fixed missing information in lock file. 0.7.1 - 2018-04-05 # Fixed # Fixed dependency resolution for custom repositories. 0.7.0 - 2018-04-04 # Added # Added compatibility with Python 3.4 and 3.5. Added the version command to automatically bump the package‚Äôs version. Added a standalone installer to install poetry isolated. Added support for classifiers in pyproject.toml . Added the script command. Changed # Improved dependency resolution to avoid unnecessary operations. Improved dependency resolution speed. Improved CLI reactivity by deferring imports. License classifier is not automatically added to classifiers. Fixed # Fixed handling of markers with the in operator. Fixed update not properly adding new packages to the lock file. Fixed solver adding uninstall operations for non-installed packages. Fixed new command creating invalid pyproject.toml files. 0.6.5 - 2018-03-22 # Fixed # Fixed handling of extras in wheels metadata. 0.6.4 - 2018-03-21 # Added # Added a debug:info command to get information about current environment. Fixed # Fixed Python version retrieval inside virtualenvs. Fixed optional dependencies being set as required in sdist. Fixed --optional option in the add command not being used. 0.6.3 - 2018-03-20 # Fixed # Fixed built wheels not getting information from the virtualenv. Fixed building wheel with conditional extensions. Fixed missing files in built wheel with extensions. Fixed call to venv binaries on windows. Fixed subdependencies representation in lock file. 0.6.2 - 2018-03-19 # Changed # Changed how wildcard constraints are handled. Fixed # Fixed errors with pip 9.0.2. 0.6.1 - 2018-02-18 # Fixed # Fixed wheel entry points being written on a single line. Fixed wheel metadata (Tag and Root-Is-Purelib). 0.6.0 - 2018-03-16 # Added # Added support for virtualenv autogeneration (Python 3.6+ only). Added the run command to execute commands inside the created virtualenvs. Added the debug:resolve command to debug dependency resolution. Added pyproject.toml file validation. Added support for Markdown readme files. Fixed # Fixed color displayed in show command for semver-compatible updates. Fixed Python requirements in publishing metadata. Fixed update command reinstalling every dependency. 0.5.0 - 2018-03-14 # Added # Added experimental support for package with C extensions. Changed # Added hashes check when installing packages. Fixed # Fixed handling of post releases. Fixed python restricted dependencies not being checked against virtualenv version. Fixed python/platform constraint not being picked up for subdependencies. Fixed skipped packages appearing as installing. Fixed platform specification not being used when resolving dependencies. 0.4.2 - 2018-03-10 # Fixed # Fixed TypeError when requires_dist is null on PyPI. 0.4.1 - 2018-03-08 # Fixed # Fixed missing entry point 0.4.0 - 2018-03-08 # Added # Added packaging support (sdist and pure-python wheel). Added the build command. Added support for extras definition. Added support for dependencies extras specification. Added the config command. Added the publish command. Changed # Dependencies system constraints are now respected when installing packages. Complied with PEP 440 Fixed # Fixed show command for VCS dependencies. Fixed handling of releases with bad markers in PyPiRepository. 0.3.0 - 2018-03-05 # Added # Added show command. Added the --dry-run option to the add command. Changed # Changed the poetry.toml file for the new, standardized pyproject.toml . Dependencies of each package is now stored in the lock file. Improved TOML file management. Dependency resolver now respects the root package python version requirements. Fixed # Fixed the add command for packages with dots in their names. 0.2.0 - 2018-03-01 # Added # Added remove command. Added basic support for VCS (git) dependencies. Added support for private repositories. Changed # Changed poetry.lock format. Fixed # Fixed dependencies solving that would lead to dependencies not being written to lock. 0.1.0 - 2018-02-28 # Initial release Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/repositories/", "text": "Repositories | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Private Repository Example Package Sources Project Configuration Supported Package Sources Publishable Repositories Configuring Credentials Certificates Custom certificate authority and mutual TLS authentication Caches Debugging Issues Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Repositories Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Repositories Private Repository Example Package Sources Project Configuration Supported Package Sources Publishable Repositories Configuring Credentials Certificates Custom certificate authority and mutual TLS authentication Caches Debugging Issues Repositories # Poetry supports the use of PyPI and private repositories for discovery of\npackages as well as for publishing your projects. By default, Poetry is configured to use the PyPI repository,\nfor package installation and publishing. So, when you add dependencies to your project, Poetry will assume they are available\non PyPI. This represents most cases and will likely be enough for most users. Private Repository Example # Installing from private package sources # By default, Poetry discovers and installs packages from PyPI . But, you want to\ninstall a dependency to your project for a simple API repository ? Let‚Äôs\ndo it. First, configure the package source as a supplemental (or explicit ) package source to your\nproject. poetry source add --priority = supplemental foo https://pypi.example.org/simple/ Then, assuming the repository requires authentication, configure credentials for it. poetry config http-basic.foo <username> <password> Warning Depending on your system configuration, credentials might be saved in your command line history.\nMany shells do not save commands to history when they are prefixed by a space character. For more information, please refer to your shell‚Äôs documentation. Note If you would like to provide the password interactively, you can simply omit <password> in your command. And\nPoetry will prompt you to enter the credential manually. poetry config http-basic.foo <username> Once this is done, you can add dependencies to your project from this source. poetry add --source foo private-package Publishing to a private repository # Great, now all that is left is to publish your package. Assuming you‚Äôd want to share it privately\nwith your team, you can configure the Upload API endpoint for your publishable repository . poetry config repositories.foo https://pypi.example.org/legacy/ Note If you need to use a different credential for your package source , then it is\nrecommended to use a different name for your publishing repository. poetry config repositories.foo-pub https://pypi.example.org/legacy/\npoetry config http-basic.foo-pub <username> <password> Note When configuring a repository using environment variables, note that correct suffixes need to be used. export POETRY_REPOSITORIES_FOO_URL = https://pypi.example.org/legacy/ export POETRY_HTTP_BASIC_FOO_USERNAME = <username> export POETRY_HTTP_BASIC_FOO_PASSWORD = <password> Now, all the is left is to build and publish your project using the publish . poetry publish --build --repository foo-pub Package Sources # By default, Poetry is configured to use the Python ecosystem‚Äôs canonical package index PyPI . Note With the exception of the implicitly configured source for PyPI named pypi ,\npackage sources are local to a project and must be configured within the project‚Äôs pyproject.toml file. This is not the same configuration used\nwhen publishing a package. Project Configuration # These package sources may be managed using the source command for\nyour project. poetry source add foo https://foo.bar/simple/ Note If your package source requires credentials or certificates , please refer to the relevant sections below. This will generate the following configuration snippet in your pyproject.toml file. [[ tool . poetry . source ]] name = \"foo\" url = \"https://foo.bar/simple/\" priority = \"primary\" If priority is undefined, the source is considered a primary source that takes precedence over PyPI, secondary, supplemental and explicit sources. Package sources are considered in the following order: default source (DEPRECATED), primary sources , implicit PyPI (unless disabled by another primary source , default source or configured explicitly), secondary sources (DEPRECATED), supplemental sources . Explicit sources are considered only for packages that explicitly indicate their source . Within each priority class, package sources are considered in order of appearance in pyproject.toml . Note If you want to change the priority of PyPI , you can set it explicitly, e.g. poetry source add --priority = primary PyPI If you prefer to disable PyPI completely,\njust add a primary source or configure PyPI as explicit source . Default Package Source (DEPRECATED) # Deprecated in 1.8.0 Warning Configuring a default package source is deprecated because it is the same\nas the topmost primary source .\nJust configure a primary package source and put it first in the list of package sources. By default, if you have not configured any primary source,\nPoetry will configure PyPI as the package source for your project.\nYou can alter this behaviour and exclusively look up packages only from the configured\npackage sources by adding at least one primary source (recommended)\nor a single source with priority = \"default\" (deprecated). poetry source add --priority = default foo https://foo.bar/simple/ Primary Package Sources # All primary package sources are searched for each dependency without a source constraint .\nIf you configure at least one primary source, the implicit PyPI source is disabled. poetry source add --priority = primary foo https://foo.bar/simple/ Sources without a priority are considered primary sources, too. poetry source add foo https://foo.bar/simple/ Warning The implicit PyPI source is disabled automatically if at least one primary source is configured.\nIf you want to use PyPI in addition to a primary source, configure it explicitly\nwith a certain priority, e.g. poetry source add --priority = primary PyPI This way, the priority of PyPI can be set in a fine-granular way. The equivalent specification in pyproject.toml is: [[ tool . poetry . source ]] name = \"pypi\" priority = \"primary\" Omit the url when specifying PyPI explicitly. Because PyPI is internally configured\nwith Poetry, the PyPI repository cannot be configured with a given URL. Remember, you can always use poetry check to ensure the validity of the pyproject.toml file. Secondary Package Sources (DEPRECATED) # Deprecated in 1.5.0 If package sources are configured as secondary, all it means is that these will be given a lower\npriority when selecting compatible package distribution that also exists in your default and primary package sources. If the package source should instead be searched only if higher-priority repositories did not return results, please consider a supplemental source instead. You can configure a package source as a secondary source with priority = \"secondary\" in your package\nsource configuration. poetry source add --priority = secondary foo https://foo.bar/simple/ There can be more than one secondary package source. Warning Secondary package sources are deprecated in favor of supplemental package sources. Supplemental Package Sources # Introduced in 1.5.0 Package sources configured as supplemental are only searched if no other (higher-priority) source yields a compatible package distribution. This is particularly convenient if the response time of the source is high and relatively few package distributions are to be fetched from this source. You can configure a package source as a supplemental source with priority = \"supplemental\" in your package\nsource configuration. poetry source add --priority = supplemental foo https://foo.bar/simple/ There can be more than one supplemental package source. Warning Take into account that someone could publish a new package to a primary source\nwhich matches a package in your supplemental source. They could coincidentally\nor intentionally replace your dependency with something you did not expect. Explicit Package Sources # Introduced in 1.5.0 If package sources are configured as explicit, these sources are only searched when a package configuration explicitly indicates that it should be found on this package source. You can configure a package source as an explicit source with priority = \"explicit\" in your package source configuration. poetry source add --priority = explicit foo https://foo.bar/simple/ There can be more than one explicit package source. Note A real-world example where an explicit package source is useful, is for PyTorch GPU packages. poetry source add --priority = explicit pytorch-gpu-src https://download.pytorch.org/whl/cu118\npoetry add --source pytorch-gpu-src torch torchvision torchaudio Package Source Constraint # All package sources (including secondary and possibly supplemental sources) will be searched during the package lookup\nprocess. These network requests will occur for all sources, regardless of if the package is\nfound at one or more sources. In order to limit the search for a specific package to a particular package repository, you can specify the source explicitly. poetry add --source internal-pypi httpx This results in the following configuration in pyproject.toml : [ tool . poetry . dependencies ] ... httpx = { version = \"^0.22\" , source = \"internal-pypi\" } [[ tool . poetry . source ]] name = \"internal-pypi\" url = ... priority = ... Note A repository that is configured to be the only source for retrieving a certain package can itself have any priority.\nIn particular, it does not need to have priority \"explicit\" .\nIf a repository is configured to be the source of a package, it will be the only source that is considered for that package\nand the repository priority will have no effect on the resolution. Note Package source keys are not inherited by their dependencies.\nIn particular, if package-A is configured to be found in source = internal-pypi ,\nand package-A depends on package-B that is also to be found on internal-pypi ,\nthen package-B needs to be configured as such in pyproject.toml .\nThe easiest way to achieve this is to add package-B with a wildcard constraint: poetry add --source internal-pypi package-B@* This will ensure that package-B is searched only in the internal-pypi package source.\nThe version constraints on package-B are derived from package-A (and other client packages), as usual. If you want to avoid additional main dependencies,\nyou can add package-B to a dedicated dependency group : poetry add --group explicit --source internal-pypi package-B@* Note Package source constraints are strongly suggested for all packages that are expected\nto be provided only by one specific source to avoid dependency confusion attacks. Supported Package Sources # Python Package Index (PyPI) # Poetry interacts with PyPI via its JSON API . This is used to retrieve a requested\npackage‚Äôs versions, metadata, files, etc. Note If the the package‚Äôs published metadata is invalid, Poetry will download the available bdist/sdist to\ninspect it locally to identify the relevant metadata. If you want to explicitly select a package from PyPI you can use the --source option with the add command, like shown below. poetry add --source pypi httpx@^0.22.0 This will generate the following configuration snippet in your pyproject.toml file. httpx = { version = \"^0.22.0\" , source = \"pypi\" } Warning If any source within a project is configured with priority = \"default\" , The implicit pypi source will\nbe disabled and not used for any packages. Simple API Repository # Poetry can fetch and install package dependencies from public or private custom repositories that\nimplement the simple repository API as described in PEP 503 . Warning When using sources that distributes large wheels without providing file checksum in file URLs,\nPoetry will download each candidate wheel at least once in order to generate the checksum. This can\nmanifest as long dependency resolution times when adding packages from this source. These package sources may be configured via the following command in your project. poetry source add testpypi https://test.pypi.org/simple/ Note Note the trailing /simple/ . This is important when configuring PEP 503 compliant package sources. In addition to PEP 503 , Poetry can also handle simple API\nrepositories that implement PEP 658 ( Introduced in 1.2.0 ).\nThis is helpful in reducing dependency resolution time for packages from these sources as Poetry can\navoid having to download each candidate distribution, in order to determine associated metadata. Note Why does Poetry insist on downloading all candidate distributions for all platforms when metadata\nis not available? The need for this stems from the fact that Poetry‚Äôs lock file is platform-agnostic. This means, in\norder to resolve dependencies for a project, Poetry needs metadata for all platform specific\ndistributions. And when this metadata is not readily available, downloading the distribution and\ninspecting it locally is the only remaining option. Single Page Link Source # Introduced in 1.2.0 Some projects choose to release their binary distributions via a single page link source that\npartially follows the structure of a package page in PEP 503 . These package sources may be configured via the following command in your project. poetry source add jax https://storage.googleapis.com/jax-releases/jax_releases.html Note All caveats regarding slower resolution times described for simple API repositories do apply here as\nwell. Publishable Repositories # Poetry treats repositories to which you publish packages as user specific and not project specific\nconfiguration unlike package sources . Poetry, today, only supports the Legacy Upload API when publishing\nyour project. These are configured using the config command, under the repositories key. poetry config repositories.testpypi https://test.pypi.org/legacy/ Note Legacy Upload API URLs are\ntypically different to the same one provided by the repository for the simple API. You‚Äôll note that\nin the example of Test PyPI , both the host ( test.pypi.org ) as\nwell as the path ( /legacy ) are different to its simple API ( https://test.pypi.org/simple ). Configuring Credentials # If you want to store your credentials for a specific repository, you can do so easily: poetry config http-basic.foo <username> <password> If you do not specify the password you will be prompted to write it. Note To publish to PyPI, you can set your credentials for the repository named pypi . Note that it is recommended to use API tokens when uploading packages to PyPI.\nOnce you have created a new token, you can tell Poetry to use it: poetry config pypi-token.pypi <my-token> If you have configured testpypi as a Publishable Repository , the token can be set using poetry config pypi-token.testpypi <your-token> If you still want to use your username and password, you can do so with the following\ncall to config . poetry config http-basic.pypi <username> <password> You can also specify the username and password when using the publish command\nwith the --username and --password options. If a system keyring is available and supported, the password is stored to and retrieved from the keyring. In the above example, the credential will be stored using the name poetry-repository-pypi . If access to keyring fails or is unsupported, this will fall back to writing the password to the auth.toml file along with the username. Keyring support is enabled using the keyring library . For more information on supported backends refer to the library documentation . If you do not want to use the keyring, you can tell Poetry to disable it and store the credentials in plaintext config files: poetry config keyring.enabled false Note Poetry will fallback to Pip style use of keyring so that backends like\nMicrosoft‚Äôs artifacts-keyring get a chance to retrieve\nvalid credentials. It will need to be properly installed into Poetry‚Äôs virtualenv,\npreferably by installing a plugin. Alternatively, you can use environment variables to provide the credentials: export POETRY_PYPI_TOKEN_FOO = my-token export POETRY_HTTP_BASIC_FOO_USERNAME = <username> export POETRY_HTTP_BASIC_FOO_PASSWORD = <password> where FOO is the name of the repository in uppercase (e.g. PYPI ).\nSee Using environment variables for more information\non how to configure Poetry with environment variables. If your password starts with a dash (e.g. randomly generated tokens in a CI environment), it will be parsed as a\ncommand line option instead of a password.\nYou can prevent this by adding double dashes to prevent any following argument from being parsed as an option. poetry config -- http-basic.pypi myUsername -myPasswordStartingWithDash Certificates # Custom certificate authority and mutual TLS authentication # Poetry supports repositories that are secured by a custom certificate authority as well as those that require\ncertificate-based client authentication. The following will configure the ‚Äúfoo‚Äù repository to validate the repository‚Äôs\ncertificate using a custom certificate authority and use a client certificate (note that these config variables do not\nboth need to be set): poetry config certificates.foo.cert /path/to/ca.pem\npoetry config certificates.foo.client-cert /path/to/client.pem Note The value of certificates.<repository>.cert can be set to false if certificate verification is\nrequired to be skipped. This is useful for cases where a package source with self-signed certificates\nare used. poetry config certificates.foo.cert false Warning Disabling certificate verification is not recommended as it is does not conform to security\nbest practices. Caches # Poetry employs multiple caches for package sources in order to improve user experience and avoid duplicate network\nrequests. The first level cache is a Cache-Control header based cache for almost all HTTP requests. Further, every HTTP backed package source caches metadata associated with a package once it is fetched or generated.\nAdditionally, downloaded files (package distributions) are also cached. Debugging Issues # If you encounter issues with package sources, one of the simplest steps you might take to debug an issue is rerunning\nyour command with the --no-cache flag. poetry --no-cache add pycowsay If this solves your issue, you can consider clearing your cache using the cache command. Alternatively, you could also consider enabling very verbose logging -vvv along with the --no-cache to see network\nrequests being made in the logs. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/configuration/", "text": "Configuration | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Commands Configuration Local configuration Listing the current configuration Displaying a single configuration setting Adding or updating a configuration setting Removing a specific setting Using environment variables Default Directories Config Directory Data Directory Cache Directory Available settings cache-dir experimental.system-git-client installer.max-workers installer.modern-installation installer.no-binary installer.parallel solver.lazy-wheel virtualenvs.create virtualenvs.in-project virtualenvs.options.always-copy virtualenvs.options.no-pip virtualenvs.options.no-setuptools virtualenvs.options.system-site-packages virtualenvs.path virtualenvs.prefer-active-python (experimental) virtualenvs.prompt repositories.<name>.url http-basic.<name>.[username|password] : pypi-token.<name> : certificates.<name>.cert : certificates.<name>.client-cert : keyring.enabled : Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Configuration Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Configuration Local configuration Listing the current configuration Displaying a single configuration setting Adding or updating a configuration setting Removing a specific setting Using environment variables Default Directories Config Directory Data Directory Cache Directory Available settings cache-dir experimental.system-git-client installer.max-workers installer.modern-installation installer.no-binary installer.parallel solver.lazy-wheel virtualenvs.create virtualenvs.in-project virtualenvs.options.always-copy virtualenvs.options.no-pip virtualenvs.options.no-setuptools virtualenvs.options.system-site-packages virtualenvs.path virtualenvs.prefer-active-python (experimental) virtualenvs.prompt repositories.<name>.url http-basic.<name>.[username|password] : pypi-token.<name> : certificates.<name>.cert : certificates.<name>.client-cert : keyring.enabled : Configuration # Poetry can be configured via the config command ( see more about its usage here )\nor directly in the config.toml file that will be automatically created when you first run that command.\nThis file can typically be found in one of the following directories: macOS: ~/Library/Application Support/pypoetry Windows: %APPDATA%\\pypoetry For Unix, we follow the XDG spec and support $XDG_CONFIG_HOME .\nThat means, by default ~/.config/pypoetry . Local configuration # Poetry also provides the ability to have settings that are specific to a project\nby passing the --local option to the config command. poetry config virtualenvs.create false --local Note Your local configuration of Poetry application is stored in the poetry.toml file,\nwhich is separate from pyproject.toml . Warning Be mindful about checking in this file into your repository since it may contain user-specific or sensitive information. Listing the current configuration # To list the current configuration you can use the --list option\nof the config command: poetry config --list which will give you something similar to this: cache - dir = \"/path/to/cache/directory\" virtualenvs . create = true virtualenvs . in - project = null virtualenvs . options . always - copy = true virtualenvs . options . no - pip = false virtualenvs . options . no - setuptools = false virtualenvs . options . system - site - packages = false virtualenvs . path = \"{cache-dir}/virtualenvs\" # /path/to/cache/directory/virtualenvs virtualenvs . prefer - active - python = false virtualenvs . prompt = \"{project_name}-py{python_version}\" Displaying a single configuration setting # If you want to see the value of a specific setting, you can\ngive its name to the config command poetry config virtualenvs.path For a full list of the supported settings see Available settings . Adding or updating a configuration setting # To change or otherwise add a new configuration setting, you can pass\na value after the setting‚Äôs name: poetry config virtualenvs.path /path/to/cache/directory/virtualenvs For a full list of the supported settings see Available settings . Removing a specific setting # If you want to remove a previously set setting, you can use the --unset option: poetry config virtualenvs.path --unset The setting will then retrieve its default value. Using environment variables # Sometimes, in particular when using Poetry with CI tools, it‚Äôs easier\nto use environment variables and not have to execute configuration commands. Poetry supports this and any setting can be set by using environment variables. The environment variables must be prefixed by POETRY_ and are comprised of the uppercase\nname of the setting and with dots and dashes replaced by underscore, here is an example: export POETRY_VIRTUALENVS_PATH = /path/to/virtualenvs/directory This also works for secret settings, like credentials: export POETRY_HTTP_BASIC_MY_REPOSITORY_PASSWORD = secret Default Directories # Poetry uses the following default directories: Config Directory # Linux: $XDG_CONFIG_HOME/pypoetry or ~/.config/pypoetry Windows: %APPDATA%\\pypoetry MacOS: ~/Library/Application Support/pypoetry You can override the Config directory by setting the POETRY_CONFIG_DIR environment variable. Data Directory # Linux: $XDG_DATA_HOME/pypoetry or ~/.local/share/pypoetry Windows: %APPDATA%\\pypoetry MacOS: ~/Library/Application Support/pypoetry You can override the Data directory by setting the POETRY_DATA_DIR or POETRY_HOME environment variables. If POETRY_HOME is set, it will be given higher priority. Cache Directory # Linux: $XDG_CACHE_HOME/pypoetry or ~/.cache/pypoetry Windows: %LOCALAPPDATA%\\pypoetry MacOS: ~/Library/Caches/pypoetry You can override the Cache directory by setting the POETRY_CACHE_DIR environment variable. Available settings # cache-dir # Type : string Environment Variable : POETRY_CACHE_DIR The path to the cache directory used by Poetry. Defaults to one of the following directories: macOS: ~/Library/Caches/pypoetry Windows: C:\\Users\\<username>\\AppData\\Local\\pypoetry\\Cache Unix: ~/.cache/pypoetry experimental.system-git-client # Type : boolean Default : false Environment Variable : POETRY_EXPERIMENTAL_SYSTEM_GIT_CLIENT Introduced in 1.2.0 Use system git client backend for git related tasks. Poetry uses dulwich by default for git related tasks to not rely on the availability of a git client. If you encounter any problems with it, set to true to use the system git backend. installer.max-workers # Type : int Default : number_of_cores + 4 Environment Variable : POETRY_INSTALLER_MAX_WORKERS Introduced in 1.2.0 Set the maximum number of workers while using the parallel installer.\nThe number_of_cores is determined by os.cpu_count() .\nIf this raises a NotImplementedError exception, number_of_cores is assumed to be 1. If this configuration parameter is set to a value greater than number_of_cores + 4 ,\nthe number of maximum workers is still limited at number_of_cores + 4 . Note This configuration is ignored when installer.parallel is set to false . installer.modern-installation # Type : boolean Default : true Environment Variable : POETRY_INSTALLER_MODERN_INSTALLATION Introduced in 1.4.0 Use a more modern and faster method for package installation. If this causes issues, you can disable it by setting it to false and report the problems\nyou encounter on the issue tracker . installer.no-binary # Type : string | boolean Default : false Environment Variable : POETRY_INSTALLER_NO_BINARY Introduced in 1.2.0 When set this configuration allows users to configure package distribution format policy for all or\nspecific packages. Configuration Description :all: or true Disallow binary distributions for all packages. :none: or false Allow binary distributions for all packages. package[,package,..] Disallow binary distributions for specified packages only. Note As with all configurations described here, this is a user specific configuration. This means that this\nis not taken into consideration when a lockfile is generated or dependencies are resolved. This is\napplied only when selecting which distribution for dependency should be installed into a Poetry managed\nenvironment. Note For project specific usage, it is recommended that this be configured with the --local . poetry config --local installer.no-binary :all: Note For CI or container environments using environment variable to configure this might be useful. export POETRY_INSTALLER_NO_BINARY = :all: Warning Unless this is required system-wide, if configured globally, you could encounter slower install times\nacross all your projects if incorrectly set. installer.parallel # Type : boolean Default : true Environment Variable : POETRY_INSTALLER_PARALLEL Introduced in 1.1.4 Use parallel execution when using the new ( >=1.1.0 ) installer. solver.lazy-wheel # Type : boolean Default : true Environment Variable : POETRY_SOLVER_LAZY_WHEEL Introduced in 1.8.0 Do not download entire wheels to extract metadata but use HTTP range requests to only download the METADATA files of wheels.\nEspecially with slow network connections this setting can speed up dependency resolution significantly.\nIf the cache has already been filled or the server does not support HTTP range requests,\nthis setting makes no difference. virtualenvs.create # Type : boolean Default : true Environment Variable : POETRY_VIRTUALENVS_CREATE Create a new virtual environment if one doesn‚Äôt already exist. If set to false , Poetry will not create a new virtual environment. If it detects an already enabled virtual\nenvironment or an existing one in {cache-dir}/virtualenvs or {project-dir}/.venv it will\ninstall dependencies into them, otherwise it will install dependencies into the systems python environment. Note If Poetry detects it‚Äôs running within an activated virtual environment, it will never create a new virtual environment,\nregardless of the value set for virtualenvs.create . Note Be aware that installing dependencies into the system environment likely upgrade or uninstall existing packages and thus\nbreak other applications. Installing additional Python packages after installing the project might break the Poetry\nproject in return. This is why it is recommended to always create a virtual environment. This is also true in Docker containers, as they\nmight contain additional Python packages as well. virtualenvs.in-project # Type : boolean Default : None Environment Variable : POETRY_VIRTUALENVS_IN_PROJECT Create the virtualenv inside the project‚Äôs root directory. If not set explicitly, poetry by default will create a virtual environment under {cache-dir}/virtualenvs or use the {project-dir}/.venv directory if one already exists. If set to true , the virtualenv will be created and expected in a folder named .venv within the root directory of the project. Note If a virtual environment has already been created for the project under {cache-dir}/virtualenvs , setting this variable to true will not cause poetry to create or use a local virtual environment. In order for this setting to take effect for a project already in that state, you must delete the virtual environment folder located in {cache-dir}/virtualenvs . You can find out where the current project‚Äôs virtual environment (if there is one) is stored\nwith the command poetry env info --path . If set to false , poetry will ignore any existing .venv directory. virtualenvs.options.always-copy # Type : boolean Default : false Environment Variable : POETRY_VIRTUALENVS_OPTIONS_ALWAYS_COPY Introduced in 1.2.0 If set to true the --always-copy parameter is passed to virtualenv on creation of the virtual environment, so that\nall needed files are copied into it instead of symlinked. virtualenvs.options.no-pip # Type : boolean Default : false Environment Variable : POETRY_VIRTUALENVS_OPTIONS_NO_PIP Introduced in 1.2.0 If set to true the --no-pip parameter is passed to virtualenv on creation of the virtual environment. This means\nwhen a new virtual environment is created, pip will not be installed in the environment. Note Poetry, for its internal operations, uses the pip wheel embedded in the virtualenv package installed as a dependency\nin Poetry‚Äôs runtime environment. If a user runs poetry run pip when this option is set to true , the pip the\nembedded instance of pip is used. You can safely set this, along with no-setuptools , to true , if you desire a virtual environment with no additional\npackages. This is desirable for production environments. virtualenvs.options.no-setuptools # Type : boolean Default : false Environment Variable : POETRY_VIRTUALENVS_OPTIONS_NO_SETUPTOOLS Introduced in 1.2.0 If set to true the --no-setuptools parameter is passed to virtualenv on creation of the virtual environment. This\nmeans when a new virtual environment is created, setuptools will not be installed in the environment. Poetry, for its\ninternal operations, does not require setuptools and this can safely be set to true . For environments using python 3.12 or later, virtualenv defaults to not\ninstalling setuptools when creating a virtual environment.\nIn such environments this poetry configuration option therefore has no effect: setuptools is not installed either way.\nIf your project relies on setuptools , you should declare it as a dependency. Warning Some development tools like IDEs, make an assumption that setuptools (and other) packages are always present and\navailable within a virtual environment. This can cause some features in these tools to not work as expected. virtualenvs.options.system-site-packages # Type : boolean Default : false Environment Variable : POETRY_VIRTUALENVS_OPTIONS_SYSTEM_SITE_PACKAGES Give the virtual environment access to the system site-packages directory.\nApplies on virtualenv creation. virtualenvs.path # Type : string Default : {cache-dir}/virtualenvs Environment Variable : POETRY_VIRTUALENVS_PATH Directory where virtual environments will be created. Note This setting controls the global virtual environment storage path. It most likely will not be useful at the local level. To store virtual environments in the project root, see virtualenvs.in-project . virtualenvs.prefer-active-python (experimental) # Type : boolean Default : false Environment Variable : POETRY_VIRTUALENVS_PREFER_ACTIVE_PYTHON Introduced in 1.2.0 Use currently activated Python version to create a new virtual environment.\nIf set to false , Python version used during Poetry installation is used. virtualenvs.prompt # Type : string Default : {project_name}-py{python_version} Environment Variable : POETRY_VIRTUALENVS_PROMPT Introduced in 1.2.0 Format string defining the prompt to be displayed when the virtual environment is activated.\nThe variables project_name and python_version are available for formatting. repositories.<name>.url # Type : string Environment Variable : POETRY_REPOSITORIES_<NAME>_URL Set the repository URL for <name> . See Publishable Repositories for more information. http-basic.<name>.[username|password] : # Type : string Environment Variables : POETRY_HTTP_BASIC_<NAME>_USERNAME , POETRY_HTTP_BASIC_<NAME>_PASSWORD Set repository credentials ( username and password ) for <name> .\nSee Repositories - Configuring credentials for more information. pypi-token.<name> : # Type : string Environment Variable : POETRY_PYPI_TOKEN_<NAME> Set repository credentials (using an API token) for <name> .\nSee Repositories - Configuring credentials for more information. certificates.<name>.cert : # Type : string | boolean Environment Variable : POETRY_CERTIFICATES_<NAME>_CERT Set custom certificate authority for repository <name> .\nSee Repositories - Configuring credentials - Custom certificate authority for more information. This configuration can be set to false , if TLS certificate verification should be skipped for this\nrepository. certificates.<name>.client-cert : # Type : string Environment Variable : POETRY_CERTIFICATES_<NAME>_CLIENT_CERT Set client certificate for repository <name> .\nSee Repositories - Configuring credentials - Custom certificate authority for more information. keyring.enabled : # Type : boolean Default : true Environment Variable : POETRY_KEYRING_ENABLED Enable the system keyring for storing credentials.\nSee Repositories - Configuring credentials for more information. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/libraries/", "text": "Libraries | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Versioning Lock file Packaging Publishing to PyPI Publishing to a private repository Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Libraries Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Libraries Versioning Lock file Packaging Publishing to PyPI Publishing to a private repository Libraries # This chapter will tell you how to make your library installable through Poetry. Versioning # Poetry requires PEP 440 -compliant versions for all projects. While Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver. Note As an example, 1.0.0-hotfix.1 is not compatible with PEP 440 . You can instead\nchoose to use 1.0.0-post1 or 1.0.0.post1 . Lock file # For your library, you may commit the poetry.lock file if you want to.\nThis can help your team to always test against the same dependency versions.\nHowever, this lock file will not have any effect on other projects that depend on it.\nIt only has an effect on the main project. If you do not want to commit the lock file and you are using git, add it to the .gitignore . Packaging # Before you can actually publish your library, you will need to package it. poetry build This command will package your library in two different formats: sdist which is\nthe source format, and wheel which is a compiled package. Poetry will automatically include some metadata files when building a package. When building\na wheel , the following files are included in the .dist-info directory: LICENSE LICENSE.* COPYING COPYING.* LICENSES/** When building an sdist , the following files will be included in the root folder: LICENSE* Once building is done you are ready to publish your library. Publishing to PyPI # Alright, so now you can publish packages. Poetry will publish to PyPI by default. Anything that is published to PyPI\nis available automatically through Poetry. Since pendulum is on PyPI we can depend on it without having to specify any additional repositories. If we wanted to share poetry-demo with the Python community, we would publish on PyPI as well.\nDoing so is really easy. poetry publish This will package and publish the library to PyPI, at the condition that you are a registered user\nand you have configured your credentials properly. Note The publish command does not execute build by default. If you want to build and publish your packages together,\njust pass the --build option. Once this is done, your library will be available to anyone. Publishing to a private repository # Sometimes, you may want to keep your library private but also being accessible to your team. In this case, you will need to use a private repository. In order to publish to a private repository, you will need to add it to your\nglobal list of repositories. See Adding a repository for more information. Once this is done, you can actually publish to it like so: poetry publish -r my-repository Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/managing-dependencies/", "text": "Managing dependencies | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Dependency groups Optional groups Adding a dependency to a group Installing group dependencies Removing dependencies from a group Synchronizing dependencies Layering optional groups Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Managing dependencies Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Managing dependencies Dependency groups Optional groups Adding a dependency to a group Installing group dependencies Removing dependencies from a group Synchronizing dependencies Layering optional groups Managing dependencies # Dependency groups # Poetry provides a way to organize your dependencies by groups . For instance, you might have\ndependencies that are only needed to test your project or to build the documentation. To declare a new dependency group, use a tool.poetry.group.<group> section\nwhere <group> is the name of your dependency group (for instance, test ): [ tool . poetry . group . test ] # This part can be left out [ tool . poetry . group . test . dependencies ] pytest = \"^6.0.0\" pytest - mock = \"*\" Note All dependencies must be compatible with each other across groups since they will\nbe resolved regardless of whether they are required for installation or not (see Installing group dependencies ). Think of dependency groups as labels associated with your dependencies: they don‚Äôt have any bearings\non whether their dependencies will be resolved and installed by default , they are simply a way to organize\nthe dependencies logically. Note The dependencies declared in tool.poetry.dependencies are part of an implicit main group. [ tool . poetry . dependencies ] # main dependency group httpx = \"*\" pendulum = \"*\" [ tool . poetry . group . test . dependencies ] pytest = \"^6.0.0\" pytest - mock = \"*\" Note Dependency groups, other than the implicit main group, must only contain dependencies you need in your development\nprocess. Installing them is only possible by using Poetry. To declare a set of dependencies, which add additional functionality to the project during runtime,\nuse extras instead. Extras can be installed by the end user using pip . Note A note about defining a dev dependencies group The proper way to define a dev dependencies group since Poetry 1.2.0 is the following: [ tool . poetry . group . dev . dependencies ] pytest = \"^6.0.0\" pytest - mock = \"*\" This group notation is preferred since Poetry 1.2.0 and not usable in earlier versions.\nFor backwards compatibility with older versions of Poetry,\nany dependency declared in the dev-dependencies section will automatically be added to the dev group.\nSo the above and following notations are equivalent: # Poetry pre-1.2.x style, understood by Poetry 1.0‚Äì1.2 [ tool . poetry . dev - dependencies ] pytest = \"^6.0.0\" pytest - mock = \"*\" Poetry will slowly transition away from the dev-dependencies notation which will soon be deprecated,\nso it‚Äôs advised to migrate your existing development dependencies to the new group notation. Optional groups # A dependency group can be declared as optional. This makes sense when you have\na group of dependencies that are only required in a particular environment or for\na specific purpose. [ tool . poetry . group . docs ] optional = true [ tool . poetry . group . docs . dependencies ] mkdocs = \"*\" Optional groups can be installed in addition to the default dependencies by using the --with option of the install command. poetry install --with docs Warning Optional group dependencies will still be resolved alongside other dependencies, so\nspecial care should be taken to ensure they are compatible with each other. Adding a dependency to a group # The add command is the preferred way to add dependencies\nto a group. This is done by using the --group (-G) option. poetry add pytest --group test If the group does not already exist, it will be created automatically. Installing group dependencies # By default , dependencies across all non-optional groups will be installed when executing poetry install . Note The default set of dependencies for a project includes the implicit main group defined in tool.poetry.dependencies as well as all groups that are not explicitly marked as an optional group . You can exclude one or more groups with the --without option: poetry install --without test,docs You can also opt in optional groups by using the --with option: poetry install --with docs Warning When used together, --without takes precedence over --with . For example, the following command\nwill only install the dependencies specified in the optional test group. poetry install --with test,docs --without docs Finally, in some case you might want to install only specific groups of dependencies\nwithout installing the default set of dependencies. For that purpose, you can use\nthe --only option. poetry install --only docs Note If you only want to install the project‚Äôs runtime dependencies, you can do so with the --only main notation: poetry install --only main Note If you want to install the project root, and no other dependencies, you can use\nthe --only-root option. poetry install --only-root Removing dependencies from a group # The remove command supports a --group option\nto remove packages from a specific group: poetry remove mkdocs --group docs Synchronizing dependencies # Poetry supports what‚Äôs called dependency synchronization. Dependency synchronization ensures\nthat the locked dependencies in the poetry.lock file are the only ones present\nin the environment, removing anything that‚Äôs not necessary. This is done by using the --sync option of the install command: poetry install --sync The --sync option can be combined with any dependency groups related options\nto synchronize the environment with specific groups. Note that extras are separate. Any\nextras not selected for install are always removed, regardless of --sync . poetry install --without dev --sync\npoetry install --with docs --sync\npoetry install --only dev Note The --sync option replaces the --remove-untracked option which is now deprecated. Layering optional groups # When you omit the --sync option, you can install any subset of optional groups without removing\nthose that are already installed. This is very useful, for example, in multi-stage\nDocker builds, where you run poetry install multiple times in different build stages. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/1.7/", "text": "Introduction | 1.7 | Documentation | Poetry - Python dependency management and packaging made easy Home 1.7 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\nname The name of the package. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction System requirements Installation Enable tab completion for Bash, Fish, or Zsh Bash Fish Zsh Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Introduction Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Introduction System requirements Installation Enable tab completion for Bash, Fish, or Zsh Bash Fish Zsh Introduction # Poetry is a tool for dependency management and packaging in Python.\nIt allows you to declare the libraries your project depends on and it will manage (install/update) them for you.\nPoetry offers a lockfile to ensure repeatable installs, and can build your project for distribution. System requirements # Poetry requires Python 3.8+ . It is multi-platform and the goal is to make it work equally well\non Linux, macOS and Windows. Installation # Warning Poetry should always be installed in a dedicated virtual environment to isolate it from the rest of your system.\nIn no case, it should be installed in the environment of the project that is to be managed by Poetry.\nThis ensures that Poetry‚Äôs own dependencies will not be accidentally upgraded or uninstalled.\n(Each of the following installation methods ensures that Poetry is installed into an isolated environment.) Note If you are viewing documentation for the development branch, you may wish to install a preview or development version of Poetry.\nSee the advanced installation instructions to use a preview or alternate version of Poetry. With pipx With pipx With the official installer Manually (advanced) CI recommendations With pipx With the official installer Manually (advanced) CI recommendations pipx is used to install Python CLI applications globally while still isolating them in virtual environments. pipx will manage upgrades and uninstalls when used to install Poetry. Install Poetry pipx install poetry Install Poetry (advanced) pipx can install different versions of Poetry, using the same syntax as pip: pipx install poetry == 1.2.0 pipx can also install versions of Poetry in parallel, which allows for easy testing of alternate or prerelease\nversions. Each version is given a unique, user-specified suffix, which will be used to create a unique binary name: pipx install --suffix = @1.2.0 poetry == 1.2.0\npoetry@1.2.0 --version pipx install --suffix = @preview --pip-args = --pre poetry\npoetry@preview --version Finally, pipx can install any valid pip requirement spec , which\nallows for installations of the development version from git , or even for local testing of pull requests: pipx install --suffix @master git+https://github.com/python-poetry/poetry.git@master\npipx install --suffix @pr1234 git+https://github.com/python-poetry/poetry.git@refs/pull/1234/head Update Poetry pipx upgrade poetry Uninstall Poetry pipx uninstall poetry We provide a custom installer that will install Poetry in a new virtual environment\nand allows Poetry to manage its own environment. Install Poetry The installer script is available directly at install.python-poetry.org ,\nand is developed in its own repository .\nThe script can be executed directly (i.e. ‚Äòcurl python‚Äô) or downloaded and then executed from disk\n(e.g. in a CI environment). Warning The install-poetry.py installer has been deprecated and removed from the Poetry repository.\nPlease migrate from the in-tree version to the standalone version described above. Linux, macOS, Windows (WSL) curl -sSL https://install.python-poetry.org | python3 - Note Note: On some systems, python may still refer to Python 2 instead of Python 3. We always suggest the python3 binary to avoid ambiguity. Windows (Powershell) ( Invoke-WebRequest -Uri https : // install . python-poetry . org -UseBasicParsing ). Content | py - Note If you have installed Python through the Microsoft Store, replace py with python in the command\nabove. Install Poetry (advanced) By default, Poetry is installed into a platform and user-specific directory: ~/Library/Application Support/pypoetry on MacOS. ~/.local/share/pypoetry on Linux/Unix. %APPDATA%\\pypoetry on Windows. If you wish to change this, you may define the $POETRY_HOME environment variable: curl -sSL https://install.python-poetry.org | POETRY_HOME = /etc/poetry python3 - If you want to install prerelease versions, you can do so by passing the --preview option to the installation script\nor by using the $POETRY_PREVIEW environment variable: curl -sSL https://install.python-poetry.org | python3 - --preview\ncurl -sSL https://install.python-poetry.org | POETRY_PREVIEW = 1 python3 - Similarly, if you want to install a specific version, you can use --version option or the $POETRY_VERSION environment variable: curl -sSL https://install.python-poetry.org | python3 - --version 1.2.0\ncurl -sSL https://install.python-poetry.org | POETRY_VERSION = 1.2.0 python3 - You can also install Poetry from a git repository by using the --git option: curl -sSL https://install.python-poetry.org | python3 - --git https://github.com/python-poetry/poetry.git@master Add Poetry to your PATH The installer creates a poetry wrapper in a well-known, platform-specific directory: $HOME/.local/bin on Unix. %APPDATA%\\Python\\Scripts on Windows. $POETRY_HOME/bin if $POETRY_HOME is set. If this directory is not present in your $PATH , you can add it in order to invoke Poetry\nas poetry . Alternatively, the full path to the poetry binary can always be used: ~/Library/Application Support/pypoetry/venv/bin/poetry on MacOS. ~/.local/share/pypoetry/venv/bin/poetry on Linux/Unix. %APPDATA%\\pypoetry\\venv\\Scripts\\poetry on Windows. $POETRY_HOME/venv/bin/poetry if $POETRY_HOME is set. Use Poetry Once Poetry is installed and in your $PATH , you can execute the following: poetry --version If you see something like Poetry (version 1.2.0) , your install is ready to use! Update Poetry Poetry is able to update itself when installed using the official installer. Warning Especially on Windows, self update may be problematic\nso that a re-install with the installer should be preferred. poetry self update If you want to install pre-release versions, you can use the --preview option. poetry self update --preview And finally, if you want to install a specific version, you can pass it as an argument\nto self update . poetry self update 1.2.0 Warning Poetry 1.1 series releases are not able to update in-place to 1.2 or newer series releases.\nTo migrate to newer releases, uninstall using your original install method, and then reinstall\nusing the methods above . Uninstall Poetry If you decide Poetry isn‚Äôt your thing, you can completely remove it from your system\nby running the installer again with the --uninstall option or by setting\nthe POETRY_UNINSTALL environment variable before executing the installer. curl -sSL https://install.python-poetry.org | python3 - --uninstall\ncurl -sSL https://install.python-poetry.org | POETRY_UNINSTALL = 1 python3 - Warning If you installed using the deprecated get-poetry.py script, you should remove the path it uses manually, e.g. rm -rf \" ${ POETRY_HOME :- ~/.poetry } \" Also remove ~/.poetry/bin from your $PATH in your shell configuration, if it is present. Poetry can be installed manually using pip and the venv module. By doing so you will essentially perform the steps carried\nout by the official installer. As this is an advanced installation method, these instructions are Unix-only and omit specific\nexamples such as installing from git . The variable $VENV_PATH will be used to indicate the path at which the virtual environment was created. python3 -m venv $VENV_PATH $VENV_PATH /bin/pip install -U pip setuptools $VENV_PATH /bin/pip install poetry Poetry will be available at $VENV_PATH/bin/poetry and can be invoked directly or symlinked elsewhere. To uninstall Poetry, simply delete the entire $VENV_PATH directory. Unlike development environments, where making use of the latest tools is desirable, in a CI environment reproducibility\nshould be made the priority. Here are some suggestions for installing Poetry in such an environment. Version pinning Whatever method you use, it is highly recommended to explicitly control the version of Poetry used, so that you are able\nto upgrade after performing your own validation. Each install method has a different syntax for setting the version that\nis used in the following examples. Using pipx Just as pipx is a powerful tool for development use, it is equally useful in a CI environment\nand should be one of your top choices for use of Poetry in CI. pipx install poetry == 1.2.0 Using install.python-poetry.org Note The official installer script ( install.python-poetry.org ) offers a streamlined and\nsimplified installation of Poetry, sufficient for developer use or for simple pipelines. However, in a CI environment\nthe other two supported installation methods (pipx and manual) should be seriously considered. Downloading a copy of the installer script to a place accessible by your CI pipelines (or maintaining a copy of the repository ) is strongly suggested, to ensure your\npipeline‚Äôs stability and to maintain control over what code is executed. By default, the installer will install to a user-specific directory. In more complex pipelines that may make accessing\nPoetry difficult (especially in cases like multi-stage container builds). It is highly suggested to make use of $POETRY_HOME when using the official installer in CI, as that way the exact paths can be controlled. export POETRY_HOME = /opt/poetry\npython3 install-poetry.py --version 1.2.0 $POETRY_HOME /bin/poetry --version Using pip (aka manually) For maximum control in your CI environment, installation with pip is fully supported and something you should\nconsider. While this requires more explicit commands and knowledge of Python packaging from you, it in return offers the\nbest debugging experience, and leaves you subject to the fewest external tools. export POETRY_HOME = /opt/poetry\npython3 -m venv $POETRY_HOME $POETRY_HOME /bin/pip install poetry == 1.2.0 $POETRY_HOME /bin/poetry --version Note If you install Poetry via pip , ensure you have Poetry installed into an isolated environment that is not the same as the target environment managed by Poetry. If Poetry and your project are installed into the same environment, Poetry\nis likely to upgrade or uninstall its own dependencies (causing hard-to-debug and understand errors). Enable tab completion for Bash, Fish, or Zsh # poetry supports generating completion scripts for Bash, Fish, and Zsh.\nSee poetry help completions for full details, but the gist is as simple as using one of the following: Bash # Auto-loaded (recommended) # poetry completions bash >> ~/.bash_completion Lazy-loaded # poetry completions bash > ${ XDG_DATA_HOME :- ~/.local/share } /bash-completion/completions/poetry Fish # poetry completions fish > ~/.config/fish/completions/poetry.fish Zsh # poetry completions zsh > ~/.zfunc/_poetry You must then add the following lines in your ~/.zshrc , if they do not already exist: fpath += ~/.zfunc\nautoload -Uz compinit && compinit Oh My Zsh # mkdir $ZSH_CUSTOM /plugins/poetry\npoetry completions zsh > $ZSH_CUSTOM /plugins/poetry/_poetry You must then add poetry to your plugins array in ~/.zshrc : plugins(\n\tpoetry\n\t...\n\t) prezto # poetry completions zsh > ~/.zprezto/modules/completion/external/src/_poetry Note You may need to restart your shell in order for these changes to take effect. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/basic-usage/", "text": "Basic usage | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Project setup Setting a Python Version Initialising a pre-existing project Operating modes Specifying dependencies Using your virtual environment Using poetry run Activating the virtual environment Version constraints Installing dependencies Installing without poetry.lock Installing with poetry.lock Committing your poetry.lock file to version control Installing dependencies only Updating dependencies to their latest versions Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Basic usage Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Basic usage Project setup Setting a Python Version Initialising a pre-existing project Operating modes Specifying dependencies Using your virtual environment Using poetry run Activating the virtual environment Version constraints Installing dependencies Installing without poetry.lock Installing with poetry.lock Committing your poetry.lock file to version control Installing dependencies only Updating dependencies to their latest versions Basic usage # For the basic usage introduction we will be installing pendulum , a datetime library.\nIf you have not yet installed Poetry, refer to the Introduction chapter. Project setup # First, let‚Äôs create our new project, let‚Äôs call it poetry-demo : poetry new poetry-demo This will create the poetry-demo directory with the following content: poetry-demo\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ poetry_demo\n‚îÇ   ‚îî‚îÄ‚îÄ __init__.py\n‚îî‚îÄ‚îÄ tests\n    ‚îî‚îÄ‚îÄ __init__.py The pyproject.toml file is what is the most important here. This will orchestrate\nyour project and its dependencies. For now, it looks like this: [ tool . poetry ] name = \"poetry-demo\" version = \"0.1.0\" description = \"\" authors = [ \"S√©bastien Eustace <sebastien@eustace.io>\" ] readme = \"README.md\" packages = [{ include = \"poetry_demo\" }] [ tool . poetry . dependencies ] python = \"^3.7\" [ build - system ] requires = [ \"poetry-core\" ] build - backend = \"poetry.core.masonry.api\" Poetry assumes your package contains a package with the same name as tool.poetry.name located in the root of your\nproject. If this is not the case, populate tool.poetry.packages to specify\nyour packages and their locations. Similarly, the traditional MANIFEST.in file is replaced by the tool.poetry.readme , tool.poetry.include , and tool.poetry.exclude sections. tool.poetry.exclude is additionally implicitly populated by your .gitignore . For\nfull documentation on the project format, see the pyproject section of the documentation. Setting a Python Version # Note Unlike with other packages, Poetry will not automatically install a python interpreter for you.\nIf you want to run Python files in your package like a script or application, you must bring your own python interpreter to run them. Poetry will require you to explicitly specify what versions of Python you intend to support, and its universal locking\nwill guarantee that your project is installable (and all dependencies claim support for) all supported Python versions.\nAgain, it‚Äôs important to remember that ‚Äì unlike other dependencies ‚Äì setting a Python version is merely specifying which versions of Python you intend to support. For example, in this pyproject.toml file: [ tool . poetry . dependencies ] python = \"^3.7.0\" we are allowing any version of Python 3 that is greater than 3.7.0 . When you run poetry install , you must have access to some version of a Python inrepreter that satisfies this constraint available on your system.\nPoetry will not install a Python interpreter for you.\nIf you use a tool like pyenv , you can use the experimental configuration value virtualenvs.prefer-active-python . Initialising a pre-existing project # Instead of creating a new project, Poetry can be used to ‚Äòinitialise‚Äô a pre-populated\ndirectory. To interactively create a pyproject.toml file in directory pre-existing-project : cd pre-existing-project\npoetry init Operating modes # Poetry can be operated in two different modes. The default mode is the package mode , which is the right mode\nif you want to package your project into an sdist or a wheel and perhaps publish it to a package index.\nIn this mode, some metadata such as name and version , which are required for packaging, are mandatory.\nFurther, the project itself will be installed in editable mode when running poetry install . If you want to use Poetry only for dependency management but not for packaging, you can use the non-package mode : [ tool . poetry ] package - mode = false In this mode, metadata such as name and version are optional.\nTherefore, it is not possible to build a distribution or publish the project to a package index.\nFurther, when running poetry install , Poetry does not try to install the project itself,\nbut only its dependencies (same as poetry install --no-root ). Note In the pyproject section you can see which fields are required in package mode. Specifying dependencies # If you want to add dependencies to your project, you can specify them in the tool.poetry.dependencies section. [ tool . poetry . dependencies ] pendulum = \"^2.1\" As you can see, it takes a mapping of package names and version constraints . Poetry uses this information to search for the right set of files in package ‚Äúrepositories‚Äù that you register\nin the tool.poetry.source section, or on PyPI by default. Also, instead of modifying the pyproject.toml file by hand, you can use the add command. $ poetry add pendulum It will automatically find a suitable version constraint and install the package and sub-dependencies. Poetry supports a rich dependency specification syntax, including caret,\ntilde, wildcard, inequality and multiple constraints requirements. Using your virtual environment # By default, Poetry creates a virtual environment in {cache-dir}/virtualenvs .\nYou can change the cache-dir value\nby editing the Poetry configuration.\nAdditionally, you can use the virtualenvs.in-project configuration variable to create\nvirtual environments within your project directory. There are several ways to run commands within this virtual environment. Note External virtual environment management Poetry will detect and respect an existing virtual environment that has been externally activated. This is a powerful\nmechanism that is intended to be an alternative to Poetry‚Äôs built-in, simplified environment management. To take advantage of this, simply activate a virtual environment using your preferred method or tooling, before running\nany Poetry commands that expect to manipulate an environment. Using poetry run # To run your script simply use poetry run python your_script.py .\nLikewise if you have command line tools such as pytest or black you can run them using poetry run pytest . Note If managing your own virtual environment externally, you do not need to use poetry run or poetry shell since\nyou will, presumably, already have activated that virtual environment and made available the correct python instance.\nFor example, these commands should output the same python path: conda activate your_env_name\nwhich python\npoetry run which python\npoetry shell\nwhich python Activating the virtual environment # The easiest way to activate the virtual environment is to create a nested shell with poetry shell . To deactivate the virtual environment and exit this new shell type exit .\nTo deactivate the virtual environment without leaving the shell use deactivate . Note Why a nested shell? Child processes inherit their environment from their parents, but do not share\nthem. As such, any modifications made by a child process is not persisted after\nthe child process exits. A Python application (Poetry), being a child process,\ncannot modify the environment of the shell that it has been called from such\nthat an activated virtual environment remains active after the Poetry command\nhas completed execution. Therefore, Poetry has to create a sub-shell with the virtual environment activated\nin order for the subsequent commands to run from within the virtual environment. If you‚Äôd like to prevent poetry shell from modifying your shell prompt on virtual environment activation, you should\nset VIRTUAL_ENV_DISABLE_PROMPT=1 as an environment variable before running the command. Alternatively, to avoid creating a new shell, you can manually activate the\nvirtual environment by running source {path_to_venv}/bin/activate ( {path_to_venv}\\Scripts\\activate.ps1 in PowerShell).\nTo get the path to your virtual environment run poetry env info --path .\nYou can also combine these into a one-liner, such as source $(poetry env info --path)/bin/activate ( & ((poetry env info --path) + \"\\Scripts\\activate.ps1\") in Powershell). To deactivate this virtual environment simply use deactivate . POSIX Shell Windows (PowerShell) Exit/Deactivate Sub-shell poetry shell poetry shell exit Manual Activation source {path_to_venv}/bin/activate {path_to_venv}\\Scripts\\activate.ps1 deactivate One-liner source $(poetry env info --path)/bin/activate & ((poetry env info --path) + \"\\Scripts\\activate.ps1\") deactivate Version constraints # In our example, we are requesting the pendulum package with the version constraint ^2.1 .\nThis means any version greater or equal to 2.1.0 and less than 3.0.0 ( >=2.1.0 <3.0.0 ). Please read Dependency specification for more in-depth information on versions, how versions relate to each other, and on the different ways you can specify\ndependencies. Note How does Poetry download the right files? When you specify a dependency in pyproject.toml , Poetry first takes the name of the package\nthat you have requested and searches for it in any repository you have registered using the repositories key.\nIf you have not registered any extra repositories, or it does not find a package with that name in the\nrepositories you have specified, it falls back to PyPI. When Poetry finds the right package, it then attempts to find the best match for the version constraint you have\nspecified. Installing dependencies # To install the defined dependencies for your project, just run the install command. poetry install When you run this command, one of two things may happen: Installing without poetry.lock # If you have never run the command before and there is also no poetry.lock file present,\nPoetry simply resolves all dependencies listed in your pyproject.toml file and downloads the latest version of their files. When Poetry has finished installing, it writes all the packages and their exact versions that it downloaded to the poetry.lock file,\nlocking the project to those specific versions.\nYou should commit the poetry.lock file to your project repo so that all people working on the project are locked to the same versions of dependencies (more below). Installing with poetry.lock # This brings us to the second scenario. If there is already a poetry.lock file as well as a pyproject.toml file\nwhen you run poetry install , it means either you ran the install command before,\nor someone else on the project ran the install command and committed the poetry.lock file to the project (which is good). Either way, running install when a poetry.lock file is present resolves and installs all dependencies that you listed in pyproject.toml ,\nbut Poetry uses the exact versions listed in poetry.lock to ensure that the package versions are consistent for everyone working on your project.\nAs a result you will have all dependencies requested by your pyproject.toml file,\nbut they may not all be at the very latest available versions\n(some dependencies listed in the poetry.lock file may have released newer versions since the file was created).\nThis is by design, it ensures that your project does not break because of unexpected changes in dependencies. Committing your poetry.lock file to version control # As an application developer # Application developers commit poetry.lock to get more reproducible builds. Committing this file to VC is important because it will cause anyone who sets up the project\nto use the exact same versions of the dependencies that you are using.\nYour CI server, production machines, other developers in your team,\neverything and everyone runs on the same dependencies,\nwhich mitigates the potential for bugs affecting only some parts of the deployments.\nEven if you develop alone, in six months when reinstalling the project you can feel confident\nthe dependencies installed are still working even if your dependencies released many new versions since then.\n(See note below about using the update command.) Warning If you have added the recommended [build-system] section to your project‚Äôs pyproject.toml then you can successfully install your project and its dependencies into a virtual environment using a command like pip install -e . . However, pip will not use the lock file to determine dependency versions as the poetry-core build system is intended for library developers (see next section). As a library developer # Library developers have more to consider. Your users are application developers, and your library will run in a Python environment you don‚Äôt control. The application ignores your library‚Äôs lock file. It can use whatever dependency version meets the constraints in your pyproject.toml . The application will probably use the latest compatible dependency version. If your library‚Äôs poetry.lock falls behind some new dependency version that breaks things for your users, you‚Äôre likely to be the last to find out about it. A simple way to avoid such a scenario is to omit the poetry.lock file. However, by doing so, you sacrifice reproducibility and performance to a certain extent. Without a lockfile, it can be difficult to find the reason for failing tests, because in addition to obvious code changes an unnoticed library update might be the culprit. Further, Poetry will have to lock before installing a dependency if poetry.lock has been omitted. Depending on the number of dependencies, locking may take a significant amount of time. If you do not want to give up the reproducibility and performance benefits, consider a regular refresh of poetry.lock to stay up-to-date and reduce the risk of sudden breakage for users. Installing dependencies only # The current project is installed in editable mode by default. If you want to install the dependencies only, run the install command with the --no-root flag: poetry install --no-root Updating dependencies to their latest versions # As mentioned above, the poetry.lock file prevents you from automatically getting the latest versions\nof your dependencies.\nTo update to the latest versions, use the update command.\nThis will fetch the latest matching versions (according to your pyproject.toml file)\nand update the lock file with the new versions.\n(This is equivalent to deleting the poetry.lock file and running install again.) Note Poetry will display a Warning when executing an install command if poetry.lock and pyproject.toml are not synchronized. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/cli/", "text": "Commands | Documentation | Poetry - Python dependency management and packaging made easy Home 1.8 main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction Basic usage Managing dependencies Libraries Commands Global options new Options init Options install Options update Options add Options remove Options show Options build Options publish Options config Usage Options run shell check Options search lock Options version Options export Options env cache cache list cache clear source source add source show source remove about help list self self add self update self lock self show self show plugins self remove self install Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Commands Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Commands Global options new Options init Options install Options update Options add Options remove Options show Options build Options publish Options config Usage Options run shell check Options search lock Options version Options export Options env cache cache list cache clear source source add source show source remove about help list self self add self update self lock self show self show plugins self remove self install Commands # You‚Äôve already learned how to use the command-line interface to do some things.\nThis chapter documents all the available commands. To get help from the command-line, simply call poetry to see the complete list of commands,\nthen --help combined with any of those can give you more information. Global options # --verbose (-v|vv|vvv) : Increase the verbosity of messages: ‚Äú-v‚Äù for normal output, ‚Äú-vv‚Äù for more verbose output and ‚Äú-vvv‚Äù for debug. --help (-h) : Display help information. --quiet (-q) : Do not output any message. --ansi : Force ANSI output. --no-ansi : Disable ANSI output. --version (-V) : Display this application version. --no-interaction (-n) : Do not ask any interactive question. --no-plugins : Disables plugins. --no-cache : Disables Poetry source caches. --directory=DIRECTORY (-C) : The working directory for the Poetry command (defaults to the current working directory). new # This command will help you kickstart your new Python project by creating\na directory structure suitable for most projects. poetry new my-package will create a folder as follows: my-package\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ my_package\n‚îÇ   ‚îî‚îÄ‚îÄ __init__.py\n‚îî‚îÄ‚îÄ tests\n    ‚îî‚îÄ‚îÄ __init__.py If you want to name your project differently than the folder, you can pass\nthe --name option: poetry new my-folder --name my-package If you want to use a src folder, you can use the --src option: poetry new --src my-package That will create a folder structure as follows: my-package\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ src\n‚îÇ   ‚îî‚îÄ‚îÄ my_package\n‚îÇ       ‚îî‚îÄ‚îÄ __init__.py\n‚îî‚îÄ‚îÄ tests\n    ‚îî‚îÄ‚îÄ __init__.py The --name option is smart enough to detect namespace packages and create\nthe required structure for you. poetry new --src --name my.package my-package will create the following structure: my-package\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ src\n‚îÇ   ‚îî‚îÄ‚îÄ my\n‚îÇ       ‚îî‚îÄ‚îÄ package\n‚îÇ           ‚îî‚îÄ‚îÄ __init__.py\n‚îî‚îÄ‚îÄ tests\n    ‚îî‚îÄ‚îÄ __init__.py Options # --name : Set the resulting package name. --src : Use the src layout for the project. --readme : Specify the readme file extension. Default is md . If you intend to publish to PyPI\nkeep the recommendations for a PyPI-friendly README in mind. init # This command will help you create a pyproject.toml file interactively\nby prompting you to provide basic information about your package. It will interactively ask you to fill in the fields, while using some smart defaults. poetry init Options # --name : Name of the package. --description : Description of the package. --author : Author of the package. --python Compatible Python versions. --dependency : Package to require with a version constraint. Should be in format foo:1.0.0 . --dev-dependency : Development requirements, see --dependency . install # The install command reads the pyproject.toml file from the current project,\nresolves the dependencies, and installs them. poetry install If there is a poetry.lock file in the current directory,\nit will use the exact versions from there instead of resolving them.\nThis ensures that everyone using the library will get the same versions of the dependencies. If there is no poetry.lock file, Poetry will create one after dependency resolution. If you want to exclude one or more dependency groups for the installation, you can use\nthe --without option. poetry install --without test,docs Note The --no-dev option is now deprecated. You should use the --only main or --without dev notation instead. You can also select optional dependency groups with the --with option. poetry install --with test,docs It‚Äôs also possible to only install specific dependency groups by using the only option. poetry install --only test,docs To only install the project itself with no dependencies, use the --only-root flag. poetry install --only-root See Dependency groups for more information\nabout dependency groups. If you want to synchronize your environment ‚Äì and ensure it matches the lock file ‚Äì use the --sync option. poetry install --sync The --sync can be combined with group-related options: poetry install --without dev --sync\npoetry install --with docs --sync\npoetry install --only dev --sync You can also specify the extras you want installed\nby passing the -E|--extras option (See Extras for more info).\nPass --all-extras to install all defined extras for a project. poetry install --extras \"mysql pgsql\" poetry install -E mysql -E pgsql\npoetry install --all-extras Extras are not sensitive to --sync . Any extras not specified will always be removed. poetry install --extras \"A B\" # C is removed By default poetry will install your project‚Äôs package every time you run install : $ poetry install\nInstalling dependencies from lock file\n\nNo dependencies to install or update\n\n  - Installing <your-package-name> ( x.x.x ) If you want to skip this installation, use the --no-root option. poetry install --no-root Similar to --no-root you can use --no-directory to skip directory path dependencies: poetry install --no-directory This is mainly useful for caching in CI or when building Docker images. See the FAQ entry for more information on this option. By default poetry does not compile Python source files to bytecode during installation.\nThis speeds up the installation process, but the first execution may take a little more\ntime because Python then compiles source files to bytecode automatically.\nIf you want to compile source files to bytecode during installation,\nyou can use the --compile option: poetry install --compile Note The --compile option has no effect if installer.modern-installation is set to false because the old installer always compiles source files to bytecode. Options # --without : The dependency groups to ignore. --with : The optional dependency groups to include. --only : The only dependency groups to include. --only-root : Install only the root project, exclude all dependencies. --sync : Synchronize the environment with the locked packages and the specified groups. --no-root : Do not install the root package (your project). --no-directory : Skip all directory path dependencies (including transitive ones). --dry-run : Output the operations but do not execute anything (implicitly enables ‚Äìverbose). --extras (-E) : Features to install (multiple values allowed). --all-extras : Install all extra features (conflicts with ‚Äìextras). --compile : Compile Python source files to bytecode. --no-dev : Do not install dev dependencies. ( Deprecated , use --only main or --without dev instead) --remove-untracked : Remove dependencies not presented in the lock file. ( Deprecated , use --sync instead) Note When --only is specified, --with and --without options are ignored. update # In order to get the latest versions of the dependencies and to update the poetry.lock file,\nyou should use the update command. poetry update This will resolve all dependencies of the project and write the exact versions into poetry.lock . If you just want to update a few packages and not all, you can list them as such: poetry update requests toml Note that this will not update versions for dependencies outside their version constraints specified in the pyproject.toml file.\nIn other terms, poetry update foo will be a no-op if the version constraint\nspecified for foo is ~2.3 or 2.3 and 2.4 is available.\nIn order for foo to be updated, you must update the constraint, for example ^2.3 .\nYou can do this using the add command. Options # --without : The dependency groups to ignore. --with : The optional dependency groups to include. --only : The only dependency groups to include. --dry-run : Outputs the operations but will not execute anything (implicitly enables ‚Äìverbose). --no-dev : Do not update the development dependencies. ( Deprecated , use --only main or --without dev instead) --lock : Do not perform install (only update the lockfile). --sync : Synchronize the environment with the locked packages and the specified groups. Note When --only is specified, --with and --without options are ignored. add # The add command adds required packages to your pyproject.toml and installs them. If you do not specify a version constraint,\npoetry will choose a suitable one based on the available package versions. poetry add requests pendulum Note A package is looked up, by default, only from the Default Package Source .\nYou can modify the default source (PyPI); or add and use Supplemental Package Sources or Explicit Package Sources . For more information, refer to the Package Sources documentation. You can also specify a constraint when adding a package: # Allow >=2.0.5, <3.0.0 versions poetry add pendulum@^2.0.5 # Allow >=2.0.5, <2.1.0 versions poetry add pendulum@~2.0.5 # Allow >=2.0.5 versions, without upper bound poetry add \"pendulum>=2.0.5\" # Allow only 2.0.5 version poetry add pendulum == 2.0.5 Note See the Dependency specification page for more information about the @ operator. If you try to add a package that is already present, you will get an error.\nHowever, if you specify a constraint, like above, the dependency will be updated\nby using the specified constraint. If you want to get the latest version of an already\npresent dependency, you can use the special latest constraint: poetry add pendulum@latest Note See the Dependency specification for more information on setting the version constraints for a package. You can also add git dependencies: poetry add git+https://github.com/sdispater/pendulum.git or use ssh instead of https: poetry add git+ssh://git@github.com/sdispater/pendulum.git # or alternatively: poetry add git+ssh://git@github.com:sdispater/pendulum.git If you need to checkout a specific branch, tag or revision,\nyou can specify it when using add : poetry add git+https://github.com/sdispater/pendulum.git#develop\npoetry add git+https://github.com/sdispater/pendulum.git#2.0.5 # or using SSH instead: poetry add git+ssh://git@github.com:sdispater/pendulum.git#develop\npoetry add git+ssh://git@github.com:sdispater/pendulum.git#2.0.5 or reference a subdirectory: poetry add git+https://github.com/myorg/mypackage_with_subdirs.git@main#subdirectory = subdir You can also add a local directory or file: poetry add ./my-package/\npoetry add ../my-package/dist/my-package-0.1.0.tar.gz\npoetry add ../my-package/dist/my_package-0.1.0.whl If you want the dependency to be installed in editable mode you can use the --editable option. poetry add --editable ./my-package/\npoetry add --editable git+ssh://github.com/sdispater/pendulum.git#develop Alternatively, you can specify it in the pyproject.toml file. It means that changes in the local directory will be reflected directly in environment. [ tool . poetry . dependencies ] my - package = { path = \"../my/path\" , develop = true } Note Before poetry 1.1 path dependencies were installed in editable mode by default. You should always set the develop attribute explicitly,\nto make sure the behavior is the same for all poetry versions. Note The develop attribute is a Poetry-specific feature, so it is not included in the package distribution metadata.\nIn other words, it is only considered when using Poetry to install the project. If the package(s) you want to install provide extras, you can specify them\nwhen adding the package: poetry add \"requests[security,socks]\" poetry add \"requests[security,socks]~=2.22.0\" poetry add \"git+https://github.com/pallets/flask.git@1.1.1[dotenv,dev]\" Warning Some shells may treat square braces ( [ and ] ) as special characters. It is suggested to always quote arguments containing these characters to prevent unexpected shell expansion. If you want to add a package to a specific group of dependencies, you can use the --group (-G) option: poetry add mkdocs --group docs See Dependency groups for more information\nabout dependency groups. Options # --group (-G) : The group to add the dependency to. --dev (-D) : Add package as development dependency. ( Deprecated , use -G dev instead) --editable (-e) : Add vcs/path dependencies as editable. --extras (-E) : Extras to activate for the dependency. (multiple values allowed) --optional : Add as an optional dependency. --python : Python version for which the dependency must be installed. --platform : Platforms for which the dependency must be installed. --source : Name of the source to use to install the package. --allow-prereleases : Accept prereleases. --dry-run : Output the operations but do not execute anything (implicitly enables ‚Äìverbose). --lock : Do not perform install (only update the lockfile). remove # The remove command removes a package from the current\nlist of installed packages. poetry remove pendulum If you want to remove a package from a specific group of dependencies, you can use the --group (-G) option: poetry remove mkdocs --group docs See Dependency groups for more information\nabout dependency groups. Options # --group (-G) : The group to remove the dependency from. --dev (-D) : Removes a package from the development dependencies. ( Deprecated , use -G dev instead) --dry-run : Outputs the operations but will not execute anything (implicitly enables ‚Äìverbose). --lock : Do not perform operations (only update the lockfile). show # To list all the available packages, you can use the show command. poetry show If you want to see the details of a certain package, you can pass the package name. poetry show pendulum\n\nname        : pendulum\nversion     : 1.4.2\ndescription : Python datetimes made easy\n\ndependencies\n - python-dateutil > = 2.6.1\n - tzlocal > = 1.4\n - pytzdata > = 2017.2.2\n\nrequired by\n - calendar > = 1.4.0 Options # --without : The dependency groups to ignore. --why : When showing the full list, or a --tree for a single package, display whether they are a direct dependency or required by other packages. --with : The optional dependency groups to include. --only : The only dependency groups to include. --no-dev : Do not list the dev dependencies. ( Deprecated , use --only main or --without dev instead) --tree : List the dependencies as a tree. --latest (-l) : Show the latest version. --outdated (-o) : Show the latest version but only for packages that are outdated. --all (-a) : Show all packages (even those not compatible with current system). --top-level (-T) : Only show explicitly defined packages. Note When --only is specified, --with and --without options are ignored. build # The build command builds the source and wheels archives. poetry build Note that, at the moment, only pure python wheels are supported. Options # --format (-f) : Limit the format to either wheel or sdist . --output (-o) : Set output directory for build artifacts. Default is dist . publish # This command publishes the package, previously built with the build command, to the remote repository. It will automatically register the package before uploading if this is the first time it is submitted. poetry publish It can also build the package if you pass it the --build option. Note See Publishable Repositories for more information on how to configure and use publishable repositories. Options # --repository (-r) : The repository to register the package to (default: pypi ).\nShould match a repository name set by the config command. --username (-u) : The username to access the repository. --password (-p) : The password to access the repository. --cert : Certificate authority to access the repository. --client-cert : Client certificate to access the repository. --dist-dir : Dist directory where built artifact are stored. Default is dist . --build : Build the package before publishing. --dry-run : Perform all actions except upload the package. --skip-existing : Ignore errors from files already existing in the repository. Note See Configuring Credentials for more information on how to configure credentials. config # The config command allows you to edit poetry config settings and repositories. poetry config --list Usage # poetry config [ options ] [ setting-key ] [ setting-value1 ] ... [ setting-valueN ] setting-key is a configuration option name and setting-value1 is a configuration value.\nSee Configuration for all available settings. Warning Use -- to terminate option parsing if your values may start with a hyphen ( - ), e.g. poetry config http-basic.custom-repo gitlab-ci-token -- ${ GITLAB_JOB_TOKEN } Without -- this command will fail if ${GITLAB_JOB_TOKEN} starts with a hyphen. Options # --unset : Remove the configuration element named by setting-key . --list : Show the list of current config variables. --local : Set/Get settings that are specific to a project (in the local configuration file poetry.toml ). run # The run command executes the given command inside the project‚Äôs virtualenv. poetry run python -V It can also execute one of the scripts defined in pyproject.toml . So, if you have a script defined like this: [ tool . poetry . scripts ] my - script = \"my_module:main\" You can execute it like so: poetry run my-script Note that this command has no option. shell # The shell command spawns a shell within the project‚Äôs virtual environment. By default, the current active shell is detected and used. Failing that,\nthe shell defined via the environment variable SHELL (on *nix) or COMSPEC (on Windows) is used. If a virtual environment does not exist, it will be created. poetry shell Note that this command starts a new shell and activates the virtual environment. As such, exit should be used to properly exit the shell and the virtual environment instead of deactivate . Note Poetry internally uses the Shellingham project to detect current\nactive shell. check # The check command validates the content of the pyproject.toml file\nand its consistency with the poetry.lock file.\nIt returns a detailed report if there are any errors. Note This command is also available as a pre-commit hook. See pre-commit hooks for more information. poetry check Options # --lock : Verifies that poetry.lock exists for the current pyproject.toml . search # This command searches for packages on a remote index. poetry search requests pendulum lock # This command locks (without installing) the dependencies specified in pyproject.toml . Note By default, this will lock all dependencies to the latest available compatible versions. To only refresh the lock file, use the --no-update option.\nThis command is also available as a pre-commit hook. See pre-commit hooks for more information. poetry lock Options # --check : Verify that poetry.lock is consistent with pyproject.toml . ( Deprecated ) Use poetry check --lock instead. --no-update : Do not update locked versions, only refresh lock file. version # This command shows the current version of the project or bumps the version of\nthe project and writes the new version back to pyproject.toml if a valid\nbump rule is provided. The new version should be a valid PEP 440 string or a valid bump rule: patch , minor , major , prepatch , preminor , premajor , prerelease . Note If you would like to use semantic versioning for your project, please see here . The table below illustrates the effect of these rules with concrete examples. rule before after major 1.3.0 2.0.0 minor 2.1.4 2.2.0 patch 4.1.1 4.1.2 premajor 1.0.2 2.0.0a0 preminor 1.0.2 1.1.0a0 prepatch 1.0.2 1.0.3a0 prerelease 1.0.2 1.0.3a0 prerelease 1.0.3a0 1.0.3a1 prerelease 1.0.3b0 1.0.3b1 The option --next-phase allows the increment of prerelease phase versions. rule before after prerelease ‚Äìnext-phase 1.0.3a0 1.0.3b0 prerelease ‚Äìnext-phase 1.0.3b0 1.0.3rc0 prerelease ‚Äìnext-phase 1.0.3rc0 1.0.3 Options # --next-phase : Increment the phase of the current version. --short (-s) : Output the version number only. --dry-run : Do not update pyproject.toml file. export # This command exports the lock file to other formats. poetry export -f requirements.txt --output requirements.txt Warning This command is provided by the Export Poetry Plugin .\nIn a future version of Poetry this plugin will not be installed by default anymore.\nIn order to avoid a breaking change and make your automation forward-compatible,\nplease install poetry-plugin-export explicitly.\nSee Using plugins for details on how to install a plugin. Note This command is also available as a pre-commit hook.\nSee pre-commit hooks for more information. Note Unlike the install command, this command only includes the project‚Äôs dependencies defined in the implicit main group defined in tool.poetry.dependencies when used without specifying any options. Options # --format (-f) : The format to export to (default: requirements.txt ).\nCurrently, only constraints.txt and requirements.txt are supported. --output (-o) : The name of the output file. If omitted, print to standard\noutput. --dev : Include development dependencies. ( Deprecated , use --with dev instead) --extras (-E) : Extra sets of dependencies to include. --without : The dependency groups to ignore. --with : The optional dependency groups to include. --only : The only dependency groups to include. --without-hashes : Exclude hashes from the exported file. --without-urls : Exclude source repository urls from the exported file. --with-credentials : Include credentials for extra indices. env # The env command regroups sub commands to interact with the virtualenvs\nassociated with a specific project. See Managing environments for more information about these commands. cache # The cache command regroups sub commands to interact with Poetry‚Äôs cache. cache list # The cache list command lists Poetry‚Äôs available caches. poetry cache list cache clear # The cache clear command removes packages from a cached repository. For example, to clear the whole cache of packages from the pypi repository, run: poetry cache clear pypi --all To only remove a specific package from a cache, you have to specify the cache entry in the following form cache:package:version : poetry cache clear pypi:requests:2.24.0 source # The source namespace regroups sub commands to manage repository sources for a Poetry project. source add # The source add command adds source configuration to the project. For example, to add the pypi-test source, you can run: poetry source add pypi-test https://test.pypi.org/simple/ You cannot use the name pypi for a custom repository as it is reserved for use by\nthe default PyPI source. However, you can set the priority of PyPI: poetry source add --priority = explicit pypi Options # --default : Set this source as the default (disable PyPI). Deprecated in favor of --priority . --secondary : Set this source as a secondary source. Deprecated in favor of --priority . --priority : Set the priority of this source. Accepted values are: default , secondary , supplemental , and explicit . Refer to the dedicated sections in Repositories for more information. Note At most one of the options above can be provided. See package sources for more information. source show # The source show command displays information on all configured sources for the project. poetry source show Optionally, you can show information of one or more sources by specifying their names. poetry source show pypi-test Note This command will only show sources configured via the pyproject.toml and does not include the implicit default PyPI. source remove # The source remove command removes a configured source from your pyproject.toml . poetry source remove pypi-test about # The about command displays global information about Poetry, including the current version and version of poetry-core . poetry about help # The help command displays global help, or help for a specific command. To display global help: poetry help To display help for a specific command, for instance show : poetry help show Note The --help option can also be passed to any command to get help for a specific command. For instance: poetry show --help list # The list command displays all the available Poetry commands. poetry list self # The self namespace regroups sub commands to manage the Poetry installation itself. Note Use of these commands will create the required pyproject.toml and poetry.lock files in your configuration directory . Warning Especially on Windows, self commands that update or remove packages may be problematic\nso that other methods for installing plugins and updating Poetry are recommended.\nSee Using plugins and Installing Poetry for more information. self add # The self add command installs Poetry plugins and make them available at runtime. Additionally, it can\nalso be used to upgrade Poetry‚Äôs own dependencies or inject additional packages into the runtime\nenvironment Note The self add command works exactly like the add command . However, is different in that the packages\nmanaged are for Poetry‚Äôs runtime environment. The package specification formats supported by the self add command are the same as the ones supported\nby the add command . For example, to install the poetry-plugin-export plugin, you can run: poetry self add poetry-plugin-export To update to the latest poetry-core version, you can run: poetry self add poetry-core@latest To add a keyring provider artifacts-keyring , you can run: poetry self add artifacts-keyring Options # --editable (-e) : Add vcs/path dependencies as editable. --extras (-E) : Extras to activate for the dependency. (multiple values allowed) --allow-prereleases : Accept prereleases. --source : Name of the source to use to install the package. --dry-run : Output the operations but do not execute anything (implicitly enables ‚Äìverbose). self update # The self update command updates Poetry version in its current runtime environment. Note The self update command works exactly like the update command . However,\nis different in that the packages managed are for Poetry‚Äôs runtime environment. poetry self update Options # --preview : Allow the installation of pre-release versions. --dry-run : Output the operations but do not execute anything (implicitly enables ‚Äìverbose). self lock # The self lock command reads this Poetry installation‚Äôs system pyproject.toml file. The system\ndependencies are locked in the corresponding poetry.lock file. poetry self lock Options # --check : Verify that poetry.lock is consistent with pyproject.toml . ( Deprecated ) --no-update : Do not update locked versions, only refresh lock file. self show # The self show command behaves similar to the show command, but\nworking within Poetry‚Äôs runtime environment. This lists all packages installed within\nthe Poetry install environment. To show only additional packages that have been added via self add and their\ndependencies use self show --addons . poetry self show Options # --addons : List only add-on packages installed. --tree : List the dependencies as a tree. --latest (-l) : Show the latest version. --outdated (-o) : Show the latest version but only for packages that are outdated. self show plugins # The self show plugins command lists all the currently installed plugins. poetry self show plugins self remove # The self remove command removes an installed addon package. poetry self remove poetry-plugin-export Options # --dry-run : Outputs the operations but will not execute anything (implicitly enables ‚Äìverbose). self install # The self install command ensures all additional packages specified are installed in the current\nruntime environment. Note The self install command works similar to the install command . However,\nis different in that the packages managed are for Poetry‚Äôs runtime environment. poetry self install --sync Options # --sync : Synchronize the environment with the locked packages and the specified groups. --dry-run : Output the operations but do not execute anything (implicitly enables ‚Äìverbose). Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://python-poetry.org/docs/main/", "text": "Introduction | main | Documentation | Poetry - Python dependency management and packaging made easy Home main main 1.8 1.7 Use dark mode Open menu Documentation Introduction Poetry is a tool for dependency management and packaging in Python. Basic usage For the basic usage introduction we will be installing pendulum, a datetime library. Managing dependencies Dependency groups Poetry provides a way to organize your dependencies by groups. Libraries This chapter will tell you how to make your library installable through Poetry.\nVersioning Poetry requires PEP 440-compliant versions for all projects.\nWhile Poetry does not enforce any release convention, it used to encourage the use of semantic versioning within the scope of PEP 440 and supports version constraints that are especially suitable for semver.\nNote As an example, 1.0.0-hotfix.1 is not compatible with PEP 440. Commands You‚Äôve already learned how to use the command-line interface to do some things. Configuration Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically created when you first run that command. Repositories Poetry supports the use of PyPI and private repositories for discovery of packages as well as for publishing your projects.\nBy default, Poetry is configured to use the PyPI repository, for package installation and publishing.\nSo, when you add dependencies to your project, Poetry will assume they are available on PyPI.\nThis represents most cases and will likely be enough for most users.\nPrivate Repository Example Installing from private package sources By default, Poetry discovers and installs packages from PyPI.. Managing environments Poetry makes project environment isolation one of its core features.\nWhat this means is that it will always work isolated from your global Python installation. Dependency specification Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.\nVersion constraints Caret requirements Caret requirements allow SemVer compatible updates to a specified version. Plugins Poetry supports using and building plugins if you wish to alter or expand Poetry‚Äôs functionality with your own.\nFor example if your environment poses special requirements on the behaviour of Poetry which do not apply to the majority of its users or if you wish to accomplish something with Poetry in a way that is not desired by most users.\nIn these cases you could consider creating a plugin to handle your specific logic.. The pyproject.toml file The tool.poetry section of the pyproject.toml file is composed of multiple sections.\npackage-mode Whether Poetry operates in package mode (default) or not. Contributing to Poetry First off, thanks for taking the time to contribute!\nThe following is a set of guidelines for contributing to Poetry on GitHub. Community Badge For any projects using Poetry, you may add its official badge somewhere prominent like the README.\nMarkdown\n[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/) reStructuredText\n.. FAQ Why is the dependency resolution process slow? While the dependency resolver at the heart of Poetry is highly optimized and should be fast enough for most cases, with certain sets of dependencies it can take time to find a valid solution.\nThis is due to the fact that not all libraries on PyPI have properly declared their metadata and, as such, they are not available via the PyPI JSON API.. pre-commit hooks pre-commit is a framework for building and running git hooks. 1.8 Stable Documentation for the latest, stable , branch. main Development Documentation for the latest, in-development , branch. Blog History Close menu Documentation Blog History Introduction System requirements Installation Enable tab completion for Bash, Fish, or Zsh Bash Fish Zsh Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Introduction Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Introduction System requirements Installation Enable tab completion for Bash, Fish, or Zsh Bash Fish Zsh Introduction # Poetry is a tool for dependency management and packaging in Python.\nIt allows you to declare the libraries your project depends on and it will manage (install/update) them for you.\nPoetry offers a lockfile to ensure repeatable installs, and can build your project for distribution. System requirements # Poetry requires Python 3.8+ . It is multi-platform and the goal is to make it work equally well\non Linux, macOS and Windows. Installation # Warning Poetry should always be installed in a dedicated virtual environment to isolate it from the rest of your system.\nIt should in no case be installed in the environment of the project that is to be managed by Poetry.\nThis ensures that Poetry‚Äôs own dependencies will not be accidentally upgraded or uninstalled.\n(Each of the following installation methods ensures that Poetry is installed into an isolated environment.)\nIn addition, the isolated virtual environment in which poetry is installed should not be activated for running poetry commands. Note If you are viewing documentation for the development branch, you may wish to install a preview or development version of Poetry.\nSee the advanced installation instructions to use a preview or alternate version of Poetry. With pipx With pipx With the official installer Manually (advanced) CI recommendations With pipx With the official installer Manually (advanced) CI recommendations pipx is used to install Python CLI applications globally while still isolating them in virtual environments. pipx will manage upgrades and uninstalls when used to install Poetry. Install pipx If pipx is not already installed, you can follow any of the options in the official pipx installation instructions .\nAny non-ancient version of pipx will do. Install Poetry pipx install poetry Install Poetry (advanced) Note You can skip this step, if you simply want the latest version and already installed Poetry as described in the\nprevious step. This step details advanced usages of this installation method. For example, installing Poetry from\nsource, having multiple versions installed at the same time etc. pipx can install different versions of Poetry, using the same syntax as pip: pipx install poetry == 1.2.0 pipx can also install versions of Poetry in parallel, which allows for easy testing of alternate or prerelease\nversions. Each version is given a unique, user-specified suffix, which will be used to create a unique binary name: pipx install --suffix = @1.2.0 poetry == 1.2.0\npoetry@1.2.0 --version pipx install --suffix = @preview --pip-args = --pre poetry\npoetry@preview --version Finally, pipx can install any valid pip requirement spec , which\nallows for installations of the development version from git , or even for local testing of pull requests: pipx install --suffix @main git+https://github.com/python-poetry/poetry.git@main\npipx install --suffix @pr1234 git+https://github.com/python-poetry/poetry.git@refs/pull/1234/head Update Poetry pipx upgrade poetry Uninstall Poetry pipx uninstall poetry We provide a custom installer that will install Poetry in a new virtual environment\nand allows Poetry to manage its own environment. Install Poetry The installer script is available directly at install.python-poetry.org ,\nand is developed in its own repository .\nThe script can be executed directly (i.e. ‚Äòcurl python‚Äô) or downloaded and then executed from disk\n(e.g. in a CI environment). Warning The install-poetry.py installer has been deprecated and removed from the Poetry repository.\nPlease migrate from the in-tree version to the standalone version described above. Linux, macOS, Windows (WSL) curl -sSL https://install.python-poetry.org | python3 - Note Note: On some systems, python may still refer to Python 2 instead of Python 3. We always suggest the python3 binary to avoid ambiguity. Windows (Powershell) ( Invoke-WebRequest -Uri https : // install . python-poetry . org -UseBasicParsing ). Content | py - Note If you have installed Python through the Microsoft Store, replace py with python in the command\nabove. Install Poetry (advanced) Note You can skip this step, if you simply want the latest version and already installed Poetry as described in the\nprevious step. This step details advanced usages of this installation method. For example, installing Poetry from\nsource, using a pre-release build, configuring a different installation location etc. By default, Poetry is installed into a platform and user-specific directory: ~/Library/Application Support/pypoetry on macOS. ~/.local/share/pypoetry on Linux/Unix. %APPDATA%\\pypoetry on Windows. If you wish to change this, you may define the $POETRY_HOME environment variable: curl -sSL https://install.python-poetry.org | POETRY_HOME = /etc/poetry python3 - If you want to install prerelease versions, you can do so by passing the --preview option to the installation script\nor by using the $POETRY_PREVIEW environment variable: curl -sSL https://install.python-poetry.org | python3 - --preview\ncurl -sSL https://install.python-poetry.org | POETRY_PREVIEW = 1 python3 - Similarly, if you want to install a specific version, you can use --version option or the $POETRY_VERSION environment variable: curl -sSL https://install.python-poetry.org | python3 - --version 1.2.0\ncurl -sSL https://install.python-poetry.org | POETRY_VERSION = 1.2.0 python3 - You can also install Poetry from a git repository by using the --git option: curl -sSL https://install.python-poetry.org | python3 - --git https://github.com/python-poetry/poetry.git@main If you want to install different versions of Poetry in parallel, a good approach is the installation with pipx and suffix. Add Poetry to your PATH The installer creates a poetry wrapper in a well-known, platform-specific directory: $HOME/.local/bin on Unix. %APPDATA%\\Python\\Scripts on Windows. $POETRY_HOME/bin if $POETRY_HOME is set. If this directory is not present in your $PATH , you can add it in order to invoke Poetry\nas poetry . Alternatively, the full path to the poetry binary can always be used: ~/Library/Application Support/pypoetry/venv/bin/poetry on macOS. ~/.local/share/pypoetry/venv/bin/poetry on Linux/Unix. %APPDATA%\\pypoetry\\venv\\Scripts\\poetry on Windows. $POETRY_HOME/venv/bin/poetry if $POETRY_HOME is set. Use Poetry Once Poetry is installed and in your $PATH , you can execute the following: poetry --version If you see something like Poetry (version 1.2.0) , your installation is ready to use! Update Poetry Poetry is able to update itself when installed using the official installer. Warning Especially on Windows, self update may be problematic\nso that a re-install with the installer should be preferred. poetry self update If you want to install pre-release versions, you can use the --preview option. poetry self update --preview And finally, if you want to install a specific version, you can pass it as an argument\nto self update . poetry self update 1.2.0 Warning Poetry 1.1 series releases are not able to update in-place to 1.2 or newer series releases.\nTo migrate to newer releases, uninstall using your original install method, and then reinstall\nusing the methods above . Uninstall Poetry If you decide Poetry isn‚Äôt your thing, you can completely remove it from your system\nby running the installer again with the --uninstall option or by setting\nthe POETRY_UNINSTALL environment variable before executing the installer. curl -sSL https://install.python-poetry.org | python3 - --uninstall\ncurl -sSL https://install.python-poetry.org | POETRY_UNINSTALL = 1 python3 - Warning If you installed using the deprecated get-poetry.py script, you should remove the path it uses manually, e.g. rm -rf \" ${ POETRY_HOME :- ~/.poetry } \" Also remove ~/.poetry/bin from your $PATH in your shell configuration, if it is present. Poetry can be installed manually using pip and the venv module. By doing so you will essentially perform the steps carried\nout by the official installer. As this is an advanced installation method, these instructions are Unix-only and omit specific\nexamples such as installing from git . The variable $VENV_PATH will be used to indicate the path at which the virtual environment was created. python3 -m venv $VENV_PATH $VENV_PATH /bin/pip install -U pip setuptools $VENV_PATH /bin/pip install poetry Poetry will be available at $VENV_PATH/bin/poetry and can be invoked directly or symlinked elsewhere. To uninstall Poetry, simply delete the entire $VENV_PATH directory. Unlike development environments, where making use of the latest tools is desirable, in a CI environment reproducibility\nshould be made the priority. Here are some suggestions for installing Poetry in such an environment. Version pinning Whatever method you use, it is highly recommended to explicitly control the version of Poetry used, so that you are able\nto upgrade after performing your own validation. Each install method has a different syntax for setting the version that\nis used in the following examples. Using pipx Just as pipx is a powerful tool for development use, it is equally useful in a CI environment\nand should be one of your top choices for use of Poetry in CI. pipx install poetry == 1.2.0 Using install.python-poetry.org Note The official installer script ( install.python-poetry.org ) offers a streamlined and\nsimplified installation of Poetry, sufficient for developer use or for simple pipelines. However, in a CI environment\nthe other two supported installation methods (pipx and manual) should be seriously considered. Downloading a copy of the installer script to a place accessible by your CI pipelines (or maintaining a copy of the repository ) is strongly suggested, to ensure your\npipeline‚Äôs stability and to maintain control over what code is executed. By default, the installer will install to a user-specific directory. In more complex pipelines that may make accessing\nPoetry difficult (especially in cases like multi-stage container builds). It is highly suggested to make use of $POETRY_HOME when using the official installer in CI, as that way the exact paths can be controlled. export POETRY_HOME = /opt/poetry\npython3 install-poetry.py --version 1.2.0 $POETRY_HOME /bin/poetry --version Using pip (aka manually) For maximum control in your CI environment, installation with pip is fully supported and something you should\nconsider. While this requires more explicit commands and knowledge of Python packaging from you, it in return offers the\nbest debugging experience, and leaves you subject to the fewest external tools. export POETRY_HOME = /opt/poetry\npython3 -m venv $POETRY_HOME $POETRY_HOME /bin/pip install poetry == 1.2.0 $POETRY_HOME /bin/poetry --version Note If you install Poetry via pip , ensure you have Poetry installed into an isolated environment that is not the same as the target environment managed by Poetry. If Poetry and your project are installed into the same environment, Poetry\nis likely to upgrade or uninstall its own dependencies (causing hard-to-debug and understand errors). Enable tab completion for Bash, Fish, or Zsh # poetry supports generating completion scripts for Bash, Fish, and Zsh.\nSee poetry help completions for full details, but the gist is as simple as using one of the following: Bash # Auto-loaded (recommended) # poetry completions bash >> ~/.bash_completion Lazy-loaded # poetry completions bash > ${ XDG_DATA_HOME :- ~/.local/share } /bash-completion/completions/poetry Fish # poetry completions fish > ~/.config/fish/completions/poetry.fish Zsh # poetry completions zsh > ~/.zfunc/_poetry You must then add the following lines in your ~/.zshrc , if they do not already exist: fpath += ~/.zfunc\nautoload -Uz compinit && compinit Oh My Zsh # mkdir $ZSH_CUSTOM /plugins/poetry\npoetry completions zsh > $ZSH_CUSTOM /plugins/poetry/_poetry You must then add poetry to your plugins array in ~/.zshrc : plugins(\n\tpoetry\n\t...\n\t) prezto # poetry completions zsh > ~/.zprezto/modules/completion/external/src/_poetry Note You may need to restart your shell in order for these changes to take effect. Footer Python packaging and dependency management made easy. GitHub Discord Documentation Introduction Basic usage Managing dependencies Libraries Commands Configuration Repositories Managing environments Dependency specification Plugins The pyproject.toml file Contributing to Poetry Community FAQ pre-commit hooks Github Project Issues Discussions Other Projects poetry-core install.python-poetry.org Bundle plugin Export plugin Copyright ¬© 2018-2024. All Rights\nReserved. Powered by"},
{"url": "https://github.com/python-poetry/poetry", "text": "GitHub - python-poetry/poetry: Python packaging and dependency management made easy Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert python-poetry / poetry Public Notifications You must be signed in to change notification settings Fork 2.2k Star 30.7k Python packaging and dependency management made easy python-poetry.org License MIT license 30.7k stars 2.2k forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 604 Pull requests 75 Discussions Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights python-poetry/poetry This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. main Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit History 3,291 Commits .github .github assets assets docs docs src/ poetry src/ poetry tests tests .cirrus.yml .cirrus.yml .gitattributes .gitattributes .gitignore .gitignore .pre-commit-config.yaml .pre-commit-config.yaml .pre-commit-hooks.yaml .pre-commit-hooks.yaml CHANGELOG.md CHANGELOG.md CITATION.cff CITATION.cff CODE_OF_CONDUCT.md CODE_OF_CONDUCT.md LICENSE LICENSE README.md README.md poetry.lock poetry.lock pyproject.toml pyproject.toml View all files Repository files navigation README Code of conduct MIT license Poetry: Python packaging and dependency management made easy Poetry helps you declare, manage and install dependencies of Python projects,\nensuring you have the right stack everywhere. Poetry replaces setup.py , requirements.txt , setup.cfg , MANIFEST.in and Pipfile with a simple pyproject.toml based project format. [ tool . poetry ] name = \" my-package \" version = \" 0.1.0 \" description = \" The description of the package \" license = \" MIT \" authors = [ \" S√©bastien Eustace <sebastien@eustace.io> \" ] repository = \" https://github.com/python-poetry/poetry \" homepage = \" https://python-poetry.org \" # README file(s) are used as the package description readme = [ \" README.md \" , \" LICENSE \" ] # Keywords (translated to tags on the package index) keywords = [ \" packaging \" , \" poetry \" ]\n\n[ tool . poetry . dependencies ] # Compatible Python versions python = \" >=3.8 \" # Standard dependency with semver constraints aiohttp = \" ^3.8.1 \" # Dependency with extras requests = { version = \" ^2.28 \" , extras = [ \" security \" ] } # Version-specific dependencies with prereleases allowed tomli = { version = \" ^2.0.1 \" , python = \" <3.11 \" , allow-prereleases = true } # Git dependencies cleo = { git = \" https://github.com/python-poetry/cleo.git \" , branch = \" main \" } # Optional dependencies (installed by extras) pendulum = { version = \" ^2.1.2 \" , optional = true } # Dependency groups are supported for organizing your dependencies [ tool . poetry . group . dev . dependencies ] pytest = \" ^7.1.2 \" pytest-cov = \" ^3.0 \" # ...and can be installed only when explicitly requested [ tool . poetry . group . docs ] optional = true [ tool . poetry . group . docs . dependencies ] Sphinx = \" ^5.1.1 \" # Python-style entrypoints and scripts are easily expressed [ tool . poetry . scripts ] my-script = \" my_package:main \" Installation Poetry supports multiple installation methods, including a simple script found at install.python-poetry.org . For full\ninstallation instructions, including advanced usage of the script, alternate install methods, and CI best practices, see\nthe full installation documentation . Documentation Documentation for the current version of Poetry (as well as the development branch and recently out of support\nversions) is available from the official website . Contribute Poetry is a large, complex project always in need of contributors. For those new to the project, a list of suggested issues to work on in Poetry and poetry-core is available. The full contributing documentation also\nprovides helpful guidance. Resources Releases Official Website Documentation Issue Tracker Discord Related Projects poetry-core : PEP 517 build-system for Poetry projects, and\ndependency-free core functionality of the Poetry frontend poetry-plugin-export : Export Poetry projects/lock files to\nforeign formats like requirements.txt poetry-plugin-bundle : Install Poetry projects/lock files to\nexternal formats like virtual environments install.python-poetry.org : The official Poetry\ninstallation script website : The official Poetry website and blog About Python packaging and dependency management made easy python-poetry.org Topics python package-manager dependency-manager packaging poetry Resources Readme License MIT license Code of conduct Code of conduct Activity Custom properties Stars 30.7k stars Watchers 191 watching Forks 2.2k forks Report repository Releases 133 1.8.3 Latest May 8, 2024 + 132 releases Contributors 558 + 544 contributors Languages Python 87.2% HTML 12.8% Footer ¬© 2024 GitHub,¬†Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time."},
{"url": "https://arxiv.org/pdf/2404.16130", "text": ""}
]